[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Psychological data analysis for graduate students: PSYC411",
    "section": "",
    "text": "1 Preface: Our approach\nWe can, here, explain a development in the approach we take in teaching this course. Naturally, this development in approach will require a parallel development in your approach to learning.\nWe are going to focus on working in research in context (see Figure 1.1).\n\n\n\n\n\n\n\n\nG\n\n\n\nreading\n\nreading\n\n\n\nknowledge\n\nknowledge\n\n\n\nreading--knowledge\n\n\n\n\nconventions\n\nconventions\n\n\n\nknowledge--conventions\n\n\n\n\nconcepts\n\nconcepts\n\n\n\nknowledge--concepts\n\n\n\n\npractices\n\npractices\n\n\n\nknowledge--practices\n\n\n\n\n\n\n\nFigure 1.1: Working in research in context.\n\n\n\n\n\nYou have been introduced to R. We know that some of you are new to R so we will practice the skills you are learning. We will consolidate, revise, and extend these skills.\nWe will encounter — some, for the first time – the linear model also known as regression analysis. But the big change is this focus on context. The reason is that not talking about the context is risky for how you approach, do, or think about data analysis.\nIn traditional methods teaching, the schedule of classes will progress through a series of tests, one test a week, from simpler to more complex tests. In this approach, the presentation is often brief about the context: the question the researchers are investigating; the methods they use to collect data; and, critically, the assumptions they make about how your reasoning can get you from the things you measure to the things you are trying to understand.\nThis approach is understandable but it presents a misleading view. It implies that if you learn the method, and can match the textbook example to your context then all you need to do is to apply the analysis code to get the right result. This style of working is common, and it is often a reasonable place to start, but the isolation from context reduces the application of judgment, and limits critical evaluation of measurement, analysis assumptions, and sources of uncertainty.\n\n\n\n\n\n\nTip\n\n\n\nWe can do better.\n\n\nA more productive approach – this is the approach we will take – is to expose, and talk about some of the real challenges that anybody who handles data, or quantitative evidence, deals with in professional life:\n\nThinking about the mapping from our concerns to the research questions, to the things we measure, the analysis we do, and then the conclusions we make.\nSelecting or constructing valid measures that can be assumed to measure the things they are supposed to measure.\nTaking samples of observations, and making conclusions about the population.\nMaking estimates and linking these estimates to an account that is explicit about causes.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface: Our approach</span>"
    ]
  },
  {
    "objectID": "hypotheses-associations.html",
    "href": "hypotheses-associations.html",
    "title": "2  Hypotheses and associations",
    "section": "",
    "text": "2.1 Overview\nWelcome to your overview of the work we will do together in Week 6.\nThis week, we focus on methods that allow us to make sense of the evidence for associations.\nLooking ahead to your professional lives, our aim is to help you to build the understanding and to learn the skills that will ensure that you can practise, inspire and manage the most effective ways to make sense of associations.\nWe will work in the context of a live research project with potential real world impacts: the Clearly understood project. Working in a concrete context will help you to make sense of what you are doing, even if you are interested in other topics.\nAs we work together, we will be revisiting some of the ideas and techniques you have seen in previous classes, so that you can consolidate your learning. Then, we will extend your development with some new ideas to strengthen your skills.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Hypotheses and associations</span>"
    ]
  },
  {
    "objectID": "hypotheses-associations.html#sec-associations-overview",
    "href": "hypotheses-associations.html#sec-associations-overview",
    "title": "2  Hypotheses and associations",
    "section": "",
    "text": "Tip\n\n\n\nThe Clearly understood project aims to fix the problem that we are not sure how health information should be communicated so that everyone can understand it.\nWe ask the research questions:\n\nWhat person attributes predict success in understanding?\nCan people accurately evaluate whether they correctly understand written health information?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Hypotheses and associations</span>"
    ]
  },
  {
    "objectID": "hypotheses-associations.html#sec-associations-goals",
    "href": "hypotheses-associations.html#sec-associations-goals",
    "title": "2  Hypotheses and associations",
    "section": "2.2 Our learning goals",
    "text": "2.2 Our learning goals\nThis week, we develop your critical thinking and we strengthen your practical skills.\n1. Critical thinking\n\nConcepts: how we go from ideas and questions to hypotheses\n\n2. Practical skills\n\nConcepts – associations: correlations, estimates and hypothesis tests\nSkills – visualizing covariation\nSkills – writing the code\nSkills – estimating correlations\nSkills – hypothesis tests for correlations\nSkills – interpreting and reporting correlations",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Hypotheses and associations</span>"
    ]
  },
  {
    "objectID": "hypotheses-associations.html#sec-associations-resources",
    "href": "hypotheses-associations.html#sec-associations-resources",
    "title": "2  Hypotheses and associations",
    "section": "2.3 Learning resources",
    "text": "2.3 Learning resources\nYou will see, next, the lectures we share to explain the concepts behind the critical thinking and analysis skills you will develop (Section 2.3.1), then you will see information about the practical materials you can use to practise your skills (Section 2.3.2).\n\n\n\n\n\n\nTip\n\n\n\nWe think your learning best if you first watch the lectures then do the practical exercises.\n\n\n\n2.3.1 Lectures\nThe lecture material for this week is presented in five short parts.\nClick on a link and your browser should open a tab showing the Panopto video for the lecture part.\n\nPart 1 (12 minutes) Overview: The concepts and skills we will learn about in weeks 6-10, and why it helps to embed the classes in the context of a research project.\n\n\n\nPart 2 (18 minutes): Hypotheses, measurements and associations – how you can think critically.\n\n\n\nPart 3 (14 minutes): The live research project we will use to put our practical skills exercises and critical thinking challenges into context.\n\n\n\nPart 4 (20 minutes): Seeing, thinking and talking about associations, correlations.\n\n\n\nPart 5 (12 minutes): How we use R to estimate and test correlations. How we write about correlation results\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe slides presented in the videos can be downloaded here:\n\nThe slides exactly as presented (22 MB).\n\nYou can download the web page .html file and click on it to open it in any browser (e.g., Chrome, Edge or Safari). The slide images are high quality so the file is quite big and may take a few seconds to download.\n\n\nWe are going to work through some practical exercises, next, to stimulate your thinking and learn the practical skills you need to see, test and talk about correlation analyses of the associations between variables.\n\n\n2.3.2 Practical materials: data and R-Studio\nWe will work with two data files which you can download by clicking on their names (below):\n\nstudy-one-general-participants.csv.\nstudy-two-general-participants.csv.\n\nOnce you have downloaded the files, you will need to upload them to the R-Studio server to access and use the R files.\n\n\n\n\n\n\nImportant\n\n\n\nHere is a link to the sign-in page for R-Studio Server\n\n\n\n\n2.3.3 Practical materials guide\nYou will find that the practical exercises are simpler to do if you follow these steps in order.\n\nThe data — We will take a quick look at what is inside the data files so you know what everything means.\nThe how-to guide — We will go through the practical analysis and visualization coding steps, showing all the code required for each step.\nThe practical exercises — We will set out the tasks, questions and challenges that you should complete to learn the practical skills we target this week.\n\n\n\n\n\n\n\nTip\n\n\n\nWe show you how to do everything you need to do in the practical exercises, first, in the how-to guide.\n\nStart by looking at the how-to guide to understand what steps you need to follow in the lab activity.\n\nIf you want to make it more challenging for yourself, go straight to Step 3.\n\n\nWe will take things step-by-step:\n\ndifferent parts for different phases of the analysis workflow;\ndifferent tasks for different things you need to do;\ndifferent questions to examine different ideas or coding challenges.\n\n\n2.3.3.1 The data files\nEach of the data files we will work with has a similar structure.\nHere are what the first few rows in the data file study.one.gen looks like:\n\n\n\n\n\nparticipant_ID\nmean.acc\nmean.self\nstudy\nAGE\nSHIPLEY\nHLVA\nFACTOR3\nQRITOTAL\nGENDER\nEDUCATION\nETHNICITY\n\n\n\n\nstudyone.1\n0.49\n7.96\nstudyone\n34\n33\n7\n53\n11\nNon-binary\nHigher\nWhite\n\n\nstudyone.10\n0.85\n7.28\nstudyone\n25\n33\n7\n60\n11\nFemale\nHigher\nWhite\n\n\nstudyone.100\n0.82\n7.36\nstudyone\n43\n40\n8\n46\n12\nMale\nFurther\nWhite\n\n\nstudyone.101\n0.94\n7.88\nstudyone\n46\n33\n11\n51\n15\nMale\nHigher\nWhite\n\n\nstudyone.102\n0.58\n6.96\nstudyone\n18\n32\n3\n51\n12\nMale\nSecondary\nMixed\n\n\nstudyone.103\n0.84\n7.88\nstudyone\n19\n37\n13\n45\n19\nFemale\nFurther\nAsian\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe webpage has a slider under the data table window, so you can scroll across the columns: move your cursor over the window to show the slider.\n\n\nWhen you look at the data table, you can see the columns:\n\nparticipant_ID participant code\nmean.acc average accuracy of response to questions testing understanding of health guidance\nmean.self average self-rated accuracy of understanding of health guidance\nstudy variable coding for what study the data were collected in\nAGE age in years\nHLVA health literacy test score\nSHIPLEY vocabulary knowledge test score\nFACTOR3 reading strategy survey score\nGENDER gender code\nEDUCATION education level code\nETHNICITY ethnicity (Office National Statistics categories) code\n\n\n\n2.3.3.2 The how-to guide\nWe will take things step-by-step.\nWe split .Rmd scripts by steps, tasks and questions:\n\ndifferent parts for different phases of the analysis workflow;\ndifferent tasks for different things you need to do;\ndifferent questions to examine different ideas or coding challenges.\n\n\n\n\n\n\n\nTip\n\n\n\n\nMake sure you start and work your way, in order, through each part, task and question.\nComplete each task before you move on to the next task.\n\n\n\n\n\n2.3.3.3 How-to Part 1: Set-up\nTo begin, we set up our environment in R.\n\n2.3.3.3.1 How-to Task 1 – Run code to empty the R environment\n\nrm(list=ls())                            \n\n\n\n2.3.3.3.2 How-to Task 2 – Run code to load libraries\nLoad libraries using library().\n\nlibrary(\"tidyverse\")\n\n\n\n\n2.3.3.4 How-to Part 2: Load and examine the data\n\n2.3.3.4.1 How-to Task 3 – Read in the data file we will be using\nThe code in the how-to guide was written to work with the data file:\n\nstudy-one-general-participants.csv.\n\nRead in the data file – using read_csv().\n\nstudy.one.gen &lt;- read_csv(\"study-one-general-participants.csv\")\n\n\n\n\n\n\n\nTip\n\n\n\nYou can choose your own file name, but be sure to give the data-set a distinct name, e.g., study.one.gen so that R can distinguish between the different data you will work with.\n\n\n\n\n2.3.3.4.2 How-to Task 4 – Inspect the data file\nUse the summary() function to take a look.\n\nsummary(study.one.gen)\n\n participant_ID        mean.acc        mean.self        study          \n Length:169         Min.   :0.3600   Min.   :3.440   Length:169        \n Class :character   1st Qu.:0.7600   1st Qu.:6.080   Class :character  \n Mode  :character   Median :0.8400   Median :7.080   Mode  :character  \n                    Mean   :0.8163   Mean   :6.906                     \n                    3rd Qu.:0.9000   3rd Qu.:7.920                     \n                    Max.   :0.9900   Max.   :9.000                     \n      AGE           SHIPLEY           HLVA           FACTOR3     \n Min.   :18.00   Min.   :23.00   Min.   : 3.000   Min.   :34.00  \n 1st Qu.:24.00   1st Qu.:33.00   1st Qu.: 7.000   1st Qu.:46.00  \n Median :32.00   Median :35.00   Median : 9.000   Median :51.00  \n Mean   :34.87   Mean   :34.96   Mean   : 8.905   Mean   :50.33  \n 3rd Qu.:42.00   3rd Qu.:38.00   3rd Qu.:10.000   3rd Qu.:55.00  \n Max.   :76.00   Max.   :40.00   Max.   :14.000   Max.   :63.00  \n    QRITOTAL        GENDER           EDUCATION          ETHNICITY        \n Min.   : 6.00   Length:169         Length:169         Length:169        \n 1st Qu.:12.00   Class :character   Class :character   Class :character  \n Median :13.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   :13.36                                                           \n 3rd Qu.:15.00                                                           \n Max.   :19.00                                                           \n\n\n\n\n\n\n\n\nTip\n\n\n\n\nsummary() will give you either descriptive statistics for variable columns classified as numeric or will tell you that columns in the dataset are not numeric.\nsummary(...) is a function and, again, you put the name of the dataset inside the brackets to view it.\n\n\n\n\nQ.1. What is the mean of mean.acc?\n\n\nA.1. 0.8163\n\n\nQ.2. What class is the variable study?\n\n\nA.2. character\n\n\nQ.3. Does the summary indicate if any variable has missing values (NA)?\n\n\nA.3. No\n\n\n\n2.3.3.4.3 How-to Task 5 – Change the class or type of a variable\nYou can use the as.factor() function you used in Week 5.\n\nstudy.one.gen$study &lt;- as.factor(study.one.gen$study)\n\n\nQ.4. After you have done this, what information does summary() give you about the variable study?\n\n\nA.4. We can see the number 169 beside the word studyone: this tells us that there are 169 observations, in the column, each one is a value: the word or character string studyone.\n\n\n\n\n\n\n\nTip\n\n\n\nRemember from Week 3 that we can only count how many times a category value (or factor level) occurs: here, we are counting how many times the word studyone occurs in the factor column study.\n\n\n\n\n\n2.3.3.5 How-to Part 3: Visualize associations\n\n2.3.3.5.1 How-to Task 6 – Draw scatterplots to examine associations between variables\nYou have seen these code moves before, in previous classes (weeks 3 and 4): we are consolidating skills by practising your coding in different contexts, using different data.\nWe extend your skills by adding some new moves.\nCreate scatterplots to examine the association between some variables.\n\n\n\n\n\n\nTip\n\n\n\nWe are working with geom_point() and you need x and y aesthetic mappings.\n\n\nFor example, we can draw a scatterplot to examine the association between mean.self and mean.acc.\n\nggplot(data = study.one.gen, aes(x = mean.self, y = mean.acc)) + \n    geom_point()\n\n\n\n\n\n\n\n\nExamine this plot — it shows: the possible association between x-axis variable mean.self and y-axis variable mean.acc.\nYou have seen this kind of code before but it will help your learning if, now, we take a look at the code step-by-step.\nThe plot code moves through the following steps:\n\nggplot(...) makes a plot.\nggplot(data = study.one.gen, ...) uses the study.one.gen data-set.\nggplot(...aes(x = mean.self, y = mean.acc)) uses two aesthetic mappings.\ngeom_point() show the mappings as points.\n\n\n\n\n\n\n\nTip\n\n\n\nWhat are aesthetic mappings?\nAesthetic mappings translate data information – numbers or values in column variables – into things you can see in plots.\nFor a scatterplot, we need to translate values for two variables into the position of each point in the plot.\nHere:\n\nx = mean.self maps mean.self values to x-axis (horizontal, left to right) positions.\ny = mean.acc maps mean.acc values to y-axis (vertical, bottom to top) positions.\n\n\n\nNow do scatterplots with any pair of numeric variables you like.\nRemember that we saw with summary() that not every variable consists of numbers.\nCheck out the example code, below.\n\nggplot(data = study.one.gen, aes(y = mean.self, x = mean.acc)) +\n    geom_point()  \n\nggplot(data = study.one.gen, aes(x = AGE, y = mean.self)) +\n  geom_point()  \n  \nggplot(data = study.one.gen, aes(x = SHIPLEY, y = mean.self)) +\n  geom_point()  \n\nggplot(data = study.one.gen, aes(x = HLVA, y = mean.self)) +\n  geom_point()  \n\n\n\n2.3.3.5.2 How-to Task 7 – Edit the scatterplots to change how they look\nEdit the appearance of a plot step-by-step.\nWe are going to edit:\n\nthe appearance of the points using alpha and size;\nthe colour of the background using theme_bw();\nthe appearance of the labels using labs().\n\nWe make the changes, one change at a time.\nYou have seen one of these moves before and you can guess at how to do the others. Click on the drop-down view to see the code but, if you want a challenge, try first to write the code on your own.\n\nthe appearance of the points using alpha and size\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.one.gen, aes(x = HLVA, y = mean.self)) +\n  geom_point(alpha = 0.5, size = 2) \n\n\n\n\n\n\n\n\n\n\n\n\nthe colour of the background using theme_bw()\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.one.gen, aes(x = HLVA, y = mean.self)) +\n  geom_point(alpha = 0.5, size = 2)  +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\nthe appearance of the labels using labs()\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.one.gen, aes(x = HLVA, y = mean.self)) +\n  geom_point(alpha = 0.5, size = 2)  +\n  theme_bw() +\n  labs(x = \"HLVA\", y = \"mean self rated accuracy\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe arguments alpha and size can change the appearance of most geometric objects (geoms) in ggplot:\n\nin the code example, here, we vary the alpha number to change how opaque or transparent the points are;\nand we vary the size number to vary the size of the points.\n\n\n\n\n\n2.3.3.5.3 How-to Task 8 – Now experiment\nThere are no right answers here: edit the appearance of your plots by changing alpha, size and colour of points.\n\n\n\n\n\n\nTip\n\n\n\nPlay is an important part of learning.\n\nExperimenting with how plots look is a key element in becoming a master at data visualization. You won’t know what looks good to you unless you try different things.\n\n\n\nIf you really want to extend your skills, it is really important that you learn how to find useful online help.\n\nQ.5. Can you find the ggplot reference page on scatterplots?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nDo a search with the keywords: ggplot reference geom_point\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA.5. You can find reference information here: https://ggplot2.tidyverse.org/reference/geom_point.html\n\n\n\n\n\nQ.6. Can you change the colour of the points to a colour you like?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nDo a search with the keywords: ggplot colours.\nUseful information on colour can be found here:\nhttps://r-graphics.org/recipe-colors-setting\nor\nhttp://www.cookbook-r.com/Graphs/Colors_(ggplot2)/\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA.6. Here is how you do it: R will recognize many English colour names.\n\n\nggplot(data = study.one.gen, aes(x = HLVA, y = mean.self)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"hotpink\")  +\n  theme_bw() +\n  labs(x = \"HLVA\", y = \"mean self rated accuracy\")\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNow you: experiment!\n\n\n\n\n\n2.3.3.6 How-to Part 4: Analyse associations\n\n2.3.3.6.1 How-to Task 9 – Use correlation to to answer a research question\nExamine associations between variables using correlation.\nOne of our research questions is:\n\nCan people accurately evaluate whether they correctly understand written health information?\n\nWe can answer this question by examining whether mean self-rated accuracy of understanding correlates with mean accuracy of understanding. The logic is that if we can accurately rate our own understanding (from bad to good) then that rating should be associated – should be correlated with – how accurately we can actually respond to questions that test that understanding.\n\nQ.7. How do we examine the correlation between mean self-rated accuracy (mean.self) and mean accuracy (mean.acc)?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nRemember from the Week 6 lecture, that we use cor.test():\n\n\n\nCan you figure out how to code a correlation test? It helps with your learning if you first try to anticipate what the code will look like. Then reveal the code, below, to see what you guessed right. (Getting some things right, and some things wrong, is part of the learning process.)\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nA.7. Here is how you do it: take a look at the code and the results of the correlation test.\n\n\ncor.test(study.one.gen$mean.acc, study.one.gen$mean.self, method = \"pearson\",  alternative = \"two.sided\")\n\n\n    Pearson's product-moment correlation\n\ndata:  study.one.gen$mean.acc and study.one.gen$mean.self\nt = 7.1936, df = 167, p-value = 2.026e-11\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3619961 0.5937425\nsample estimates:\n      cor \n0.4863771 \n\n\n\n\n\n\nQ.3. What is r, the correlation coefficient?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA.3. r = 0.4863771\n\n\n\n\n\nQ.4. Is the correlation significant?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA.4. r is significant\n\n\n\n\n\nQ.5. What are the values for t and p for the significance test for the correlation?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA.5. t = 7.1936, p = 2.026e-11\n\n\n\n\n\nQ.6. What do you conclude, given the correlation results?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nReview the scatterplot you drew earlier (or draw one now) to examine the shape of the association between these variables.\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA.6. mean.acc and mean.self are positively correlated suggesting that as # mean.acc scores increase so also do mean.self scores\n\n\n\n\n\n\n\n2.3.3.7 The practical exercises\nNow you will progress through a series of tasks, and challenges, to test what you have learnt.\n\n\n\n\n\n\nWarning\n\n\n\nNow we will work with the data file\n\nstudy-two-general-participants.csv\n\n\n\nWe again split the steps into into parts, tasks and questions.\nWe are going to work through the following workflow steps:\n\nEmpty the R environment\nLoad relevant libraries\nRead in the data file\nInspect the data\nChange the type classification of variables if necessary\nDraw scatterplots to visualize the association between pairs of variables\nEstimate and test the correlations between pairs of variables\n\n\n\n\n\n\n\nTip\n\n\n\n\nThe how-to guide showed you what functions you needed and how you should write the function code.\nUse the code structure you have seen and change it to use the data required for these practical exercises: you will need to change the data-set name, the variable names, to get the code to work for the following exercises.\nIn learning how to code, the process of adapting example code is a key skill: we are learning what can change, and what has to stay the same.\n\n\n\nIn the following, we will guide you through the tasks and questions step by step.\nAn answers version of the workbook will be provided after the practical class.\n\n\n2.3.3.8 Practical Part 1: Set-up\nTo begin, we set up our environment in R.\n\n2.3.3.8.1 Practical Task 1 – Run code to empty the R environment\n\nrm(list=ls())                            \n\n\n\n2.3.3.8.2 Practical Task 2 – Run code to load relevant libraries\n\nlibrary(\"tidyverse\")\n\n\n\n\n2.3.3.9 Practical Part 2: Load the data\n\n2.3.3.9.1 Practical Task 3 – Read in the data file we will be using\nThe data file for the workbook is called:\n\nstudy-two-general-participants.csv\n\nUse the read_csv() function to read the data file into R.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nstudy.two.gen &lt;- read_csv(\"study-two-general-participants.csv\")\n\n\n\n\nWhen you code this, you can choose your own file name, but be sure to give the data object you create a distinct name e.g. study.two.gen.\n\n\n2.3.3.9.2 Practical Task 4 – Inspect the data file\nUse the summary() or head() functions to take a look\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nsummary(study.two.gen)\n\n\n\n\n\nQ.1. What is the median of AGE?\n\n\n\nQ.2. What class is the variable ETHNICITY?\n\n\n\nQ.3. Does the summary indicate if any variable has missing values (NAs)?\n\n\n\n\n2.3.3.9.3 Practical Task 5 – Change the class or type of the variable ETHNICITY to factor\nYou can use the as.factor() function you have used before: how do you write the code for these data? Try it for yourself before revealing the code.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nstudy.two.gen$ETHNICITY &lt;- as.factor(study.two.gen$ETHNICITY)\n\n\n\n\n\nQ.4. After you have done this, what information does summary() give you about the variable ETHNICITY?\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nsummary(study.two.gen)\n\n\n\n\n\n\n\n\n\n\n\n\n2.3.3.10 Practical Part 3: Visualise the associations between variables\n\n2.3.3.10.1 Practical Task 6 – Create a scatterplot to examine the association between some variables\nFor this practical exercise, uou always want to use the outcome mean.acc as the y-axis variable so:\n\ny = mean.acc\n\nThen you can use each numeric predictor variable as the x-axis variable so:\n\nx = mean.self\n\nProduce scatterplots with every numeric predictor variable in the study.two.gen dataset\n\n\n\n\n\n\nTip\n\n\n\nRemember what we saw with summary(): not every variable consists of numbers\nIf the summary() does not show you a mean for a variable, then R does not think that variable is numeric\n\n\nNow, let’s build some intuition.\nScientists often use scatterplots to get an intuitive understanding of the relationships between variables.\n\n\n\n\n\n\nTip\n\n\n\nYou can read a scatterplot to learn about the size and the direction of an association between two variables (see the week 6 lecture):\n\nIs the cloud of points more diffuse (the association is weaker) or more tightly clustered (the association is stronger)?\nDoes the cloud of points slope upwards (the association is more positive) or slope downwards (the association is negative).\n\n\n\nThe following questions ask you to look at plots, and make some judgments about what the plots tell you.\nIt can be hard to decide what an association looks like so make a decision based on what you see.\nRight now, we are working to build your intuitions so: reflect, trust your intuition, and make a decision.\n\n\n\n\n\n\nTip\n\n\n\nFirst draw the plot, then answer the question.\n\n\n\nQ.5. What is the shape (direction) of the association between mean.self and mean.acc?\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = mean.self, y = mean.acc)) +\n  geom_point()\n\n\n\n\n\nQ.6. What is the shape (direction) of the association between AGE and mean.acc?\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = AGE, y = mean.acc)) +\n  geom_point()\n\n\n\n\n\nQ.7. What is the shape (direction) of the association between SHIPLEY and mean.acc?\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point()\n\n\n\n\n\nQ.8. What is the shape (direction) of the association between HLVA and mean.acc?\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = HLVA, y = mean.acc)) +\n  geom_point()\n\n\n\n\n\nQ.9. What is the shape (direction) of the association between FACTOR3 and mean.acc?\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = FACTOR3, y = mean.acc)) +\n  geom_point()\n\n\n\n\n\nQ.10. What is the shape (direction) of the association between QRITOTAL and mean.acc?\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = QRITOTAL, y = mean.acc)) +\n  geom_point()\n\n\n\n\n\n\n\n2.3.3.11 Practical Part 4: Learn how to edit plotting code\n\n2.3.3.11.1 Practical Task 7 – Edit the appearance of one plot step-by-step\nEdit your plotting code to make plots with a professional appearance.\nEdit a scatterplot to adjust\n\nthe appearance of the points using alpha, size and colour;\nthe colour of the background using theme_bw();\nthe appearance of the labels using labs().\n\nIn the following, we ask you to edit one plot element at a time. You can work out what to do.\n\nQ.11. Can you edit the appearance of the points in a scatterplot using alpha, size and colour?\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = HLVA, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"red\")\n\n\n\n\n\nQ.12. Can you edit the appearance of the plot background?\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = HLVA, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"red\")   +\n  theme_bw()\n\n\n\n\n\nQ.13. Can you edit the appearance of the labels using labs()?\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = HLVA, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"red\")   +\n  theme_bw() +\n  labs(x = \"HLVA\", y = \"mean accuracy\")\n\n\n\n\n\nQ.14. Can you find online information about colour blind palettes?\n\nTry doing a search with the keywords: ggplot colour blind\n\n\n\nNow it’s your turn: experiment!\n\nWhat about different colour words? Replace \"red\" with \"...\"?\nWhat about a different size? Replace size = 2 with size =  ... using a different number.\nWhat about a different level of transparency (alpha)? Replace alpha = 0.5 with alpha =  ... using a different fraction.\n\n\n\n\n\n2.3.3.12 Practical Part 5: Use correlation to to answer the research questions\n\n2.3.3.12.1 Practical Task 8 – Examine the correlation between mean accuracy (mean.acc) and some numeric predictor variables\n\nQ.15. What is r (given as cor in the output) for the correlation between HLVA and mean.acc?\n\nCan you figure out the code to do the calculation?\n\n\n\n\n\n\nCode\n\n\n\n\n\n\ncor.test(study.two.gen$HLVA, study.two.gen$mean.acc, method = \"pearson\",  alternative = \"two.sided\")\n\n\n\n\n\n\nQ.16. Is the correlation significant?\n\n\n\nQ.17. What are the values for t and p for the significance test for the correlation?\n\n\n\nQ.18. What do you conclude, given the correlation results?\n\nMaybe draw a scatterplot to examine the shape of the association.\n\n\nQ.19. What is r (given as cor in the output) for the correlation between mean.self and mean.acc?\n\n\nCan you figure out the code to do the calculation?\n\n\n\n\n\n\nCode\n\n\n\n\n\n\ncor.test(study.two.gen$mean.self, study.two.gen$mean.acc, method = \"pearson\",  alternative = \"two.sided\")\n\n\n\n\n\nQ.20. Is the correlation between AGE and mean.acc significant?\n\n\nCan you figure out the code to do the calculation?\n\n\n\n\n\n\nCode\n\n\n\n\n\n\ncor.test(study.two.gen$AGE, study.two.gen$mean.acc, method = \"pearson\",  alternative = \"two.sided\")\n\n\n\n\n\nQ.21. What are the values for t and p for the significance test for the correlation between QRITOTAL and mean.acc?\n\n\nCan you figure out the code to do the calculation?\n\n\n\n\n\n\nCode\n\n\n\n\n\n\ncor.test(study.two.gen$QRITOTAL, study.two.gen$mean.acc, method = \"pearson\",  alternative = \"two.sided\")\n\n\n\n\n\nQ.22. What do you conclude, given the correlation results, about the association between SHIPLEY and mean.acc?\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\ncor.test(study.two.gen$SHIPLEY, study.two.gen$mean.acc, method = \"pearson\",  alternative = \"two.sided\")\n\n\n\n\n\n\n\n\n2.3.4 The answers\nAfter the practical class, we will reveal the answers that are currently hidden.\nThe answers version of the webpage will present my answers for questions, and some extra information where that is helpful.\n\n\n2.3.5 Look ahead: growing in independence\n\n\n\n\n\n\nImportant\n\n\n\n\nEvery problem you ever have: someone has had it before, solved it, and written a blog (or tweet or toot) about it.\nR is free open statistical software: everything you use is contributed, discussed and taught by a community of R users online, in open forums.\nLearning to navigate this knowledge is an introduction to the future of knowledge sharing.\n\n\n\n\n\n2.3.6 Optional exercises: to stretch you\nOne of the convenient and powerful things about R plotting code is that you can create a series of plots and put them together in a grid of plots for east comparison: we do that here.\nWe will use the patchwork library: check it out\nhttps://patchwork.data-imaginist.com/articles/patchwork.html\nWe get the library like this:\n\nlibrary(patchwork)\n\nHere’s an example:\nFirst create two plots: give them both names\n\np.AGE &lt;- ggplot(data = study.two.gen, aes(x = AGE, y = mean.acc)) +\n  geom_point() \n#\np.SHIPLEY &lt;- ggplot(data = study.two.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point()  \n\nSecond put the two plots together by calling their names.\n\np.AGE  + p.SHIPLEY\n\n\n\n\n\n\n\n\nNow you try it:\n\nCreate two plots, using QRITOTAL and SHIPLEY as predictors, and mean.acc as the outcome\nThen make a grid to present them side by side.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\np.QRITOTAL &lt;- ggplot(data = study.two.gen, aes(x = QRITOTAL, y = mean.acc)) +\n  geom_point() \n#\np.SHIPLEY &lt;- ggplot(data = study.two.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point()  \n\n# -- second put them together\np.QRITOTAL  + p.SHIPLEY",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Hypotheses and associations</span>"
    ]
  },
  {
    "objectID": "hypotheses-associations.html#sec-associations-notes",
    "href": "hypotheses-associations.html#sec-associations-notes",
    "title": "2  Hypotheses and associations",
    "section": "2.4 Lecture notes",
    "text": "2.4 Lecture notes\nSome people find it easier to read notes than to watch video recordings. This is why we also include the lecture notes here.\n\n\n\n\n\n\nTip\n\n\n\nIn these notes, I provide notes on the code steps that result in the plot.\n\nClick on the Notes tab to see them.\n\n\n\n\n2.4.1 Better methods can’t make up for mediocre theory\n\nPreviously – in week 6: I talked about improving science through open reproducible methods but we cannot make progress without better theory and data (Smaldino, 2019)\nWe want open reproducible findings but we do not just want reproducibility\nWe want to make sense of people in useful ways\n\n\n\n2.4.2 Open, reproducible, methods are not enough\n\nNow: we need to think causally about predictions and about measurement\nWe discuss health comprehension project to demonstrate critical self-reflection\n\n\nFor useful hypotheses, we need better theory so we can build clear testable predictions from explicit assumptions\nAnd with better models, we need better measurement because if we cannot reliably measure something then it is hard to build a theory about it\n\n\n\n2.4.3 Critical thinking and you\n\nStudents and colleagues almost never have problems coding analyses in R\nThe challenges are almost always located in the critical reflection you must do in order to develop sensible analysis, and to interpret the analysis results\nSo we need to start by highlighting the work of critical reflection in data analysis\n\n\n\n2.4.4 Why most psychological research findings are not even wrong\n\nAs you will know, it is often difficult to identify a claim in an article (Scheel, 2022)\nHere are some questions you can ask to decide if a claim you read or make is clear:\n\n\nIs the claim stated unambiguously: can the claim support or contradict (or is it uncertain about) a prediction?\nCan you understand how we get back from the claim to the data, given assumptions about measurement, sampling and procedure?\n\n\n\n2.4.5 Why hypothesis testers should spend less time testing hypotheses\n\nThe response to crisis has been to teach and use better methods\nThis improvement reveals a core problem (Scheel et al., 2021): we often work to test hypotheses but our hypotheses are often undeveloped\nWe train hypothesis testing but we also need to train hypothesising:\nhow to measure, operationalize, and how to decide if hypothesis is corroborated or not\n\n\n\n\n\n\n\nTip\n\n\n\nWe want to be capable of being wrong\n\n\n\n\n2.4.6 Statistical rituals largely eliminate critical statistical thinking\n\nTraditionally, students learn statistical tests, and learn to identify if a test statistic is significance or not\nIf we do not also talk about what is actually observed, and whether or how – or why – it is or is not compatible with theory based predictions then we do ritual not science (Gigerenzer, 2004)\nThis is a problem: the focus on anything-but-null allows us to build or accommodate vague theories that can never be wrong\n\n\n\n2.4.7 We need to think about the derivation chain\n\n\n\n\n\n\n\n\nQ\n\n\ncluster_R\n\n\n\n\nnd_1_l\n\nConcept formation\n\n\n\nnd_1_r\n\nCausal model\n\n\n\nnd_2_l\n\nMeasurement\n\n\n\nnd_1_l-&gt;nd_2_l\n\n\n\n\n\nnd_3\n\nStatistical predictions\n\n\n\nnd_2_l-&gt;nd_3\n\n\n\n\n\nnd_2_r\n\nAuxiliary assumptions\n\n\n\nnd_2_r-&gt;nd_3\n\n\n\n\n\nnd_4\n\nTesting hypotheses\n\n\n\nnd_3-&gt;nd_4\n\n\n\n\n\n\n\n\nFigure 2.1: The derivation chain\n\n\n\n\n\n\n\n2.4.8 Here’s a toolkit for thinking productively about your hypotheses\n\n\n\n\n\n\n\nThe derivation chain (Meehl, 1990; Scheel et al., 2021)\n\nDevelop your theory: the concepts, and the assumptions about causality\nSpecify how psychological concepts will be measured\nIdentify auxiliary assumptions about how we get from theoretical concepts to observable data\nIdentify theoretical predictions\nLink theoretical predictions to specific statistical tests that may support or contradict them\n\n\n\n2.4.9 Valid measures\n\nWe often teach and learn about different kinds of validity but the key idea is simple (Borsboom et al., 2004)\n\n\na test is valid for measuring an attribute if and only if (a) the attribute exists and (b) variations in the attribute causally produce variations in the outcomes of the measurement procedure\n\n\nWe want to work with valid measures but validity requires explaining: (Q.1) Does the thing exist in the world? (Q.2) Is variation in that thing be reflected in variation in our measurement?\n\n\n\n2.4.10 Summary: our critical thinking checklist\n\nWhat is our (causal) theory?\nWhat measures are we using, why?\nWhat is our specific prediction, why?\nDoes the prediction relate to sign and to magnitude?\nWhat analysis can test this prediction, why?\nHow will our results affect our beliefs, why?\n\n\n\n2.4.11 Case study: the health comprehension project\n\nBecause the important questions concern how psychologists ask and answer research questions\nWe will work in the context of a live research project: What makes it easy or difficult to understand written health information?\n\n\n\n\nflickr: Sasin Tipchair ‘Senior woman in wheelchair talking to a nurse in a hospital’\n\n\nWhy this? We don’t really know what makes it easy or difficult to understand advice about health\n\n\n2.4.12 Health comprehension project: impacts\n\nWe are working to help improve communication\nWith partners at Vienna Business University, Kantar Public, and the London School of Economics\nOur work has implications for: business communication; understanding reading development; marketing communication\n\n\n\n2.4.13 Health comprehension project: questions and analyses\n\nOur research questions are:\n\n\nWhat person attributes predict success in understanding?\nCan people accurately evaluate whether they correctly understand written health information?\n\n\nThese kinds of research questions can be answered using methods like correlation, linear models\n\n\n\n2.4.14 Health comprehension project – relevance: methods you will use in your professional work\n\nWe got funding to collect data online using online Qualtrics questionnaire surveys\nWe tested people on a range of dimensions using standardized ability and our own knowledge tests\nMany of you will go on to work with online surveys, and with data from standardized ability measures\n\n\n\n2.4.15 Health comprehension project: samples\n\nWe collected data in two studies in 2020: using the online Prolific platform to recruit participants\nWe did several replication studies in student-led projects: we analyze the data in class\n\n\n\n2.4.16 Health comprehension project: why it is a case study\n\nThe health project has strengths and limitations\nWe show how to identify and critically evaluate this project so you can do the same for your work\n\n\n\n\nExtract from Qualtrics survey\n\n\n\n\n\n2.4.17 Cognitive process theory of comprehension success\n\nWhen skilled adult readers read and try to understand written text (Kintsch, 1994)\nThey must recognize and access the meanings of words\nThen use knowledge and reasoning to build an interpretation of what is in the text\nBased on connecting the information in the text with what they already know\n\n\n\n2.4.18 Individual differences theory of comprehension success\n\nSuccessfully understanding text depends on (1.) language experience and (2.) reasoning ability (Freed et al., 2017)\n\n\n\n\n\n\n\n\n\nQ\n\n\n\nnd_1_l\n\nLanguage experience\n\n\n\nnd_2\n\nComprehension outcome\n\n\n\nnd_1_l-&gt;nd_2\n\n\n\n\n\nnd_1_r\n\nReasoning capacity\n\n\n\nnd_1_r-&gt;nd_2\n\n\n\n\n\n\n\n\nFigure 2.2: Factors influencing comprehension success\n\n\n\n\n\n\n\n\n2.4.19 Where the data come from: our measures\n\nWe measure reading comprehension: asking people to read text and then answer multiple choice questions\nWe measure background knowledge: vocabulary knowledge (Shipley); health literacy (HLVA)\nWe ask people to rate their own understanding of each text\n\n\n\n2.4.20 Example critical evaluation questions\n\nAre multiple choice questions good ways to probe understanding? – What alternatives are there?\nAre tests like the Shipley good measures of language knowledge? – What do we miss?\nCan a person accurately evaluate their own understanding? – Can we rely on subjective judgments?\n\n\n\n2.4.21 Relevance to you\n\nEven very good students sometimes do not question the validity of measures:\nNot asking questions like this has a real impact on the value of the interpretation of results\nHere, we are looking ahead to the critical thinking you will need to do for your dissertations\n\n\n\n2.4.22 Talking about the relationships between variables\n\nPsychologists and people who work in related fields often want to know about associations\nIs variation in observed values on one dimension (e.g., comprehension) related to variation in another dimension (e.g., vocabulary)?\nDo values on both dimensions vary together?\n\n\n\n2.4.23 The language in this area can vary: we will be consistent but you need to be aware of the different terms\n\nOutcome \\(=\\) response \\(=\\) criterion \\(=\\) dependent variable\nPredictor \\(=\\) covariate \\(=\\) independent variable \\(=\\) factor\nLinear model \\(=\\) regression analysis \\(=\\) regression model \\(=\\) multiple regression\n\n\n\n2.4.24 Let’s look at the data we will use\n\nFirst, we need to read the data into R\n\n\nstudy.one.gen &lt;- read_csv(\"study-one-general-participants.csv\")\n\n\nNext, we should take a look at the data: you can open the data-set in Excel or you can use the head command to show the first few rows in\nThe person in row 1 has ETHNICITY White, is AGE 34 years, scored 33 on Shipley vocabulary, scored 7 on HLVA health literacy and, on average, self-rated their understanding of health information as 7.96 (so 8/9, mean.self) while scoring 0.49 accuracy in tests of understanding (49% mean.acc)\n\n\nstudy.one.gen %&gt;%\n    select(mean.acc, mean.self, HLVA, SHIPLEY, AGE, ETHNICITY) %&gt;%\n    head(n = 4)\n\n# A tibble: 4 × 6\n  mean.acc mean.self  HLVA SHIPLEY   AGE ETHNICITY\n     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;    \n1     0.49      7.96     7      33    34 White    \n2     0.85      7.28     7      33    25 White    \n3     0.82      7.36     8      40    43 White    \n4     0.94      7.88    11      33    46 White    \n\n\n\n\n2.4.25 Destination correlation: where the correlation number comes from\n\nCovariance\n\n\\[COV_{xy} = \\frac{\\sum(x - \\bar{x})(y - \\bar{y})}{n -1}\\]\n\nIf we want to estimate the correlation between two sets of numbers: \\(x\\) and \\(y\\)\nWe want to know if variation in \\(x\\) (given by \\(x - \\bar{x}\\))\nVaries together with variation in \\(y\\) (given by \\(y - \\bar{y}\\))\n\n\n\n2.4.26 Destination correlation: where the correlation number comes from\n\nCovariance divided by standard deviations\n\n\\[r = \\frac{COV_{xy}}{s_xs_y}\\]\n\nBecause the two sets of numbers can be on different scales: e.g., SHIPLEY out of 40; mean.acc (proportion, out of 1)\nAnd because covariance values depend on the scales\nTo correlations easier to compare, we need to remove scale by dividing by the variables standard deviations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.4.27 Let’s think about an example correlation\n\nResearch question: Can people accurately evaluate whether they correctly understand written health information?\nMeasurement: Someone with higher scores on tested accuracy of understanding will also present higher scores on their ratings of their own understanding\nStatistical prediction: We predict that mean.acc and mean.self scores will be associated\nTest: If the prediction is correct, mean.acc and mean.self scores will be correlated\n\n\n\n2.4.28 Distributions: How do scores vary?\n\nPlotNotes\n\n\n\n\n\n\n\n\n\n\nFigure 2.3: Histograms showing the distribution of mean accuracy and mean self-rated accuracy scores in the ‘study.one.gen’ data-set: means calculated for each participant over all their responses\n\n\n\n\n\n\n\n\nlibrary(patchwork)\nlibrary(tidyverse)\n\np.acc &lt;- study.one.gen %&gt;%\n           ggplot(aes(x = mean.acc)) + \n           geom_histogram(binwidth = .1) +\n           geom_vline(xintercept = mean(study.one.gen$mean.acc),\n                      size = 1.5, colour = \"red\") +\n           xlab(\"mean accuracy\") +\n           xlim(0, 1.1) +\n           theme_bw()\n\np.self &lt;- study.one.gen %&gt;%\n           ggplot(aes(x = mean.self)) + \n           geom_histogram(binwidth = 1) +\n           geom_vline(xintercept = mean(study.one.gen$mean.self),\n                      size = 1.5, colour = \"red\") +\n           xlab(\"mean self-rated accuracy\") +\n           xlim(0, 10) +\n           theme_bw()\n\np.acc + p.self\n\nLet’s go through the code step-by-step.\n\n\n\n\n\n\nTip\n\n\n\n\nHere the code is broken down, line by line, and each line is numbered.\nThis presentation style is done to make it easier for you to see what each step is doing.\nNotice that when we use p.acc &lt;- study.one.gen we tell R to first create the plot (giving it the name p.acc) but to not show it yet.\n\n\n\n\nlibrary(patchwork) and library(tidyverse): We need to use {tidyverse} library functions to construct the plots and {patchwork} library functions to assemble the plots into a grid showing the plots next to each other. We use the library() function to get these libraries.\nWe construct two plots. We call the plots p.acc and p.self. Each plot is constructed in a similar way so we explain the main steps for one plot. The plots are constructed but not shown until the last line of code is run.\np.acc &lt;- ... creates a plot called p.acc, using the processes that are specified in the next lines of code that follow.\n... &lt;- study.one.gen tells R that we are working with the data-set study.one.gen that we read in earlier.\n%&gt;% is called the pipe and it is code telling R to work with the study.one.gen data in the next plot processing steps.\nggplot(aes(x = mean.acc)) + takes the study.one.gen data and asks R to use the ggplot() function to produce a plot.\naes(x = mean.acc) tells R that in the plot we want it to show the mean.acc variable values as locations on the x-axis: this is the aesthetic mapping.\ngeom_histogram(binwidth = .1) + tells R to produce a histogram then add a step, next.\ngeom_vline(...) + tells R we want to draw a vertical line.\nxintercept = mean(study.one.gen$mean.acc), ... tells R to draw the vertical line at the mean value of the variable mean.acc in the study.one.gen data-set.\nsize = 1.5, colour = \"red\" tells R we want the vertical line to be red, and 1.5 times the usual size.\nxlab(\"mean accuracy\") + tells R we want the x-axis label to say that the plot is of \"mean accuracy\".\nxlim(0, 1.1) + sets limits on the minimum and maximum mean accuracy values shown in the plot.\ntheme_bw() lastly, we change the theme.\nThen we have code to construct the second plot p.self.\np.acc + p.self: tells R to put the two plots together so they appear side-by-side in a grid for easy comparison.\n\nIn using pipes in the code, I am structuring the code so that it works — and is presented — in a sequence of steps. There are different ways to write code but I find this way easier to work with and to read and I think you will too.\nWhat you can see is that each line ending in a %&gt; pipe passes something on to the next line. A following line takes the output of the process coded in the preceding line, and works with it.\nEach step is executed in turn, in strict sequence. This means that if I delete the line study.one.gen %&gt;% then the following lines cannot work because the ggplot() function will be looking for a variable average that does not yet exist.\n\n\n\n\n\n\nTip\n\n\n\n\nYou can see that in the data processing part of the code, successive steps in data processing end in a pipe %&gt;%.\nIn contrast, successive steps of the plotting code add ggplot elements line by line with each line (except the last) ending in a +.\n\n\n\nNotice that none of the processing steps actually changes the dataset called study.one.gen. The results of the process exist and can be used only within the sequence of steps coded to produce the plots.\n\nYou can read a clear explanation of pipes here.\nYou can read about the new geom_vline() here.\n\n\n\n\n\n\n2.4.29 A histogram is a useful way to show the distribution of values\n\nWe have a sample of accuracy scores:\nMean accuracy scores vary between 0.0 and 1.0\nWe draw the plot by grouping together similar values in bins\nHeights of bars represent numbers of cases with similar values in same bin\n\n\nPlotNotes\n\n\n\n\n\n\n\n\n\n\nFigure 2.4: Distribution of mean accuracy\n\n\n\n\n\n\n\n\nstudy.one.gen %&gt;%\n           ggplot(aes(x = mean.acc)) + \n           geom_histogram(binwidth = .1) +\n           geom_vline(xintercept = mean(study.one.gen$mean.acc),\n                      size = 1.5, colour = \"red\") +\n           annotate(\"text\", x = 0.6, y = 60,\n                    colour = \"red\",\n                    label = \"average value\\nshown in red\") +\n           xlab(\"mean accuracy\") +\n           xlim(0, 1.1) +\n           theme_bw()\n\nWe go through the code line by line. It will be useful to identify some differences between this chunk of code and the previous chunk of code.\n\nWe are going to use {tidyverse} library functions to construct the plots and {patchwork} library functions to assemble the plots into a grid. I assume you have already run the library() function to get these libraries. We do not need to do it again in the same R session.\nWe construct one plot here. We do not give it a name. Run the code and R will show the plot in the plot window immediately.\nstudy.one.gen %&gt;% tells R that we are working with the data-set study.one.gen that we read in earlier.\n%&gt;% is the pipe and it is code telling R to work with the study.one.gen data in the next plot processing steps.\nggplot(aes(x = mean.acc)) + takes the study.one.gen data and asks R to use the ggplot() function to produce a plot.\naes(x = mean.acc) tells R that in the plot we want it to show the mean.acc variable values as locations on the x-axis: this is the aesthetic mapping.\ngeom_histogram(binwidth = .1) + tells R to produce a histogram then add a step, next.\ngeom_vline(...) + tells R we want to draw a vertical line.\nxintercept = mean(study.one.gen$mean.acc), ... tells R to draw the vertical line at the mean value of the variable mean.acc in the study.one.gen data-set.\nsize = 1.5, colour = \"red\" tells R we want the vertical line to be red, and 1.5 times the usual size.\nannotate(\"text\", ...** tells R to write some text.\nx = 0.6, y = 60** tells R where to write the text.\ncolour = \"red\"** tells R what colour to write the text.\nlabel = \"average value\\nshown in red\"** tells R what text to write.\nxlab(\"mean accuracy\") + tells R we want the x-axis label to say that the plot is about \"mean accuracy\".\nxlim(0, 1.1) + sets limits on the minimum and maximum mean accuracy values shown in the plot.\ntheme_bw() lastly, we change the theme.\n\n\nYou can read about the new annotate() function here.\n\n\n\n\n\n\n2.4.30 When we talk about variance we are talking about how values vary in relation to the mean for the sample\n\nThe average of these mean accuracy scores is marked with a red line where \\(\\bar{x} =\\) 0.8\nThe accuracy score for the person in row 1 is located at \\(x = .49\\), marked in blue\n\n\n\n\n\n\n\n\n\nFigure 2.5: Distribution of mean accuracy\n\n\n\n\n\n\n\n2.4.31 We are talking about how values vary in relation to the mean for the sample\n\nIn comparison, the mean accuracy score for the person in row 4 is located at \\(x = .94\\), marked in blue\n\n\n\n\n\n\n\n\n\nFigure 2.6: Distribution of mean accuracy\n\n\n\n\n\n\n\n2.4.32 The basic question when we examine covariance: do values vary together?\n\nIf the person at row 1 has a mean.accuracy score of .49, lower than the average\nAnd the person at row 4 has a mean.accuracy score of .94, higher than the average\nWhat will their mean.self scores be: will they be higher or lower than the average mean.self score?\n\n\n\n2.4.33 We can use scatterplots to examine associations\n\nIs variation in the mean accuracy of understanding (of health information) associated with variation in mean self-rated accuracy of understanding?\n\n\nPlotNotes\n\n\n\n\n\n\n\n\n\n\nFigure 2.7: Scatterplots showing whether values on mean accuracy (mean.acc) vary together with values on mean self-rated accuracy (mean.self) for the participants in this sample\n\n\n\n\n\n\n\n\np.acc &lt;- study.one.gen %&gt;%\n           ggplot(aes(x = mean.self, y = mean.acc)) +\n           geom_point(size = 2, alpha = .5) +\n           ylab(\"mean accuracy\") +\n           xlab(\"mean self-rated accuracy\") +\n           xlim(0, 10) + ylim(0, 1.1) +\n           theme_bw()\n\np.self &lt;- study.one.gen %&gt;%\n           ggplot(aes(y = mean.self, x = mean.acc)) +\n           geom_point(size = 2, alpha = .5) +\n           xlab(\"mean accuracy\") +\n           ylab(\"mean self-rated accuracy\") +\n           ylim(0, 10) + xlim(0, 1.1) +\n           theme_bw()\n\np.acc + p.self\n\n\nWe are going to use {tidyverse} library functions to construct the plots and {patchwork} library functions to assemble the plots into a grid. I assume you have already run the library() function to get these libraries. We do not need to do it again in the same R session.\nWe construct two plots. We call the plots p.acc and p.self again. Each plot is constructed in a similar way so we explain the main steps for one plot. The plots are constructed but not shown until the last line of code is run.\np.acc &lt;- ... creates a plot called p.acc, using the processes that are specified in the next lines of code that follow.\n... &lt;- study.one.gen tells R that we are working with the data-set study.one.gen that we read in earlier.\n%&gt;% is called the pipe and it is code telling R to work with the study.one.gen data in the next plot processing steps.\nggplot(...) + takes the study.one.gen data and asks R to use the ggplot() function to produce a plot.\naes(x = mean.self, ...) tells R that in the plot we want it to show the mean.self variable values as horizontal (left to right) locations on the x-axis: this is one aesthetic mapping.\naes(... , y = mean.acc) tells R that in the plot we want it to show the mean.acc variable values as vertical (low to high) locations on the y-axis: this is the second aesthetic mapping.\naes(x = mean.self, y = mean.acc) thus encodes two aesthetic mappings, telling R the position of the things it will draw (it will draw points, next): the vertical height, and the horizontal left-to-right position.\ngeom_point(...) tells R to produce a scatterplot, representing data values as points.\nsize = 2, alpha = .5 tells R we want the points to be 2 times the usual size with size = 2, and half the usual level of opacity (i.e. how solid the colour is) with alpha = .5.\nylab(\"mean accuracy\") + tells R we want the y-axis label to say that the plot is of \"mean accuracy\".\nxlab(\"mean self-rated accuracy\") + tells R we want the y-axis label to say that the plot is of \"mean self-rated accuracy\".\nylim(0, 10) + xlim(0, 1.1) + sets limits on the minimum and maximum mean accuracy and mean self-rated accuracy values shown in the plot.\ntheme_bw() changes the theme.\nThen we have code to construct the second plot p.self.\np.acc + p.self: tells R to put the two plots together so they appear side-by-side in a grid for easy comparison.\n\n\nYou can read about the geom_point() function here.\n\n\n\n\n\n\n2.4.34 A scatterplot is a useful way to examine if the values of two or more variables vary together\n\nMean accuracy scores vary between 0.0 and 1.0\n\n\nThe height of each point shows the observed value of accuracy on the y-axis\n\n\nSelf-rated accuracy scores vary between 1 and 9\n\n\nThe horizontal position of each point shows the observed value of self-rated accuracy on the x-axis\n\n\nPlotNotes\n\n\n\n\n\n\n\n\n\n\nFigure 2.8: Scatterplot showing how values on mean accuracy and mean self-rated accuracy vary together\n\n\n\n\n\n\n\n\nstudy.one.gen %&gt;%\n           ggplot(aes(x = mean.self, y = mean.acc)) +\n           geom_point(size = 2, alpha = .5) +\n           ylab(\"mean accuracy\") +\n           xlab(\"mean self-rated accuracy\") +\n           xlim(0, 10) + ylim(0, 1.1) +\n           theme_bw()\n\nWe use mostly the same code to draw Figure 2.8, compared to the code we used to draw Figure 2.7, but there is one difference. I do not walk through every line of code, here, but highlight the difference.\n\nWe construct one plot here. We do not give it a name. Run the code and R will show the plot in the plot window immediately.\nstudy.one.gen %&gt;% tells R that we are working with the data-set study.one.gen that we read in earlier.\nggplot(aes(x = mean.self, y = mean.acc)) + takes the study.one.gen data and asks R to use the ggplot() function to produce a plot.\naes(x = mean.self, y = mean.acc) thus encodes two aesthetic mappings, telling R the position of the things it will draw (it will draw points, next): the vertical height, and the horizontal left-to-right position.\n\n\n\n\nWe can focus in one point, showing the data for one person.\n\nWe have a sample of 170 people\nFor each person, we have a value for the mean accuracy and a paired value for the mean self-rated accuracy\nEach point shows the paired data values for a person\nIn red: someone scored 3.48 on mean self-rated accuracy, 0.57 on mean accuracy\n\n\n\n\n\n\n\n\n\nFigure 2.9: Scatterplot showing how values on mean accuracy and mean self-rated accuracy vary together\n\n\n\n\n\n\n\n2.4.35 The R code for a correlation test, bit by bit\n\ncor.test(study.one.gen$mean.acc,\n         study.one.gen$mean.self,\n         method = \"pearson\")\n\n\nWe specify the cor.test function, and name one variable study.one.gen$mean.acc\nThen we name the second variable study.one.gen$mean.self\nLast we specify the correlation method = \"pearson\" because we have a choice (we can apply other methods to estimate the correlation, e.g., Spearmans)\n\n\n\n2.4.36 Identifying the key information in the results from one correlation test\n\nWe look at the value of the correlation (here, cor) and the p-value\nWe can see that the correlation statistic is positive cor = .4863771 which we round to \\(cor = .49\\)\nAnd p-value = 2.026e-11 indicating that the correlation is significant \\(p &lt; .001\\)\n\n\ncor.test(study.one.gen$mean.acc,\n         study.one.gen$mean.self,\n         method = \"pearson\")\n\n\n    Pearson's product-moment correlation\n\ndata:  study.one.gen$mean.acc and study.one.gen$mean.self\nt = 7.1936, df = 167, p-value = 2.026e-11\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3619961 0.5937425\nsample estimates:\n      cor \n0.4863771 \n\n\n\n\n2.4.37 Reporting a correlation\n\nUsually, we report a correlation like this:\n\n\nMean accuracy and mean self-rated accuracy were significantly correlated (\\(r (167 \\text{ df}) = .49, p &lt; .001\\)). Higher mean accuracy scores are associated with higher mean self-rated accuracy scores.\n\n\n\n2.4.38 Interpreting correlations with the help of visualization\n\nThe correlation statistic is positive in sign and moderate in size, about \\(r = .49\\)\nWe can see that higher mean accuracy (mean.acc) scores are associated with higher mean self-rated accuracy (mean.self) scores\n\n\n\n\n\n\n\n\n\nFigure 2.10: Scatterplot showing how values on mean accuracy and mean self-rated accuracy vary together\n\n\n\n\n\n\n\n2.4.39 What will different kinds of correlations look like?\nWe can simulate data to demonstrate [what scatterplots look like if]: (left) the correlation is positive, \\(r = .5\\); (right) the correlation is negative, \\(r = -.5\\)\n\n\n\n\n\n\n\n\nFigure 2.11: Scatterplots showing how simulated data values on mean accuracy and mean self-rated accuracy could vary together given positive or negative correlations\n\n\n\n\n\n\n\n2.4.40 We can also imagine – again with simulated data – what correlations of increasing size might look like\n\nNotice how, as you compare the plots, going from left to right\nAs the correlation increases, the points cluster together more closely\n\n\n\n\n\n\n\n\n\nFigure 2.12: Scatterplots showing how simulated data values on mean accuracy and mean self-rated accuracy could vary together given positive correlations of increasing size",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Hypotheses and associations</span>"
    ]
  },
  {
    "objectID": "hypotheses-associations.html#summary",
    "href": "hypotheses-associations.html#summary",
    "title": "2  Hypotheses and associations",
    "section": "2.5 Summary",
    "text": "2.5 Summary\n\nWe are often interested in whether or how variation in the values of two variables are associated\nWe can visualize the distribution of values in any one variable using histograms\nWe visualize the association of values in two variables using scatterplots\nWe conduct correlation tests to examine the sign (positive or negative) and the strength of the association\nBut we always need to think about our research questions, about where our data come from and about whether our measures are any good\n\n\n2.5.1 Look ahead: growing in independence\n\nEvery problem you ever have: someone has had it before, solved it, and written a blog (or tweet or toot) about it\n\n\n\n2.5.2 Look ahead: the revolution in knowledge and you\n\nR is free open statistical software: everything you use is contributed, discussed and taught by a community of R users online, in open forums\nLearning to navigate this knowledge is an introduction to the future of knowledge sharing\n\n\n\n\n\nBorsboom, D., Mellenbergh, G. J., & Heerden, J. van. (2004). The concept of validity. Psychological Review, 111(4), 1061–1071. https://doi.org/10.1037/0033-295X.111.4.1061\n\n\nFreed, E. M., Hamilton, S. T., & Long, D. L. (2017). Comprehension in proficient readers: The nature of individual variation. Journal of Memory and Language, 97, 135–153. https://doi.org/10.1016/j.jml.2017.07.008\n\n\nGigerenzer, G. (2004). Mindless statistics. The Journal of Socio-Economics, 33(5), 587–606. https://doi.org/10.1016/j.socec.2004.09.033\n\n\nKintsch, W. (1994). Text comprehension, memory, and learning. American Psychologist, 49(4), 294–303. https://doi.org/10.1037/0003-066x.49.4.294\n\n\nMeehl, P. E. (1990). Why summaries of research on psychological theories are often uninterpretable. Psychological Reports, 66(1), 195–244. https://doi.org/10.2466/pr0.1990.66.1.195\n\n\nScheel, A. M. (2022). Why most psychological research findings are not even wrong. Infant and Child Development, 31(1), e2295. https://doi.org/10.1002/icd.2295\n\n\nScheel, A. M., Tiokhin, L., Isager, P. M., & Lakens, D. (2021). Why Hypothesis Testers Should Spend Less Time Testing Hypotheses. Perspectives on Psychological Science, 16(4), 744–755. https://doi.org/10.1177/1745691620966795\n\n\nSmaldino, P. (2019). Better methods can’t make up for mediocre theory. Nature, 575(7781), 9.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Hypotheses and associations</span>"
    ]
  },
  {
    "objectID": "hypotheses-associations.html#footnotes",
    "href": "hypotheses-associations.html#footnotes",
    "title": "2  Hypotheses and associations",
    "section": "",
    "text": "We write the slides and this book in Quarto in R-Studio. Quarto scripts can be rendered as .html to share web-books like this one, or to share slides like those we use in presenting the lecture. One advantage of using Quarto is that we can share a plot and the code we used to generate the plot in the same page.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Hypotheses and associations</span>"
    ]
  },
  {
    "objectID": "visualization-intro.html",
    "href": "visualization-intro.html",
    "title": "3  Introduction to visualization",
    "section": "",
    "text": "3.1 Overview\nWelcome to our overview of the materials you will work with in our data visualization class in PSYC401 Week 8. This week, we will focus on perspectives and practices in data visualization.\nOur materials are designed to help you to think about what you are doing, to understand the aims of the practical steps, as well as to learn about producing professional effective data visualizations.\nWe will continue to work with data collected for the Clearly understood project because we think that working with these data in this research context will help you to make sense of the data, and to see why we ask you to practise the skills we are teaching.\nYou can read a bit more about the project and the project data in Chapter 2.\nAs we work together, we will be revisiting some of the ideas and techniques you have seen in previous classes. This is to give you the opportunity to consolidate your learning. We will be extending your development with some new ideas to strengthen your skills.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to visualization</span>"
    ]
  },
  {
    "objectID": "visualization-intro.html#sec-vis-intro-goals",
    "href": "visualization-intro.html#sec-vis-intro-goals",
    "title": "3  Introduction to visualization",
    "section": "3.2 Our learning goals",
    "text": "3.2 Our learning goals\nThis week, we focus on both developing your critical thinking and strengthening your practical skills in data visualization.\nOur learning objectives: — what are we learning about?\nWe are working together to help you:\n\nGoals — Formulate questions you can ask yourself to help you to work effectively\nAudience — Understand the psychological factors that affect your impact\nDevelopment — Work reflectively through a development process\nImplement — Produce visualizations in line with best practice\n\nOur assessment targets: — how do you know if you have learned?\nWe are working together so you can:\n\nGoals — Identify a set of targets for a development process in your professional teams\nAudience — Explain what you need to do to make a visualization effective\nDevelopment — Locate yourself within the stages of the development process\nImplement — Produce visualizations that look good and are useful",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to visualization</span>"
    ]
  },
  {
    "objectID": "visualization-intro.html#sec-vis-intro-resources",
    "href": "visualization-intro.html#sec-vis-intro-resources",
    "title": "3  Introduction to visualization",
    "section": "3.3 Learning resources",
    "text": "3.3 Learning resources\nYou will see next links to the lectures we created to explain the concepts we want you to learn about and the practical visualization skills we want you to develop (Section 3.3.1), then information about the practical materials we have provided to help you to practise your skills (Section 3.3.2).\nAll the links to the lecture videos, the lecture slides, and everything you need for your practical work can also be found in the Week 8 files folder on Moodle here\nIn Section 3.4, we present the lecture slide points. We do this here because we can share the code we used to generate the plots we use in some of the slides 1\n\n\n\n\n\n\nTip\n\n\n\nLinked resources include:\n\nIn Chapter 4, we present a more extensive discussion of data visualization, elaborating on some ideas, incorporating additional example plots and enabling you to work with alternate data-sets. This is provided as optional reading and may support more advanced development for students interested in future professional roles involving data analysis or data visualization.\nIn the PSYC403 Week 8, I present a lecture that outlines some perspectives or useful ways of thinking about data visualization: the history; the context in professional work; and what research suggests are effective ways to produce visualizations. You can access the resources for that class here.\n\n\n\n\n3.3.1 Lectures\nThe lecture material for this week is presented in four short parts. Click on a link and your browser should open a tab showing the Panopto video for the lecture part.\nPart 1 of 4\nPart 2 of 4\nPart 3 of 4\nPart 4 of 4\nYou can download the slides we presented in the lecture in two different formats, depending on what you think will be most useful to you:\n\nDownload the slides exactly as they appear in the lecture from this link. The .html file can be opened and viewed in any web browser (e.g., Chrome, Firefox, Safari).\nOr you can download a printable Word .docx presentation of the slides from this link. The .docx can be opened in Microsoft Word. The figures will not appear exactly as they do in the lecture recording because Word cannot cope with images so well but the trade-off is that you get a document you can print and edit to add notes.\n\n\n\n3.3.2 Practical materials\nWe have collected the practical materials together into a folder.\nThe folder includes the data files:\n\nstudy-one-general-participants.csv\nstudy-two-general-participants.csv\n\nand .R code files:\n\n401-visualization-how-to.R\n401-visualization-workbook.R\n\nYou will use these files for your practical learning.\nYou can download the .R files and the data .csv files in a single folder, using the link here.\nOr you can download the files as individual files from the module Moodle page for PSYC401.\nOnce you have downloaded the file folder, you will need to upload it to the R-Studio server to access and use the R files.\n\n3.3.2.1 The how-to guide\nIn the how-to guide:\n\n401-visualization-how-to.R\n\nwe show you how to do everything you need to do in the practical workbook (see Section 3.3.3). The guide comprises an .R file 401-visualization-how-to.R with code and advice.\nThe code in the .R file was written to work with the data file\n\nstudy-one-general-participants.csv.\n\n\n\n\n\n\n\nTip\n\n\n\n\nWork through the steps in the how-to guide first, this practice will help you to understand what you need to do for the workbook tasks.\nThe how-to guide and the workbook have similar structures. This is intentional: so that you can copy and adapt code from the how-to guide to do the practical tasks in the workbook.\n\n\n\n\n\n\n3.3.3 The workbook\nIn the workbook:\n\n401-visualization-workbook.R\n\nyou will work with the data file\n\nstudy-two-general-participants.csv\n\nWe split .R scripts into parts, tasks and questions.\nFor this class on data visualization practices, our practical materials have two aims:\n\nHelping you to learn about data visualization;\nHelping you to learn how to help yourself by accessing, evaluating and exploiting the rich R knowledge ecosystem.\n\nThis means that in both the how-to and the workbook the different parts concern different sources of online information and how to access each.\nIn the how-to and the workbook we look at methods to produce visualizations that display information about both summary statistics (e.g., the average outcome) and raw or individual outcome variability. We explain why we should visualize data like this in the lecture (as you can see in Section 3.3.1 or Section 3.4).\nSpecifically, the materials are written to support learning how to work with visualizations called boxplots and rain cloud plots. These are popular data visualization techniques so it is important to learn how to build them and how to interpret them. If you have already been introduced to them, you will find that our materials here show how to edit or polish the visualizations to make them more effective.\nThere are three main sources of information you can access for free online.\n\nThe people who write software like the {tidyverse} or {ggplot2} libraries provide manuals, reference guides and tutorials. This information is often written as free web books, or as hard copy books.\nThese people, and often ther people, may write tutorials or guides or teaching materials designed to show learners (like us) how to use R functions or do certain things using R. They may present these tutorials or guides as web books, blog sites or video tutorials e.g. on Youtube or TikTok.\nMany post questions and answers to discussion forums like Stackoverflow.\n\nLearning how to find, understand and use this information teaches two lessons:\n\nA lot of scholarly and technical information is online and free.\nLearning how to access this information is a key way that most professionals work out what they want to do and how they can do it.\n\nIn our practical materials, we look at how you can:\n\nsearch for and find relevant information;\nlearn how to understand the tools using the information shared through online sources;\nwork with example or demonstration code, adapting it for your own purposes.\n\n\n\n\n\n\n\nTip\n\n\n\nThis process of adapting demonstration code is a process critical to data literacy and to effective problem solving in working with data in psychological science.\n\n\nPart 3 in the workbook focuses on locating and using {ggplot2} reference information to learn how to build box plots.\nPart 4 in the workbook focuses on locating and using online tutorial information or how-to guides to build rain cloud plots. This kind of plot incorporates elements of the boxplot, the scatterplot and the density plot to give the viewer summary information about the distribution of scores on a variable while also giving information about the variability of outcomes.\nPart 5 in the workbook focuses on using information available in the public discussion Stackoverflow so that you can learn how to export the plots you make, in order to include them in reports.\nThe activity 401-visualization-workbook.R file takes you through the tasks, one by one.\nIf you are unsure about what you need to do, check the advice in 401-visualization-how-to.R.\nYou will see that you can match a task in the activity to the same task in the how-to. The how-to shows you what function you need and how you should write the function code. You will need to change the names of the data-set or the variables to complete the tasks in the activity.\n\n\n3.3.4 The data files\nEach of the data files we will work with has a similar structure.\nHere are what the first few rows in the data file study-two-general-participants.csv looks like:\n\n\n\n\n\nparticipant_ID\nmean.acc\nmean.self\nstudy\nAGE\nSHIPLEY\nHLVA\nFACTOR3\nQRITOTAL\nGENDER\nEDUCATION\nETHNICITY\n\n\n\n\nstudytwo.1\n0.4107143\n6.071429\nstudytwo\n26\n27\n6\n50\n9\nFemale\nHigher\nAsian\n\n\nstudytwo.10\n0.6071429\n8.500000\nstudytwo\n38\n24\n9\n58\n15\nFemale\nSecondary\nWhite\n\n\nstudytwo.100\n0.8750000\n8.928571\nstudytwo\n66\n40\n13\n60\n20\nFemale\nHigher\nWhite\n\n\nstudytwo.101\n0.9642857\n8.500000\nstudytwo\n21\n31\n11\n59\n14\nFemale\nHigher\nWhite\n\n\nstudytwo.102\n0.7142857\n7.071429\nstudytwo\n74\n35\n7\n52\n18\nMale\nHigher\nWhite\n\n\nstudytwo.103\n0.7678571\n5.071429\nstudytwo\n18\n40\n11\n54\n15\nFemale\nFurther\nWhite\n\n\n\n\n\n\n\nYou can see the columns:\n\nparticipant_ID participant code\nmean.acc average accuracy of response to questions testing understanding of health guidance\nmean.self average self-rated accuracy of understanding of health guidance\nstudy variable coding for what study the data were collected in\nAGE age in years\nHLVA health literacy test score\nSHIPLEY vocabulary knowledge test score\nFACTOR3 reading strategy survey score\nGENDER gender code\nEDUCATION education level code\nETHNICITY ethnicity (Office National Statistics categories) code\n\n\n\n3.3.5 The answers\nAfter the practical class, you will be able to download the answers version of the workbook here.\n\nThe answers version will present my answers for questions, and some extra information where that is helpful.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to visualization</span>"
    ]
  },
  {
    "objectID": "visualization-intro.html#sec-vis-intro-notes",
    "href": "visualization-intro.html#sec-vis-intro-notes",
    "title": "3  Introduction to visualization",
    "section": "3.4 Lecture notes",
    "text": "3.4 Lecture notes\nSome people find it easier to read notes than to watch video recordings. This is why we also include the lecture notes here.\n\n\n\n\n\n\nTip\n\n\n\nIn these notes, I provide notes on the code steps that result in plots.\n\nClick on the Notes tab to see them.\n\n\n\n\n3.4.1 Our lesson plan\n\nIdentify your goals\nThink about your audience\nDevelop reflectively\nImplement good practice\n\n\n\n3.4.2 Our learning objectives: — what are we learning about?\nWe are working together to help you:\n\nGoals — Formulate questions you can ask yourself to help you to work effectively\nAudience — Understand the psychological factors that affect your impact\nDevelopment — Work reflectively through a development process\nImplement — Produce visualizations in line with best practice\n\n\n\n3.4.3 Our assessment targets: — how do you know if you have learned?\nWe are working together so you can:\n\nGoals — Identify a set of targets for a development process in your professional teams\nAudience — Explain what you need to do to make a visualization effective\nDevelopment — Locate yourself within the stages of the development process\nImplement — Produce visualizations that look good and are useful\n\n\n\n3.4.4 What are our goals – Questions to help you to work effectively\n\n\n\n\n\n\n\nTip\n\n\n\n\nWe begin by thinking about the questions you will ask yourself when you need to decide what you will do\nWe build, here, on the insights developed by A. Gelman & Unwin (2013).\n\n\n\n\n\n3.4.5 What are our goals?\n\nWhy don’t we just use the good enough easy to produce plots in Excel? \\(\\rightarrow\\) Why bother?\nWhy don’t we just produce a summary table? \\(\\rightarrow\\) Why bother?\nAre we engaged in making beautiful graphics or informative displays or both? \\(\\rightarrow\\) What are we doing?\nIn PSYC403, we look at \\(\\rightarrow\\) Perspectives: the context and history of thinking about visualization\n\n\n\n3.4.6 Visualize to enable comparison\n\n\n\n\n\n\n\n\n\nFigure 3.1: Scatterplot of the relation between reaction time and days in the sleepstudy data\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.2: The relation between reaction time and days: here, we plot the data for each participant separately\n\n\n\n\n\n\n\n3.4.7 What are our goals? — What are our jobs?\n\nData visualization workers: we may aim to get and keep the attention of our audience, to tell a story, to persuade our viewers\nData analysis workers: we may aim to enable our audience to understand our data, our findings, and to discover more for themselves\n\n\n\n3.4.8 What are our goals? — Where or when are we in our process?\n\nSometimes in a workflow, we are quickly sketching draft visualizations: exploring, for ourselves, or with others, what we can see in our data\nSometimes, we are ready to present our visualization to a wider audience: we aim to share a polished visual object\n\n\n\n3.4.9 What are our goals? — Discovery\nDiscovery goals\n\nDo we need an overview? – To get a sense of what is in the data, and to check our assumptions\nAre we looking for the unexpected? – Comparing groups to check for variability, exploring data open to surprises\n\n\n\n3.4.10 What are our goals? — Communication\nCommunication goals\n\nWhat do we need our audience to understand?\nWhat story are we telling?\nDo we need to attract attention or stimulate interest?\n\n\n\n3.4.11 Think about your audience – An evidence based account of what works\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nWe will produce more effective visualizations if we think about how our audience sees, and what they expect (Franconeri et al., 2021)\nCheck out the PSYC403 Perspectives lecture for more in-depth explanation; here, I present a selective summary\n\n\n\n\n\n3.4.12 The human visual system is highly developed\n\nYour audience can look at your visualization\nAnd quickly and easily extract statistical information from what you show\nYou look at a scatterplot and see the minimum, maximum and mean heights of the points\n\n\n\n\nFranconeri et al. (2021) Fig. 2\n\n\n\n\n3.4.13 Communicating uncertainty is critical\n\n\nAs scientists, we think about uncertainty all the time\nWe quantify and typically show uncertainty over estimates e.g. average differences\nWe should also show and think about outcome variability\n\n\n\n\nZhang et al. (2023): The difference between uncertainty over estimates and uncertainty over the predictability of outcomes\n\n\n\n\n3.4.14 Consider accessibility from the start\n\nThe first row shows a scatterplot encoded with two colors, green and orange\nPeople with typical vision can see that the green dots have a steep positive correlation and the orange dots make a flat line\nWe use colour blindness friendly colour palettes\n\n\n\n\nFranconeri et al. (2021) Fig.5\n\n\n\n\n3.4.15 Development – Work reflectively through a development process\n\n\n\n\n\n\nTip\n\n\n\n\nYour first question is always going to be: (why) do we need to make a plot?\nYour answer will evolve through a development process that will gradually reveal the characteristics of your data\n\n\n\n\n\n3.4.16 The benefits of investing in the development process\n\nIdentifying your goals enables you to understand what you are doing and why\nThrough the development process, you may create different versions — iterations — of a plot\nThis iterative work benefits both you and your audience (A. Gelman et al., 2002; Kastellec & Leoni, 2007)\n\n\n\n3.4.17 The benefits of investing in the development process\n\n\n\n\n\n\nTip\n\n\n\n\nAs you iterate, reflect on what your goals are, what your audience needs and expects, and how each plot version moves you closer to effective discovery or communication\nThis reflection uncovers what is interesting, useful and beautiful about your data\n\n\n\n\n\n3.4.18 Scientific thinking and data visualisation\nWe can use text and tables to communicate specific values but visualizations help us to:\n\nstimulate thinking\ndiscover what is unexpected\ncommunicate scale and complexity\nmake comparisons to show how results vary\ndisplay uncertainty about estimates\n\n\n\n3.4.19 Anscombe (1973): visualizations show data features quickly and vividly\n\ndata columnsx-variablesy-variables\n\n\n\n\n\n\n\n\n\n\nx1\n\n\nx2\n\n\nx3\n\n\nx4\n\n\ny1\n\n\ny2\n\n\ny3\n\n\ny4\n\n\n\n\n\n\n10\n\n\n10\n\n\n10\n\n\n8\n\n\n8.04\n\n\n9.14\n\n\n7.46\n\n\n6.58\n\n\n\n\n8\n\n\n8\n\n\n8\n\n\n8\n\n\n6.95\n\n\n8.14\n\n\n6.77\n\n\n5.76\n\n\n\n\n13\n\n\n13\n\n\n13\n\n\n8\n\n\n7.58\n\n\n8.74\n\n\n12.74\n\n\n7.71\n\n\n\n\n9\n\n\n9\n\n\n9\n\n\n8\n\n\n8.81\n\n\n8.77\n\n\n7.11\n\n\n8.84\n\n\n\n\n11\n\n\n11\n\n\n11\n\n\n8\n\n\n8.33\n\n\n9.26\n\n\n7.81\n\n\n8.47\n\n\n\n\n14\n\n\n14\n\n\n14\n\n\n8\n\n\n9.96\n\n\n8.10\n\n\n8.84\n\n\n7.04\n\n\n\n\n6\n\n\n6\n\n\n6\n\n\n8\n\n\n7.24\n\n\n6.13\n\n\n6.08\n\n\n5.25\n\n\n\n\n4\n\n\n4\n\n\n4\n\n\n19\n\n\n4.26\n\n\n3.10\n\n\n5.39\n\n\n12.50\n\n\n\n\n12\n\n\n12\n\n\n12\n\n\n8\n\n\n10.84\n\n\n9.13\n\n\n8.15\n\n\n5.56\n\n\n\n\n7\n\n\n7\n\n\n7\n\n\n8\n\n\n4.82\n\n\n7.26\n\n\n6.42\n\n\n7.91\n\n\n\n\n5\n\n\n5\n\n\n5\n\n\n8\n\n\n5.68\n\n\n4.74\n\n\n5.73\n\n\n6.89\n\n\n\n\n\n\nFigure 3.3: Data table view of Anscombe’s Quartet dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx1\n\n\nx2\n\n\nx3\n\n\nx4\n\n\n\n\n\n\n\n\nMin. : 4.0\n\n\nMin. : 4.0\n\n\nMin. : 4.0\n\n\nMin. : 8\n\n\n\n\n\n\n1st Qu.: 6.5\n\n\n1st Qu.: 6.5\n\n\n1st Qu.: 6.5\n\n\n1st Qu.: 8\n\n\n\n\n\n\nMedian : 9.0\n\n\nMedian : 9.0\n\n\nMedian : 9.0\n\n\nMedian : 8\n\n\n\n\n\n\nMean : 9.0\n\n\nMean : 9.0\n\n\nMean : 9.0\n\n\nMean : 9\n\n\n\n\n\n\n3rd Qu.:11.5\n\n\n3rd Qu.:11.5\n\n\n3rd Qu.:11.5\n\n\n3rd Qu.: 8\n\n\n\n\n\n\nMax. :14.0\n\n\nMax. :14.0\n\n\nMax. :14.0\n\n\nMax. :19\n\n\n\n\n\n\nFigure 3.4: Summary table view of descriptive statistics for x variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ny1\n\n\ny2\n\n\ny3\n\n\ny4\n\n\n\n\n\n\n\n\nMin. : 4.260\n\n\nMin. :3.100\n\n\nMin. : 5.39\n\n\nMin. : 5.250\n\n\n\n\n\n\n1st Qu.: 6.315\n\n\n1st Qu.:6.695\n\n\n1st Qu.: 6.25\n\n\n1st Qu.: 6.170\n\n\n\n\n\n\nMedian : 7.580\n\n\nMedian :8.140\n\n\nMedian : 7.11\n\n\nMedian : 7.040\n\n\n\n\n\n\nMean : 7.501\n\n\nMean :7.501\n\n\nMean : 7.50\n\n\nMean : 7.501\n\n\n\n\n\n\n3rd Qu.: 8.570\n\n\n3rd Qu.:8.950\n\n\n3rd Qu.: 7.98\n\n\n3rd Qu.: 8.190\n\n\n\n\n\n\nMax. :10.840\n\n\nMax. :9.260\n\n\nMax. :12.74\n\n\nMax. :12.500\n\n\n\n\n\n\nFigure 3.5: Summary table view of descriptive statistics for y variables\n\n\n\n\n\n\n\n\n\n3.4.20 Anscombe (1973): visualizations show data features quickly and vividly\n\n\n\n\n\n\n\n\nFigure 3.6: All 4 of the Anscombe (1973) x,y datasets are identical when examined using summary statistics but we see how they vary when we use scatterplots to visualize them\n\n\n\n\n\n\n\n3.4.21 Matejka & Fitzmaurice (2017) give us the Datasaurus dozen\n\n\n\n\n\n\n\n\nFigure 3.7: All 12 Matejka & Fitzmaurice (2017) x,y datasets (via jumpingrivers (n.d.)) have the same mean and standard deviation summary statistics but we only understand how the data are structured when we plot them and can look at the structure\n\n\n\n\n\n\n\n3.4.22 Develop visualizations to discover and communicate variability in outcomes\n\n\n\n\n\n\n\n\nFigure 3.8: In this plot we show data on the impact of sleep deprivation on reaction time, from Belenky et al. (2003; via Bates et al., 2015). We can see how reaction time slows with increasing deprivation on average (grey line) but that the rate of slowing varies between individuals\n\n\n\n\n\n\n\n3.4.23 Reflect on kinds of uncertainty\n\nScientists are often faced with the challenge of conveying uncertainty to their audiences (Hofman et al., 2020):\n\n\nInferential uncertainty — the degree to which a particular summary statistic (e.g., a population mean) is known to the scientist\nOutcome uncertainty — how much individual outcomes vary (e.g., around the mean, regardless of how well it has been estimated)\n\n\nInferential uncertainty can be reduced by collecting and analyzing more data, whereas outcome uncertainty cannot\n\n\n\n\n\n3.4.24 As we work, reflect on the challenges of visualizing uncertainty\n\nThe process through which we understand the world is characterized by assumptions, limitations, extrapolations, and generalizations, and this brings uncertainty (Van Der Bles et al., 2019)\nWe often face the challenge of communicating this\n\n\n\n\nZhang et al. (2023): The difference between uncertainty over estimates and uncertainty over the predictability of outcomes\n\n\n\n\n3.4.25 The challenges of uncertainty\n\nNon-expert people will tend to overstate the impact of interventions and understate the variability of outcomes\nwhen they see visualizations like error bars that show\nmean and standard error values, that focus on inferential uncertainty (Hofman et al., 2020)\n\n\n\n\nZhang et al. (2023): The difference between uncertainty over estimates and uncertainty over the predictability of outcomes\n\n\n\n\n3.4.26 The challenges of uncertainty\n\nExpert scientists also overestimate the impact of interventions when they see standard visualizations that focus on inferential uncertainty: the illusion of predictability\nWe can stimulate more accurate understanding if we show outcome variability (Zhang et al., 2023)\n\n\n\n\nZhang et al. (2023): The difference between uncertainty over estimates and uncertainty over the predictability of outcomes\n\n\n\n\n3.4.27 Variation and uncertainty — the importance, the challenges\nVasishth & Gelman (2021):\n\nThe most difficult idea to digest in data analysis is that conclusions based on data are almost always uncertain, regardless of whether the outcome of the statistical test is statistically significant or not\n\n\n\n3.4.28 Variation and uncertainty — the importance, the challenges\na. Gelman (2015):\n\nWe must move beyond the idea that effects are ‘there’ or not and the idea that the goal of a study is to reject a null hypothesis. As many observers have noted, these attitudes lead to trouble because they deny the variation inherent in real social phenomena, and they deny the uncertainty inherent in statistical inference\n\n\n\n3.4.29 We use visualizations to help us to see and understand the variation and the uncertainty in our data\n\nResults will vary: we should expect changes over time, or differences between individuals or between groups\nKnowledge is uncertain: outcomes will vary even when the average effect is precisely estimated\nWe have the responsibility to accept and to express this uncertainty\n\n\n\n3.4.30 Implement – Produce visualizations in line with best practice\n\n\n\n\n\n\nTip\n\n\n\n\nWe combine our creative thinking with the flexibility of the Grammar of Graphics to produce effective plots\n\n\n\n\n\n3.4.31 {ggplot2} means: the Grammar of Graphics Plot 2\n\nWhen we use the {ggplot2} to draw plots, we are using tools developed with a philosophy of visualization in mind (Wilkinson, 2013; wickham2010?): The Grammar of Graphics\nA grammar is a system of rules that allows people to collaborate and individuals to create\nWe do not need to think about the grammar when we produce visualizations\nBut it will help you to know that when we puzzle over how we do things, there are always reasons why we do things\n\n\n\n3.4.32 A simple plot has many elements\n\ndata and aesthetic mappings\nstatistical transformations\ngeometric objects\nscales\n\n\n\n\n\n\n\n\n\nFigure 3.9: A scatterplot showing the potential association between health literacy and vocabulary\n\n\n\n\n\n\n\n3.4.33 We begin with data: here, from the Health Comprehension project\n\n\n\n\n\n\n\n\nparticipant_ID\n\n\nmean.acc\n\n\nmean.self\n\n\nstudy\n\n\nAGE\n\n\nSHIPLEY\n\n\nHLVA\n\n\nFACTOR3\n\n\nQRITOTAL\n\n\nGENDER\n\n\nEDUCATION\n\n\nETHNICITY\n\n\n\n\n\n\nstudyone.1\n\n\n0.49\n\n\n7.96\n\n\nstudyone\n\n\n34\n\n\n33\n\n\n7\n\n\n53\n\n\n11\n\n\nNon-binary\n\n\nHigher\n\n\nWhite\n\n\n\n\nstudyone.10\n\n\n0.85\n\n\n7.28\n\n\nstudyone\n\n\n25\n\n\n33\n\n\n7\n\n\n60\n\n\n11\n\n\nFemale\n\n\nHigher\n\n\nWhite\n\n\n\n\nstudyone.100\n\n\n0.82\n\n\n7.36\n\n\nstudyone\n\n\n43\n\n\n40\n\n\n8\n\n\n46\n\n\n12\n\n\nMale\n\n\nFurther\n\n\nWhite\n\n\n\n\nstudyone.101\n\n\n0.94\n\n\n7.88\n\n\nstudyone\n\n\n46\n\n\n33\n\n\n11\n\n\n51\n\n\n15\n\n\nMale\n\n\nHigher\n\n\nWhite\n\n\n\n\nstudyone.102\n\n\n0.58\n\n\n6.96\n\n\nstudyone\n\n\n18\n\n\n32\n\n\n3\n\n\n51\n\n\n12\n\n\nMale\n\n\nSecondary\n\n\nMixed\n\n\n\n\nstudyone.103\n\n\n0.84\n\n\n7.88\n\n\nstudyone\n\n\n19\n\n\n37\n\n\n13\n\n\n45\n\n\n19\n\n\nFemale\n\n\nFurther\n\n\nAsian\n\n\n\n\n\n\nFigure 3.10: Data table view of Health Comprehension project Study One dataset\n\n\n\n\n\n\n3.4.34 A simple plot has many elements\n\nPlot with no objectsCode for the plot\n\n\n\nWhen we code a plot, we tell R we want:\nto use ggplot() to create a plot\nusing the data-set clearly.one.subjects\nand the variables SHIPLEY, HLVA\n\n\n\n\n\n\n\n\n\nFigure 3.11: Scatterplot showing association between health literacy and vocabulary\n\n\n\n\n\n\n\n\n  ggplot(data = clearly.one.subjects, aes(x = SHIPLEY, y = HLVA))\n\n\nWe bring the data-set and the variables\nWe declare the aesthetic mappings:\n\n\nSHIPLEY score \\(\\rightarrow\\) x-axis (horizontal: left-to-right position)\nHLVA score \\(\\rightarrow\\) y-axis (vertical: bottom-to-top position)\n\n\n\n\n\n\n3.4.35 A simple plot has many elements\n\nPlot with only objectsCode for the plot\n\n\n\nWhen we code a plot, we tell R we want:\nto use a geometric object, like geom_point\nto display the data aesthetic mappings\n\n\n\n\n\n\n\n\n\nFigure 3.12: Scatterplot showing association between health literacy and vocabulary\n\n\n\n\n\n\n\n\n  ggplot(data = clearly.one.subjects, aes(x = SHIPLEY, y = HLVA)) +\n  geom_point()\n\n\nWe add the geom_point() to tell R to draw the information about the SHIPLEY and HLVA scores as points\nEach point represents information about one participant in the clearly.one.subjects data-set\n\n\nSHIPLEY score \\(\\rightarrow\\) x-axis (horizontal: left-to-right position)\nHLVA score \\(\\rightarrow\\) y-axis (vertical: bottom-to-top position)\n\n\n\n\n\n\n3.4.36 When we use {ggplot2} we work in layers\n\nThe grammar of graphics define the components of a plot: the data, the mappings, and the geometric object\nTogether, the data, mappings, and geometric object form a layer\nA plot may have multiple layers\n\n\n\n3.4.37 When we use {ggplot2} we are in control and we can be creative\n\n\n\n\n\n\nTip\n\n\n\n\nHaving a system of graphics: with components, layers and rules\nReleases us to be creative: changing a single feature at a time\n\n\n\n\n\n3.4.38 Plot with layers: add a smoother\n\nPlot with smootherCode for the plot\n\n\n\nBuild a plot layer by layer\nWe can begin by using points to display the vocabulary and health literacy scores for each person\nWe add a layer using a smoother to show the average association between vocabulary and literacy\n\n\n\n\n\n\n\n\n\nFigure 3.13: Scatterplot showing association between health literacy and vocabulary\n\n\n\n\n\n\n\n\n  ggplot(data = clearly.one.subjects, aes(x = SHIPLEY, y = HLVA)) +\n  geom_point() +\n  geom_smooth()\n\n\nWe add the geom_smooth() to tell R to represent the average trend for the association between SHIPLEY and HLVA scores\nThe line is drawn by {ggplot2} which calculates a statistical transformation\nHere, the transformation summarizes the association for different ranges of SHIPLEY vocabulary scores\n\n\n\n\n\n\n3.4.39 Defaults and arguments\n\nclearly.one.subjects %&gt;%\n  ggplot(aes(SHIPLEY, HLVA)) +\n  geom_smooth() +\n  geom_point()\n\n\nThe {ggplot2} library supplies default values\nSo we do not need to tell R how to do every thing\nWe do not need to tell R that the points in a scatterplot:\nshould represent the data aesthetic mappings in Cartesian (x-horizontal, y-vertical) 2-dimensional space\nand should be black in colour\n\n\n\n3.4.40 Defaults and arguments\n\nclearly.one.subjects %&gt;%\n  ggplot(aes(SHIPLEY, HLVA)) +\n  geom_smooth() +\n  geom_point(colour = \"darkgrey\", size = 3)\n\n\nWe can over-ride the defaults by supplying arguments, entering values inside the brackets in the function calls\ngeom_point(colour = \"darkgrey\", size = 3) tells R we want:\n\n\ndark grey points when the default is black\npoints that are 3x larger than the default size\n\n\n\n3.4.41 When we use {ggplot2} we are in control and we can be creative\n\n\n\n\n\n\nTip\n\n\n\n\nWe can add layers, control the appearance of each component\nTo construct more effective plots\nThe plots can be more effective because we develop them in an iterative process\nin which we reflect on our goals and the needs of our audience\n\n\n\n\n\n3.4.42 We can use colour\n\nUsing colourCode for the plot\n\n\n\nWhen we code a plot, we tell R we want:\nto display data about people with different education levels\ndistinguishing education level by colour\n\n\n\n\n\n\n\n\n\nFigure 3.14: Using colour\n\n\n\n\n\n\n\n\nclearly.one.subjects %&gt;%\n  ggplot(aes(x = SHIPLEY, y = HLVA, \n             group = EDUCATION, colour = EDUCATION)) +\n  geom_smooth(method = \"lm\", se = FALSE, \n              linewidth = 2, alpha = .75) +\n  geom_point(size = 3)\n\n\ngroup = EDUCATION, colour = EDUCATION tells R to:\n\n\ngroup the data by EDUCATION level\ncolour the points for people with different levels of education in different colours\n\n\n\n\n\n\n3.4.43 Method, size, transparency\n\nclearly.one.subjects %&gt;%\n  ggplot(aes(x = SHIPLEY, y = HLVA, \n             group = EDUCATION, colour = EDUCATION)) +\n  geom_smooth(method = \"lm\", se = FALSE, \n              linewidth = 2, alpha = .75) +\n  geom_point(size = 3)\n\n\nmethod = \"lm\", se = FALSE tells R what method to use to draw the smoother line\nlinewidth = 2 makes the width of the smoother line 2 x larger than the default\nalpha = .75 makes the line .75 x the opacity of the default (i.e. a. bit more transparent)\nLearn to edit: shape, size, transparency and colour\n\n\n\n3.4.44 We facet plots to enable comparisons\n\nUsing facetsCode for the plot\n\n\n\nIt is often easier to compare trends\nBy presenting a separate plot for each condition or group\nShowing the separate plots in a grid side-by-side\n\n\n\n\n\n\n\n\n\nFigure 3.15: The association between health literacy and vocabulary varies by education level\n\n\n\n\n\n\n\n\nclearly.one.subjects %&gt;%\n  ggplot(aes(x = SHIPLEY, y = HLVA, \n             group = EDUCATION, colour = EDUCATION)) +\n  geom_smooth(method = \"lm\", se = FALSE, \n              linewidth = 2, alpha = .75) +\n  geom_point(size = 3) +\n  facet_wrap(~ EDUCATION)\n\n\nfacet_wrap(~ EDUCATION) tells R to split the data by EDUCATION level\nAnd show a separate plot for each EDUCATION level group side-by-side for easy comparison\n\n\n\n\n\n\n3.4.45 We can guide our audience\n\nLabelled plotCode for the plot\n\n\n\nWe do not present visualizations in isolation\nWe present plots embedded in the context of labels and titles\nWe use the text to guide the viewer\n\n\n\n\n\n\n\n\n\nFigure 3.16: A labelled plot\n\n\n\n\n\n\n\n\nclearly.one.subjects %&gt;%\n  ggplot(aes(x = SHIPLEY, y = HLVA)) +\n  geom_smooth(method = \"lm\", se = FALSE,\n              colour = \"darkgreen\", linewidth = 2, alpha = .75) +\n  geom_point(size = 3, colour = \"lightgreen\") +\n  labs(x = \"Vocabulary (Shipley)\", y = \"Health literacy (HLVA)\",\n       title = \"Scatterplot showing how higher vocabulary\\npredicts higher health literacy on average\")\n\n\nWe use the labs() function to add: the plot title and the labels for the x-axis and y-axis\nWe edit the title so that the viewer can see what we want them to see\nWe use \\n to make the title fit on two lines\n\n\n\n\n\n\n3.4.46 We annotate plots to direct attention\n\nAnnotated plotCode for the plot\n\n\n\nWe can direct the attention of our audience to key features of our data\nBy adding annotations like text and lines\n\n\n\n\n\n\n\n\n\nFigure 3.17: An annotated plot\n\n\n\n\n\n\n\n\nclearly.one.subjects %&gt;%\n  ggplot(aes(x = SHIPLEY, y = HLVA)) +\n  geom_smooth(method = \"lm\", se = FALSE,\n              colour = \"darkgreen\", linewidth = 2, alpha = .75) +\n  geom_point(size = 3, colour = \"lightgreen\") +\n  labs(x = \"Vocabulary (Shipley)\", y = \"Health literacy (HLVA)\") +\n  geom_hline(yintercept = mean(clearly.one.subjects$HLVA),\n             linetype = \"dashed\",\n             linewidth = 2,\n             colour = \"grey\",\n             alpha = .85) +\n  annotate(\"text\", x = 27, y = 9.3, label = \"Mean HLVA\", colour = \"grey\") +\n  theme_bw()\n\n\ngeom_hline() adds a line to show mean health literacy\nannotate(\"text\" ...) adds a text label\n\n\n\n\n\n\n3.4.47 Extensions free our creativity\n\nComplex plotCode for the plot\n\n\n\nThe power of the Grammar of Graphics lies in the rules\nDevelopers can use the rules to expand our capacity to visualize data\nWe add marginal histograms to our scatterplot to visualize associations and distributions\n\n\n\n\n\n\n\n\n\nFigure 3.18: A scatterplot showing the potential association between health literacy and vocabulary\n\n\n\n\n\n\n\n\nplot &lt;- clearly.one.subjects %&gt;%\n  ggplot(aes(x = SHIPLEY, y = HLVA)) +\n  geom_smooth(method = \"lm\", se = FALSE,\n              colour = \"darkgreen\", linewidth = 2, alpha = .75) +\n  geom_point(size = 3, colour = \"lightgreen\") +\n  labs(x = \"Vocabulary (Shipley)\", y = \"Health literacy (HLVA)\")\n\nggMarginal(plot, type = \"histogram\", fill = \"lightgreen\", \n           xparams = list(binwidth=2), yparams = list(binwidth=1))\n\n\nggMarginal(plot, type = \"histogram\") enables us to show the distribution of scores on each variable\nThis helps our viewer to process the association and information about each variable (Franconeri et al., 2021)\n\n\n\n\n\n\n3.4.48 Choose your plot theme\n\nWe can choose a theme to adapt the look of the whole plot to suit our needs or the needs of our audience\n\n\n\n\n\n\n\n\n\nFigure 3.19: Different themes: (left) theme_dark(); (middle) theme_bw(); and (right) theme_classic()\n\n\n\n\n\n\n\n3.4.49 Summary\nYou start your work with these questions:\n\nWhat are our goals?\nWhat does our audience need or expect?\n\nYou develop your visualization in a reflective process:\n\nBegin with a quick draft to show the distributions or make the comparisons you think about first\nThen reflect, and edit: does this enable me to discover sources of variability in my data?\nThen reflect, and edit: does this enable me to effectively communicate what I want to communicate?\nThen reflect, and edit: does this look good? – do my viewers tell me this works well?\n\n\n\n\n\n\n\nTip\n\n\n\nI can only show you the potential for creative and effective visualization\n\nexperiment and find what looks good and is useful to you\nseek out information – good places to start are:\n\nhttps://ggplot2.tidyverse.org/index.html\nhttps://r-graph-gallery.com\n\n\n\n\n\n\nAnscombe, F. J. (1973). Graphs in statistical analysis. The American Statistician, 27(1), 17–21. https://doi.org/10.2307/2682899\n\n\nBates, D., Mächler, M., Bolker, B., & Walker, S. (2015). Fitting linear mixed-effects models using lme4. Journal of Statistical Software, 67(1), 1–48. https://doi.org/10.18637/jss.v067.i01\n\n\nBelenky, G., Wesensten, N. J., Thorne, D. R., Thomas, M. L., Sing, H. C., Redmond, D. P., Russo, M. B., & Balkin, T. J. (2003). Patterns of performance degradation and restoration during sleep restriction and subsequent recovery: a sleep dose-response study. Journal of Sleep Research, 12(1), 1–12. https://doi.org/10.1046/j.1365-2869.2003.00337.x\n\n\nFranconeri, S. L., Padilla, L. M., Shah, P., Zacks, J. M., & Hullman, J. (2021). The Science of Visual Data Communication: What Works. Psychological Science in the Public Interest, 22(3), 110–161. https://doi.org/10.1177/15291006211051956\n\n\nGelman, a. (2015). The connection between varying treatment effects and the crisis of unreplicable research: A bayesian perspective. Journal of Management, 41(2), 632–643. https://doi.org/10.1177/0149206314525208\n\n\nGelman, A., Pasarica, C., & Dodhia, R. (2002). Let’s practice what we preach. The American Statistician, 56(2), 121–130. https://doi.org/10.1198/000313002317572790\n\n\nGelman, A., & Unwin, A. (2013). Infovis and Statistical Graphics: Different Goals, Different Looks. Journal of Computational and Graphical Statistics, 22(1), 2–28. https://doi.org/10.1080/10618600.2012.761137\n\n\nHofman, J. M., Goldstein, D. G., & Hullman, J. (2020). How visualizing inferential uncertainty can mislead readers about treatment effects in scientific results. 112. https://doi.org/10.1145/3313831.3376454\n\n\njumpingrivers. (n.d.). Datasets from the Datasaurus Dozen. https://jumpingrivers.github.io/datasauRus/\n\n\nKastellec, J. P., & Leoni, E. L. (2007). Using Graphs Instead of Tables in Political Science. Perspectives on Politics, 5(4), 755–771. https://doi.org/10.1017/S1537592707072209\n\n\nMatejka, J., & Fitzmaurice, G. (2017). CHI ’17: CHI Conference on Human Factors in Computing Systems. 1290–1294. https://doi.org/10.1145/3025453.3025912\n\n\nVan Der Bles, A. M., Van Der Linden, S., Freeman, A. L. J., Mitchell, J., Galvao, A. B., Zaval, L., & Spiegelhalter, D. J. (2019). Communicating uncertainty about facts, numbers and science (Vol. 6).\n\n\nVasishth, S., & Gelman, A. (2021). How to embrace variation and accept uncertainty in linguistic and psycholinguistic data analysis. Linguistics, 59(5), 1311–1342. https://doi.org/10.1515/ling-2019-0051\n\n\nWilkinson, L. (2013). The Grammar of Graphics. Springer Science & Business Media.\n\n\nZhang, S., Heck, P. R., Meyer, M. N., Chabris, C. F., Goldstein, D. G., & Hofman, J. M. (2023). An illusion of predictability in scientific results: Even experts confuse inferential uncertainty and outcome variability. Proceedings of the National Academy of Sciences, 120(33), e2302491120. https://doi.org/10.1073/pnas.2302491120",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to visualization</span>"
    ]
  },
  {
    "objectID": "visualization-intro.html#footnotes",
    "href": "visualization-intro.html#footnotes",
    "title": "3  Introduction to visualization",
    "section": "",
    "text": "We write the slides and this book in Quarto in R-Studio. Quarto scripts can be rendered as .html to share web-books like this one, or to share slides like those we use in presenting the lecture. One advantage of using Quarto is that we can share a plot and the code we used to generate the plot in the same page.↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "4  Data visualization",
    "section": "",
    "text": "4.1 Aims\nIn writing this chapter, I have two aims.\nThere is an extensive experimental and theoretical literature concerning data visualization, what choices we can or should make, and how these choices have more or less impact, in different circumstances or for different audiences. Here, we can only give you a flavour of the on-going discussion. If you are interested, you can follow-up the references in the cited articles. But, using this chapter, I hope that you will gain a sense of the reasons how or why we may choose to do different things when we produce visualizations.\nIn my experience, knowing that there are choices is the first step. In proprietary software packages like Excel and SPSS there are plenty of choices but these are limited by the menu systems to certain combinations of elements. Here, in using R to produce visualizations, there is much more freedom, and much more capacity to control what a plot shows and how it looks, but knowing where to start has to begin with seeing examples of what some of the choices result in.\nAt the end of the chapter, I highlight some resources you can use in independent learning for further development, see Section 4.9.\nSo, we are aiming to (1.) start to build insight into the choices we make and (2.) provide resources to enable making those choices in data visualization.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#sec-vis-aims",
    "href": "visualization.html#sec-vis-aims",
    "title": "4  Data visualization",
    "section": "",
    "text": "The first aim for this chapter is to expose students to an outline summary of some key ideas and techniques for data visualization in psychological science.\n\n\n\nThe second aim is to provide materials, and to show visualizations, to raise an awareness of what results come from making different choices. This is because we hope to encourage students to make choices based on reasons and it is hard to know what choices count without first seeing what the results might look like.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#sec-why-visualization-matters",
    "href": "visualization.html#sec-why-visualization-matters",
    "title": "4  Data visualization",
    "section": "4.2 Why data visualization matters",
    "text": "4.2 Why data visualization matters\nData visualization is important. Building skills in visualization matters to you because, even if you do not go on to professional work in which you produce visualizations you will certainly be working in fields in which you need to work with, or read or evaluate, visualizations.\nYou have already been doing this: our cultural or visual environment is awash in visualizations, from weather maps to charts on the television news. It will empower you if you know a bit about how or why these visualizations are produced in the ways that they are produced. That is a complex development trajectory but we can get started here.\nIn the context of the research report exercise, see Section 9.2.3.1, I mention data visualization in relation to stages of the data analysis pipeline or workflow. But the reality is that, most of the time, visualization is useful and used at every stage of data analysis workflow.\n\n\n\n\n\n\n\n\nQ\n\n\ncluster_R\n\n\n\n\nnd_1\n\nGet raw data\n\n\n\nnd_2\n\nTidy data\n\n\n\nnd_1-&gt;nd_2\n\n\n\n\n\nnd_3_l\n\nVisualize\n\n\n\nnd_2-&gt;nd_3_l\n\n\n\n\n\nnd_3\n\nAnalyze\n\n\n\nnd_2-&gt;nd_3\n\n\n\n\n\nnd_3_r\n\nExplore\n\n\n\nnd_2-&gt;nd_3_r\n\n\n\n\n\nnd_3_a\n\nAssumptions\n\n\n\nnd_3_a-&gt;nd_3_l\n\n\n\n\n\nnd_3_a-&gt;nd_3\n\n\n\n\n\nnd_3_a-&gt;nd_3_r\n\n\n\n\n\nnd_3_l-&gt;nd_3\n\n\n\n\nnd_4\n\nPresent\n\n\n\nnd_3_l-&gt;nd_4\n\n\n\n\n\nnd_3-&gt;nd_4\n\n\n\n\n\n\n\n\nFigure 4.1: The data analysis pipeline or workflow",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#sec-vis-honesty",
    "href": "visualization.html#sec-vis-honesty",
    "title": "4  Data visualization",
    "section": "4.3 Three kinds of honesty",
    "text": "4.3 Three kinds of honesty\nI write this chapter with three kinds of honesty in mind.\n\nI will expose some of the process involved in thinking about and preparing for the production of plots.\n\n\nI can assure you that when a professional data analysis worker produces plots in R they will be looking for information about what to do, and how to do it, online. I will provide links to the information I used, when I wrote this chapter, in order to figure out the coding to produce the plots.\nI won’t pretend that I got the plots “right first time” or that I know all the coding steps by memory. Neither is true for me and they would not be true for most professionals if they were to write a chapter like this. Looking things up online is something we all do so showing you where the information can be found will help you grow your skills.\n\n\nI will show how we often prepare for the production of plots by processing the data that we must use to inform the plots.\n\n\nWe almost always have to process the data we collected or gathered together from our exerimental work or our observations.\nIn this chapter, some of the coding steps I will outline are done in advance of producing a plot, to give the plotting code something to work with.\nKnowing about these processing steps will ensure you have more flexibility or power in getting your plots ready.\n\n\nI am going to expose variation, as often as I can, in observations.\n\n\nWe typically collect data about or from people, about their responses to things we may present (stimuli) or, given tasks, under different conditions, or concerning individual differences on an array of dimensions.\nSources of variation will be everywhere in our data, even though we often work with statistical analyses (like the t-test) that focus our attention on the average participant or the average response.\nModern analysis methods (like mixed-effects models) enable us to account for sources of variation systematically, so it is good to begin thinking about, say, how people vary in their response to different experimental conditions from early in your development.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#sec-vis-tidyverse",
    "href": "visualization.html#sec-vis-tidyverse",
    "title": "4  Data visualization",
    "section": "4.4 Our approach: tidyverse",
    "text": "4.4 Our approach: tidyverse\nThe approach we will take is to focus on step-by-step guides to coding. I will show plots and I will walk through the coding steps, explaining my reasons for the choices I make.\nWe will be working with plotting functions like ggplot() provided in libraries like ggplot2 (Wickham, 2016) which is part of the tidyverse (Wickham, 2017) collection of libraries.\nYou can access information about the tidyverse collection here.\n\n4.4.1 Grammar of graphics\nThe gg in ggplot stands for the “Grammar of Graphics”, and the ideas motivating the development of the ggplot2 library of functions are grounded in the ideas concerning the grammar of graphics, set out in the book of that name (Wilkinson, 2013).\nWhat is helpful to us, here, is the insight that the code elements (and how they result in visual elements) can be identified as building blocks, or layers, that we can add and adjust piece by piece when we are producing a visualization.\nA plot represents information and, critically, every time we write ggplot code we must specify somewhere the ways that our plot links data to something we see. In terms of ggplot, we specify aesthetic mappings using the aes() code to tell R what variables should be mapped e.g. to x-axis or y-axis location, to colour, or to group assignments. We then add elements to instruct R how to represent the aesthetic mappings as visual objects or attributes: geometric objects like a scatter of points geom_point() or a collection of bars geom_bar(); or visual features like colour, shape or size e.g. aes(colour = group). We can add visual elements in a series of layers, as shall see in the practical demonstrations of plot construction. We can adjust how scaling works. And we can add annotation, labels, and other elements to guide and inform the attention of the audience.\nYou can read more about mastering the grammar here.\n\n\n4.4.2 Pipes\nWe know that (some of) you want to see more use of pipes (represented as %&gt;% or |&gt;) in coding. There will be plenty of pipes in this chapter.\nIn using pipes in the code, I am structuring the code so that it works — and is presented — in a sequence of steps. There are different ways to write code but I find this way easier to work with and to read and I think you will too.\nLet’s take a small example:\n\nsleepstudy %&gt;%\n  group_by(Subject) %&gt;%\n  summarise(average = mean(Reaction)) %&gt;%\n  ggplot(aes(x = average)) + \n  geom_histogram()\n\n\n\n\n\n\n\n\nHere, we work through a series of steps:\n\nsleepstudy %&gt;% we first tell R we want to work with the dataset called sleepstudy and the %&gt;% pipe symbol at the end of the line tells R that we want it to pass that dataset on to the next step for what happens next.\ngroup_by(Subject) %&gt;% tells R that we want it to do something, here, group the rows of data according to the Subject (participant identity) coding variable, and pass the grouped data on to the next step for what happens following.\nsummarise(average = mean(Reaction)) %&gt;% tells R to take the grouped variable and calculate a summary, the mean Reaction score, for each group of observations for each participant. The %&gt;% pipe at the end of the line tells R to pass the summary dataset of mean Reaction scores on to the next process.\nggplot(aes(x = average)) + tells R that we want it to take these summary average Reaction scores and make a plot out of them.\ngeom_histogram() tells R that we want a histogram plot.\n\nWhat you can see is that each line ending in a %&gt; pipe passes something on to the next line. A following line takes the output of the process coded in the preceding line, and works with it.\nEach step is executed in turn, in strict sequence. This means that if I delete line 3 summarise(average = mean(Reaction)) %&gt;% then the following lines cannot work because the ggplot() function will be looking for a variable average that does not yet exist.\n\n\n\n\n\n\nWarning\n\n\n\n\nYou can see that in the data processing part of the code, successive steps in data processing end in a pipe %&gt;%.\nIn contrast, successive steps of the plotting code add ggplot elements line by line with each line (except the last) ending in a +.\n\n\n\nNotice that none of the processing steps actually changes the dataset called sleepstudy. The results of the process exist and can be used only within the sequence of steps that I have coded. If you want to keep the results of processing steps, you need to assign an object name to hold them, and I show how to do this, in the following.\nYou can read a clear explanation of pipes here.\n\n\n\n\n\n\nTip\n\n\n\nYou can use the code you see:\n\nEach chunk of code is highlighted in the chapter.\nIf you hover a cursor over the highlighted code a little clipboard symbol appears in the top right of the code chunk.\nClick on the clipboard symbol to copy the code, paste it into your own R-Studio instance.\nThen experiment: try out things like removing or commmenting out lines, or changing lines, to see what effect that has.\nBreaking things, or changing things, helps to show what each bit of code does.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#sec-vis-ideas",
    "href": "visualization.html#sec-vis-ideas",
    "title": "4  Data visualization",
    "section": "4.5 Key ideas",
    "text": "4.5 Key ideas\nData visualization is not really about coding, as about thinking.\n\nWhat are our goals?\nWhy do we make some choices instead of others?\n\n\n4.5.1 Purposes\nGelman & Unwin (2013) outline the goals we may contemplate when we produce or evaluate visual data displays. In general, they argue, we are doing one or both of two things.\n\nDiscovery\nCommunication\n\nIn practice, this may involve the following (I paraphrase them, here).\n\nDiscovery goals\n\n\nGetting a sense of what is in a dataset, checking assumptions, confirming expectations, and looking for distinct patterns.\nMaking sense of the scale and complexity of the dataset.\nExploring the data to reveal unexpected aspects. As we will see, using small multiples (grids of plots) can often help with this.\n\n\nCommunication goals\n\n\nWe communicate about our data to ourselves and to others. The process of constructing and evaluating a plot is often one way we speak to ourselves about own data, developing an understanding of what we have got. Once we have done this for ourselves, we can better figure out how to do it to benefit the understanding of an audience.\nWe often use a plot to tell a story: the story of our study, our data, or our insight and how we get to it.\nWe can use visualizations to attract attention and stimulate interest. Often, in presenting data to an audience through a talk or a report we need to use effective visualizations to ensure we get attention and that we locate the attention of our audience in the right places.\n\n\n\n4.5.2 Psychological science of data visualization\nYou will see a rich variety of data visualizations in media and in the research literature. You will know that some choices, in the production of those visualizations, appear to work better than others.\nSome of the reasons why some choices work better will relate to what we can understand in terms of the psychological science of how visual data communication works. A useful recent review of relevant research is presented by Franconeri et al. (2021).\nFranconeri et al. (2021) provide a reason for working on visualizations: they allow us humans to process an array of information at once, often faster than if we were reading about the information, bit by bit. Effective visualization, then, is about harnessing the power of the human visual system, or visual cognition, for quick, efficient, information processing. Critically for science, in addition, visualizations can be more effective for discovering or communicating the critical features of data than summary statistics, as we shall see.\nIn producing visualizations, we often work with a vocabulary or palette of objects or visual elements. Franconeri et al. (2021) discuss how visualizations rely on visual channels to transform numbers into images that we can process visually.\n\nDot plots and scatterplots represent values as position.\nBar graphs represent values as position (the heights of the tops of bars) but also as lengths.\nAngles are presented when we connect points to form a line, allowing us to encode the differences between points.\nIntensity can be presented through variation in luminance contrast or colour saturation.\n\nThese channels can be ordered by how precisely they have been found to communicate different numeric values to the viewer. Your audience may more accurately perceive the difference between two quantities if you communicate that difference through the difference in the location of two points than if you ask your audience to compare the angles of two lines or the intensity of two colour spots.\nIn constructing data visualizations, we often work with conventions, established through common practice in a research tradition. For example, if you are producing a scatterplot, then most of the time your audience will expect to see the outcome (or dependent variable) represented by the vertical height (on the y-axis) of points. And your audience will expect that higher points represent larger quantities of the y-axis variable.\nIn constructing visualizations, we need to be aware of the cognitive work that we require the audience to do. Comparisons are harder, requiring more processing and imposing more load on working memory. You can help your reader by guiding their attention, by grouping or ordering visual elements to identify the most important comparisons. We can vary colour and shape to group or distinguish visual elements. We can add annotation or elements like lines or arrows to guide attention.\nVisualizations are presented in context, whether in presentations or in reports. This context should be provided, by you the producer, with the intention to support the communication of your key messages. A visual representation, a plot, will be presented with a title, maybe a title note, maybe with annotation in the plot, and maybe with accompanying text. You should use these textual elements to lead your audience, to help them make sense of what they are looking at.\nThe diversity of audiences means that we should habitually add alt text for data visualizations to help those who use screen readers by providing a summary description of what images show. This chapter has been written using Quarto and rendered to .html with alt text included along with all images. Please do let me know if you are using a screen reader and the alt text description is or is not so helpful.\nYou can read a helpful explanation of alt text here.\nIf you use colour in images then we should use colour bind colour palettes.\nYou can read about using colour blind palettes here or here.\nIn the following practical exercises, we work with many of the insights in our construction of visualizations.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#sec-vis-quick-start",
    "href": "visualization.html#sec-vis-quick-start",
    "title": "4  Data visualization",
    "section": "4.6 A quick start",
    "text": "4.6 A quick start\nWe can get started before we understand in depth the key ideas or the coding steps. This will help to show where we are going. We will work with the sleepstudy dataset.\nI will model the process, to give you an example workflow:\n\nthe data, where they come from — what we can find out;\nhow we approach the data — what we expect to see;\nhow we visualize the data — discovery, communication.\n\n\n4.6.1 Sleepstudy data\nWhen we work with R, we usually work with functions like ggplot() provided in libraries like ggplot2 (Wickham, 2016). These libraries typically provide not only functions but also datasets that we can use for demonstration and learning.\nThe lme4 library (Bates et al., 2015) provides the sleepstudy dataset and we will take a look at these data to offer a taste of what we can learn to do. Usually, information about the R libraries we use will be located on the Comprehensive R Archive Network (CRAN) web pages, and we can find the technical reference information for lme4 in the CRAN reference manual for the library, where we see that the sleepstudy data are from a study reported by (Belenky et al., 2003). The manual says that the sleepstudy dataset comprises:\n\nA data frame with 180 observations on the following 3 variables. [1.] Reaction – Average reaction time (ms) [2.] Days – Number of days of sleep deprivation [3.] Subject – Subject number on which the observation was made.\n\nWe can take a look at the first few rows of the dataset.\n\nsleepstudy %&gt;%\n    head(n = 4)\n\n  Reaction Days Subject\n1 249.5600    0     308\n2 258.7047    1     308\n3 250.8006    2     308\n4 321.4398    3     308\n\n\nWhat we are looking at are:\n\nThe average reaction time per day (in milliseconds) for subjects in a sleep deprivation study. Days 0-1 were adaptation and training (T1/T2), day 2 was baseline (B); sleep deprivation started after day 2.\n\nThe abstract for Belenky et al. (2003) tells us that participants were deprived of sleep and the impact of relative deprivation was tested using a cognitive vigilance task for which the reaction times of responses were recorded.\nSo, we can expect to find:\n\nA set of rows corresponding to multiple observations for each participant (Subject)\nA reaction time value for each participant (Reaction)\nRecorded on each Day\n\n\n\n4.6.2 Discovery and communication\nIn data analysis work, we often begin with the objective to understand the structure or the nature of the data we are working with.\nYou can call this the discovery phase:\n\nwhat have we got?\ndoes it match our expectations?\n\nIf these are reaction time data (collected in an cognitive experiment) do they look like cognitive reaction time data should look? We would expect to see a skewed distribution of observed reaction times distributed around an average located somewhere in the range 200-700ms.\nFigure 4.2 represents the distribution of reaction times in the sleepstudy dataset.\nI provide notes on the code steps that result in the plot. Click on the Notes tab to see them. Later, I will discuss some of these elements.\n\nPlotNotes\n\n\n\nsleepstudy %&gt;%\n  ggplot(aes(x = Reaction)) +\n  geom_histogram(binwidth = 15) +\n  geom_vline(xintercept = mean(sleepstudy$Reaction), \n             colour = \"red\", linetype = 'dashed', size = 1.5) +\n  annotate(\"text\", x = 370, y =20, \n                    colour = \"red\", \n                    label = \"Average value shown in red\") +\n  theme_bw()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\nFigure 4.2: Figure showing a histogram of sleepstudy reaction time data\n\n\n\n\n\n\n\nThe plotting code pipes the data into the plotting code steps to produce the plot. You can see some elements that will be familiar to you and some new elements.\n\nsleepstudy %&gt;%\n  ggplot(aes(x = Reaction)) +\n  geom_histogram(binwidth = 15) +\n  geom_vline(xintercept = mean(sleepstudy$Reaction), \n             colour = \"red\", linetype = 'dashed', size = 1.5) +\n  annotate(\"text\", x = 370, y =20, \n                    colour = \"red\", \n                    label = \"Average value shown in red\") +\n  theme_bw()\n\nLet’s go through the code step-by-step:\n\nsleepstudy %&gt;% asks R to take the sleepstudy dataset and %&gt;% pipe it to the next steps for processing.\nggplot(aes(x = Reaction)) + takes the sleepstudy data and asks R to use the ggplot() function to produce a plot.\naes(x = Reaction) tells R that in the plot we want it to map the Reaction variable values to locations on the x-axis: this is the aesthetic mapping.\ngeom_histogram(binwidth = 15) + tells R to produce a histogram then add a step.\ngeom_vline(...) + tells R we want to draw vertical line.\nxintercept = mean(sleepstudy$Reaction), ... tells R to draw the vertical line at the mean value of the variable Reaction in the sleepstudy dataset.\ncolour = \"red\", linetype = 'dashed', size = 1.5 tells R we want the vertical line to be red, dashed and 1.5 times the usual size.\nannotate(\"text\", ...) tells R we want to add a text note.\nx = 370, y =20, ... tells R we want the note added at the x,y coordinates given.\ncolour = \"red\", ..; and we want the text in red.\n...label = \"Average value shown in red\") + tells R we want the text note to say that this is where the average is.\ntheme_bw() lastly, we change the theme.\n\n\n\n\nFigure 4.2 shows a distribution of reaction times, ranging from about 200ms to 500ms. The distribution has a peak around 300ms. The location of the mean is shown with a dashed red line. The distribution includes a long tail of longer times. This is pretty much what we would expect to see.\nWe may wish to communicate the information we gain through using this histogram, in a presentation or in a report.\n\n\n4.6.3 Discovery and communication\nLet us imagine that it is our study. (Here, we shall not concern ourselves too much — with apologies — with understanding what the original study authors actually did.)\nIf we are looking at the impact of sleep deprivation on cognitive performance, we might predict that reaction times got longer (responses slowed) as the study progressed. Is that what we see?\nTo examine the association between two variables, we often use scatterplots. Figure 4.3 is a scatterplot indicating the possible association between reaction time and days in the sleepstudy data. Points are ordered on x-axis from 0 to 9 days, on y-axis from 200 to 500 ms reaction time.\nI provide notes on the code steps that result in the plot. Click on the Notes tab to see them. Later, I will discuss some of these elements.\n\nPlotNotes\n\n\n\nsleepstudy %&gt;%\n  ggplot(aes(x = Days, y = Reaction)) +\n  geom_point(size = 1.5, alpha = .5) + \n  scale_x_continuous(breaks = c(0, 3, 6, 9)) +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 4.3: Figure showing a scatterplot of the relation between reaction time and days in the sleepstudy data\n\n\n\n\n\n\n\nNotice the numbered steps in producing this plot.\n\nsleepstudy %&gt;% \n  ggplot(aes(x = Days, y = Reaction)) +\n  geom_point() + \n  scale_x_continuous(breaks = c(0, 3, 6, 9)) +\n  theme_bw()\n\n\nName the dataset: the dataset is called sleepstudy in the lme4 library which makes it available therefore we use this name to specify it.\nsleepstudy %&gt;% uses the %&gt;% pipe operator to pass this dataset to ggplot() to work with, in creating the plot. Because ggplot() now knows about the sleepstudy data, we can next specify what aesthetic mappings we need to use.\nggplot(aes(x = Days, y = Reaction)) + tells R that we want to map Days information to x-axis position and Reaction (response time) information to y-axis position.\ngeom_point() + tells R that we want to locate points – creating a scatterplot – at the paired x-axis and y-xis coordinates.\nscale_x_continuous(breaks = c(0, 3, 6, 9)) + is new: we tell R that we want the x-axis tick labels – the numbers R shows as labels on the x-axis – at the values 0, 3, 6, 9 only.\ntheme_bw() requires R to make the plot background white and the foreground plot elements black.\n\nYou can find more information on scale_ functions in the ggplot2 reference information.\nhttps://ggplot2.tidyverse.org/reference/scale_continuous.html\n\n\n\nThe plot suggests that reaction time increases with increasing number of days.\nIn producing this plot, we are both (1.) engaged in discovery and, potentially, (2.) able to do communication.\n\nDiscovery: is the relation between variables what we should expect, given our assumptions?\nCommunication: to ourselves and others, what relation do we observe, given our sample?\n\nAt this time, we have used and discussed scatterplots before, why we use them, how we write code to produce them, and how we read them.\nWith two additional steps we can significantly increase the power of the visualization. Figure 4.4 is a grid of scatterplots indicating the possible association between reaction time and days separately for each participant.\nAgain, I hide an explanation of the coding steps in the Notes tab: the interested reader can click on the tab to view the step-by-step guide to what is happening.\n\nPlotNotes\n\n\n\nsleepstudy %&gt;%\n  group_by(Subject) %&gt;%\n  mutate(average = mean(Reaction)) %&gt;%\n  ungroup() %&gt;%\n  mutate(Subject = fct_reorder(Subject, average)) %&gt;%\n  ggplot(aes(x = Days, y = Reaction)) +\n  geom_point() + \n  geom_line() +\n  scale_x_continuous(breaks = c(0, 3, 6, 9)) +\n  facet_wrap(~ Subject) +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 4.4: Figure showing a scatterplot of the relation between reaction time and days: here, we plot the data for each participant separately\n\n\n\n\n\n\n\nNotice the numbered steps in producing this plot.\n\nsleepstudy %&gt;%\n  group_by(Subject) %&gt;%\n  mutate(average = mean(Reaction)) %&gt;%\n  ungroup() %&gt;%\n  mutate(Subject = fct_reorder(Subject, average)) %&gt;%\n  ggplot(aes(x = Days, y = Reaction)) +\n  geom_point() + \n  geom_line() +\n  scale_x_continuous(breaks = c(0, 3, 6, 9)) +\n  facet_wrap(~ Subject) +\n  theme_bw()\n\nYou can see that the block of code combines data processing and data plotting steps. Let’s look at the data processing steps then the plotting steps in order.\nFirst: why are we doing this? My aim is to produce a plot in which I show the association between Days and Reaction for each Subject individually. I suspect that the association between Days and Reaction may be stronger – so the trend will be steeper – for participants who are slower overall. I suspect this because, given experience, I know that slower, less accurate, participants tend to show larger effects.\nSo: in order to get a grid of plots, one plot for each Subject, in order of the average Reaction for each individual Subject, I need to first calculate the average Reaction then order the dataset rows by those averages. I do that in steps, using pipes to feed information from one step to the next step, as follows.\n\nsleepstudy %&gt;% tells R what data I want to use, and pipe it to the next step.\ngroup_by(Subject) tells R I want it to work with data (rows) grouped by Subject identity code, %&gt;% piping the grouped form of the data forward to the next step\nmutate(average = mean(Reaction)) uses mutate() to create a new variable average which I calculate as the mean() of Reaction, piping the data with this additional variable %&gt;% forward to the next step.\nungroup() %&gt;% tells R I want it to go back to working with the data in rows not grouped rows, and pipe the now ungrouped form of the data to the next step.\nmutate(Subject = fct_reorder(Subject, average)) tells R I want it to sort the rows of the whole sleepstudy dataset in order, moving groups of rows identified by Subject so that data for Subject codes associated with faster times are located near the top of the dataset.\n\nThese data, ordered by Subject by the average Reaction for each participant, are then %&gt;% piped to ggplot to create a plot.\n\nggplot(aes(x = Days, y = Reaction)) + specifies the aesthetic mappings, as before.\ngeom_point() + asks R to locate points at the x-axis, y-axis coordinates, creating a scatterplot, as before.\ngeom_line() + is new: I want R to connect the points, showing the trend in the association between Days and Reaction for each person.\nscale_x_continuous(breaks = c(0, 3, 6, 9)) + fixes the x-axis labels, as before.\nfacet_wrap(~ Subject) + is the big new step: I ask R to plot a separate scatterplot for the data for each individual Subject.\n\nYou can see more information about facetting here:\nhttps://ggplot2.tidyverse.org/reference/facet_wrap.html\nIn short, with the facet_wrap(~ .) function, we are asking R to subset the data by a grouping variable, specified (~ .) by replacing the dot with the name of the variable.\nNotice that I use %&gt;% pipes to move the data processing forward, step by step. But I use + to add plot elements, layer by layer.\n\n\n\nFigure Figure 4.4 is a grid or lattice of scatterplots revealing how the possible association between reaction time and days varies quite substantially between the participants in the sleepstudy data. Most plots indicate that reaction time increases with increasing number of days. However, different participants show this trend to differing extents.\nWhat are the two additions I made to the conventional scatterplot code?\n\nI calculated the average reaction time per participant, and I ordered the data by those averages.\nI facetted the plots, breaking them out into separate scatterplots per participant.\n\nWhy would you do this? Variation between people or groups, in effects or in average outcomes, are often to be found in psychological data (Vasishth & Gelman, 2021). The variation between people that we see in these data — in the average response reaction time, and in how days affects times — would motivate the use of linear mixed-effects models to analyze the way that sleep patterns affect responses in the sleep study (Pinheiro & Bates, 2000).\n\n\n\n\n\n\nTip\n\n\n\nThe data processing and plotting functions in the tidyverse collection of libraries enable us to discover and to communicate variation in behaviours that should strengthen our and others’ scientific understanding.\n\n\n\n\n4.6.4 Summary: Quick start lessons\nWhat we have seen, so far, is that we can make dramatic changes to the appearance of visualizations (e.g., through faceting) and also that we can exert fine control over the details (e.g., adjusting scale labels). What we need to stop and consider are what we want to do (and why), in what order.\nWe have seen how we can feed a data process into a plot to first prepare then produce the plot in a sequence of steps. In processing the data, we can take some original data and extract or calculate information that we can use for our plotting e.g. calculating the mean of a distribution in order to then highlight where that mean is located.\nWe have also seen the use of plots, and the editing of their appearance, to represent information visually. We can verbalize the thought process behind the production of these plots through a series of questions.\n\nAre we looking at the distribution of one variable (if yes: consider a histogram) or are we comparing the distributions of two or more variables (if yes: consider a scatterplot)?\nIs there a salient feature of the plot we want to draw the attention of the audience to? We can add a visual element (like a line) and annotation text to guide the audience.\nAre we interested in variation between sub-sets of the data? We can facet the plot to examine variation between sub-sets (facets) enabling the comparison of trends.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#sec-vis-practical-visualization",
    "href": "visualization.html#sec-vis-practical-visualization",
    "title": "4  Data visualization",
    "section": "4.7 A practical guide to visualization ideas",
    "text": "4.7 A practical guide to visualization ideas\nIn this guide, we illustrate some of the ideas about visualization we discussed at the start, working with practical coding examples. We will be working with real data from a published research project. We are going to focus the practical coding examples on the data collected for the analysis reported by Ricketts et al. (2021).\n\nWe will focus on working with the data from one of the tasks, in one of the studies reported by Ricketts et al. (2021).\n\nThis means that you can consolidate your learning by applying the same code moves to data from the other task in the same study, or to data from the other study.\nIn applying code to other data, you will need to be aware of differences in, say, the way that some things like the outcome response variable are coded.\n\nYou can then further extend your development by trying out the coding moves for yourself using the data collected by Rodríguez-Ferreiro et al. (2020).\n\nThese data are from a quite distinct kind of investigation, on a different research topic than the topic we will be exploring through our working examples.\nHowever, some aspects of the data structure are similar.\nCritically, the data are provided with comprehensive documentation.\n\n\n\n4.7.1 Set up for coding\nTo do our practical work, we will need functions and data. We get these at the start of our workflow.\n\n4.7.1.1 Get libraries\nWe are going to need the lme4, patchwork, psych and tidyverse libraries of functions and data.\n\nlibrary(ggeffects)\nlibrary(patchwork)\nlibrary(psych)\nlibrary(tidyverse)\n\n\n\n4.7.1.2 Get the data\nYou can access the data we are going to use in two different ways.\n\n4.7.1.2.1 Get the data from project repositories\nThe data associated with both (Ricketts et al., 2021) and (Rodríguez-Ferreiro et al., 2020) are freely available through project repositories on the Open Science Framework web pages.\nYou can get the data from the Ricketts et al. (2021) paper through the repository located here.\nYou can get the data from the Rodríguez-Ferreiro et al. (2020) paper through the repository located here.\nThese data are associated with full explanations of data collection methods, materials, data processing and data analysis code. You can review the papers and the repository material guides for further information.\nIn the following, I am going to abstract summary information about the Ricketts et al. (2021) study and data. I shall leave you to do the same for the Rodríguez-Ferreiro et al. (2020) study.\n\n\n4.7.1.2.2 Get the data through a downloadable archive\nDownload the data.zip files folder and upload the files to RStudio Server.\nThe folder includes the Ricketts et al. (2021) data files:\n\nconcurrent.orth_2020-08-11.csv\nconcurrent.sem_2020-08-11.csv\nlong.orth_2020-08-11.csv\nlong.sem_2020-08-11.csv\n\nThe folder also includes the Rodríguez-Ferreiro et al. (2020) data files:\n\nPrimDir-111019_English.csv\nPrimInd-111019_English.csv\n\n\n\n\n\n\n\nWarning\n\n\n\n\nThese data files are collected together in a folder for download, for your convenience, but the version of record for the data for each study comprise the files located on the OSF repositories associated with the original articles.\n\n\n\n\n\n\n\n4.7.2 Information about the Ricketts study and the datasets\nRicketts et al. (2021) conducted an investigation of word learning in school-aged children. They taught children 16 novel words in a study with a 2 x 2 factorial design. In this investigation, they tested whether word learning is helped by presenting targets for word learning with their spellings, and whether learning is helped by telling children that they would benefit from the presence of those spellings.\nThe presence of orthography (the word spelling) was manipulated within participants (orthography absent vs. orthography present): for all children, eight of the words were taught with orthography present and eight with orthography absent. Instructions (incidental vs. explicit) were manipulated between participants such that children in the explicit condition were alerted to the presence of orthography whereas children in the incidental condition were not.\nA pre-test was conducted to establish participants’ knowledge of the stimuli. Then, each child was seen for three 45-minute sessions to complete training (Sessions 1 and 2) and post-tests (Session 3). Ricketts et al. (2021) completed two studies: Study 1 and Study 2. All children, in both studies 1 and 2 completed the Session 3 post-tests.\nIn Study 1, longitudinal post-test data were collected because children were tested at two time points. Children were administered post-tests in Session 3, as noted: Time 1. Post-tests were then re-administered approximately eight months later at Time 2 (\\(M = 241.58\\) days from Session 3, \\(SD = 6.10\\)). In Study 2, the Study 1 sample was combined with an older sample of children. The additional Study 2 children were not tested at Time 2, and the analysis of Study 2 data did not incorporate test time as a factor.\nThe outcome data for both studies consisted of performance on post-tests.\nThe semantic post-test assessed knowledge for the meanings of newly trained words using a dynamic or sequential testing approach. I will not explain this approach in more detail, here, because the practical visualization exercises focus on the orthographic knowledge (spelling knowledge) post-test, explained next.\nThe orthographic post-test was included to ascertain the extent of orthographic knowledge after training. Children were asked to spell each word to dictation and spelling productions were transcribed for scoring. Responses were scored using a Levenshtein distance measure indexing the number of letter deletions, insertions and substitutions that distinguish between the target and child’s response. The maximum score is 0, with higher scores indicating less accurate responses.\nFor the Study 1 analysis, the files are:\n\nlong.orth_2020-08-11.csv\nlong.sem_2020-08-11.csv\n\nWhere long indicates the longitudinal nature of the data-set.\nFor the Study 2 analysis, the files are:\n\nconcurrent.orth_2020-08-11.csv\nconcurrent.sem_2020-08-11.csv\n\nWhere concurrent indicates the inclusion of concurrent (younger and older) child participant samples.\nEach column in each data-set corresponds to a variable and each row corresponds to an observation (i.e., the data are tidy). Because the design of the study involves the collection of repeated observations, the data can be understood to be in a long format.\nEach child was asked to respond to 16 words and, for each of the 16 words, we collected post-test responses from multiple children. All words were presented to all children.\nWe explain what you will find when you inspect the .csv files, next.\n\n4.7.2.1 Data – variables and value coding\nThe variables included in .csv files are listed, following, with information about value coding or calculation.\n\nParticipant — Participant identity codes were used to anonymize participation. Children included in studies 1 and 2 – participants in the longitudinal data collection – were coded “EOF[number]”. Children included in Study 2 only (i.e., the older, additional, sample) were coded “ND[number]”.\nTime — Test time was coded 1 (time 1) or 2 (time 2). For the Study 1 longitudinal data, it can be seen that each participant identity code is associated with observations taken at test times 1 and 2.\nStudy — Observations taken for children included in studies 1 and 2 – participants in the longitudinal data collection – were coded “Study1&2”. Children included in Study 2 only (i.e., the older, additional, sample) were coded “Study2”.\nInstructions — Variable coding for whether participants undertook training in the explicit or incidental conditions.\nVersion — Experiment administration coding\nWord — Letter string values show the words presented as stimuli to children.\nConsistency_H — Calculated orthography-to-phonology consistency value for each word.\nOrthography — Variable coding for whether participants had seen a word in training in the orthography absent or present conditions.\nMeasure — Variable coding for the post-test measure: Sem_all if the semantic post-test; Orth_sp if the orthographic post-test.\nScore — Variable coding for response category.\n\nFor the semantic (sequential or dynamic) post-test, responses were scored as corresponding to:\n\n3 – correct response in the definition task\n2 – correct response in the cued definition task\n1 – correct response in the recognition task\n0 – if the item wasn’t correctly defined or recognised\n\nFor the orthographic post-test, responses were scored as:\n\n1 – correct, if the target spelling was produced in full\n0 – incorrect\n\nHowever, the analysis reported by Ricketts et al. (2021) focused on the more sensitive Levenshtein distance measure (see following).\n\nWASImRS — Raw score – Matrix Reasoning subtest of the Wechsler Abbreviated Scale of Intelligence\nTOWREsweRS — Raw score – Sight Word Efficiency (SWE) subtest of the Test of Word Reading Efficiency; number of words read correctly in 45 seconds\nTOWREpdeRS — Raw score – Phonemic Decoding Efficiency (PDE) subtest of the Test of Word Reading Efficiency; number of nonwords read correctly in 45 seconds\nCC2regRS — Raw score – Castles and Coltheart Test 2; number of regular words read correctly\nCC2irregRS — Raw score – Castles and Coltheart Test 2; number of irregular words read correctly\nCC2nwRS — Raw score – Castles and Coltheart Test 2; number of nonwords read correctly\nWASIvRS — Raw score – vocabulary knowledge indexed by the Vocabulary subtest of the WASI-II\nBPVSRS — Raw score – vocabulary knowledge indexed by the British Picture Vocabulary Scale – Third Edition\nSpelling.transcription — Transcription of the spelling response produced by children in the orthographic post-test\nLevenshtein.Score — Children were asked to spell each word to dictation and spelling productions were transcribed for scoring. Responses were scored using a Levenshtein distance measure indexing the number of letter deletions, insertions and substitutions that distinguish between the target and child’s response. For example, the response ‘epegram’ for target ‘epigram’ attracts a Levenshtein score of 1 (one substitution). Thus, this score gives credit for partially correct responses, as well as entirely correct responses. The maximum score is 0, with higher scores indicating less accurate responses.\n\n(Notice that, for the sake of brevity, I do not list the z_ variables but these are explained in the study OSF repository materials.)\n\n\n\n\n\n\nWarning\n\n\n\nLevenshtein distance scores are higher if a child makes more errors in producing the letters in a spelling response.\n\nThis means that if we want to see what factors help a child to learn a word, including its spelling, then we want to see that helpful factors are associated with lower Levenshtein scores.\n\n\n\nTo demonstrate some of the processes we can enact to process and visualize data, and some of the benefits of doing so, we are going to work with the concurrent.orth_2020-08-11.csv dataset. These are data corresponding to the Ricketts et al. (2021) Study 2. concurrent refers to the analysis (a concurrent comparison) of data from younger and older children.\n\n\n\n4.7.3 Read the data into R\nAssuming you have downloaded the data files, we first read the dataset into the R environment: concurrent.orth_2020-08-11.csv. We do the data read in a bit differently than you have seen it done before; we will come back to what is going on (in Section 4.7.4.1).\n\nconc.orth &lt;- read_csv(\"concurrent.orth_2020-08-11.csv\",\n\n                      col_types = cols(\n\n                        Participant = col_factor(),\n                        Time = col_factor(),\n                        Study = col_factor(),\n                        Instructions = col_factor(),\n                        Version = col_factor(),\n                        Word = col_factor(),\n                        Orthography = col_factor(),\n                        Measure = col_factor(),\n                        Spelling.transcription = col_factor()\n\n                      ))\n\nWe can inspect these data using summary().\n\nsummary(conc.orth)\n\n  Participant   Time          Study         Instructions Version\n EOF001 :  16   1:1167   Study1&2:655   explicit  :592   a:543  \n EOF002 :  16            Study2  :512   incidental:575   b:624  \n EOF004 :  16                                                   \n EOF006 :  16                                                   \n EOF007 :  16                                                   \n EOF008 :  16                                                   \n (Other):1071                                                   \n         Word     Consistency_H     Orthography     Measure    \n Accolade  : 73   Min.   :0.9048   absent :583   Orth_sp:1167  \n Cataclysm : 73   1st Qu.:1.5043   present:584                 \n Contrition: 73   Median :1.9142                               \n Debacle   : 73   Mean   :2.3253                               \n Dormancy  : 73   3rd Qu.:3.0436                               \n Epigram   : 73   Max.   :3.9681                               \n (Other)   :729                                                \n     Score           WASImRS     TOWREsweRS      TOWREpdeRS       CC2regRS    \n Min.   :0.0000   Min.   : 5   Min.   :51.00   Min.   :19.00   Min.   :28.00  \n 1st Qu.:0.0000   1st Qu.:13   1st Qu.:69.00   1st Qu.:35.00   1st Qu.:36.00  \n Median :0.0000   Median :17   Median :74.00   Median :41.00   Median :38.00  \n Mean   :0.2913   Mean   :16   Mean   :74.23   Mean   :41.59   Mean   :36.91  \n 3rd Qu.:1.0000   3rd Qu.:19   3rd Qu.:80.00   3rd Qu.:50.00   3rd Qu.:39.00  \n Max.   :1.0000   Max.   :25   Max.   :93.00   Max.   :59.00   Max.   :40.00  \n                                                                              \n   CC2irregRS       CC2nwRS         WASIvRS          BPVSRS     \n Min.   :17.00   Min.   :13.00   Min.   :16.00   Min.   :103.0  \n 1st Qu.:23.00   1st Qu.:29.00   1st Qu.:25.00   1st Qu.:119.0  \n Median :25.00   Median :33.00   Median :29.00   Median :133.0  \n Mean   :25.24   Mean   :32.01   Mean   :29.12   Mean   :130.9  \n 3rd Qu.:27.00   3rd Qu.:37.00   3rd Qu.:33.00   3rd Qu.:142.0  \n Max.   :35.00   Max.   :40.00   Max.   :39.00   Max.   :158.0  \n                                                                \n Spelling.transcription Levenshtein.Score  zTOWREsweRS        zTOWREpdeRS      \n Epigram   : 57         Min.   :0.000     Min.   :-2.67807   Min.   :-2.33900  \n Platitude : 43         1st Qu.:0.000     1st Qu.:-0.60283   1st Qu.:-0.68243  \n Contrition: 42         Median :1.000     Median :-0.02638   Median :-0.06122  \n fracar    : 39         Mean   :1.374     Mean   : 0.00000   Mean   : 0.00000  \n Nonentity : 39         3rd Qu.:2.000     3rd Qu.: 0.66537   3rd Qu.: 0.87061  \n raconter  : 35         Max.   :7.000     Max.   : 2.16415   Max.   : 1.80243  \n (Other)   :912                                                                \n   zCC2regRS        zCC2irregRS          zCC2nwRS          zWASIvRS       \n Min.   :-3.3636   Min.   :-2.22727   Min.   :-3.1053   Min.   :-2.63031  \n 1st Qu.:-0.3435   1st Qu.:-0.60461   1st Qu.:-0.4920   1st Qu.:-0.82633  \n Median : 0.4115   Median :-0.06373   Median : 0.1614   Median :-0.02456  \n Mean   : 0.0000   Mean   : 0.00000   Mean   : 0.0000   Mean   : 0.00000  \n 3rd Qu.: 0.7890   3rd Qu.: 0.47716   3rd Qu.: 0.8147   3rd Qu.: 0.77721  \n Max.   : 1.1665   Max.   : 2.64070   Max.   : 1.3047   Max.   : 1.97986  \n                                                                          \n    zBPVSRS         mean_z_vocab       mean_z_read       zConsistency_H   \n Min.   :-1.9946   Min.   :-2.06910   Min.   :-2.39045   Min.   :-1.4153  \n 1st Qu.:-0.8495   1st Qu.:-0.85941   1st Qu.:-0.43321   1st Qu.:-0.8181  \n Median : 0.1525   Median :-0.01483   Median : 0.08829   Median :-0.4096  \n Mean   : 0.0000   Mean   : 0.00000   Mean   : 0.00000   Mean   : 0.0000  \n 3rd Qu.: 0.7967   3rd Qu.: 0.72964   3rd Qu.: 0.68438   3rd Qu.: 0.7157  \n Max.   : 1.9418   Max.   : 1.96083   Max.   : 1.52690   Max.   : 1.6368  \n                                                                          \n\n\nYou should notice one key bit of information in the summary. Focus on the summary for what is in the Participant column. You can see that we have a number of participants in this dataset, listed by Participant identity code in the summary() view e.g. EOF001. For each participant, we have 16 rows of data.\nWhen we ask R for a summary of a nominal variable or factor it will show us the levels of each factor (i.e., each category or class of objects encoded by the categorical variable), and a count for the number of observations for each level.\nTake a look at the rows of data for EOF001.\n\n\n\n\n\nParticipant\nTime\nStudy\nInstructions\nVersion\nWord\nConsistency_H\nOrthography\nMeasure\nScore\nWASImRS\nTOWREsweRS\nTOWREpdeRS\nCC2regRS\nCC2irregRS\nCC2nwRS\nWASIvRS\nBPVSRS\nSpelling.transcription\nLevenshtein.Score\nzTOWREsweRS\nzTOWREpdeRS\nzCC2regRS\nzCC2irregRS\nzCC2nwRS\nzWASIvRS\nzBPVSRS\nmean_z_vocab\nmean_z_read\nzConsistency_H\n\n\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nAccolade\n1.9142393\nabsent\nOrth_sp\n0\n15\n62\n33\n39\n27\n30\n26\n126\nacalade\n2\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n-0.4095955\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nCataclysm\n3.5060075\npresent\nOrth_sp\n1\n15\n62\n33\n39\n27\n30\n26\n126\nCataclysm\n0\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n1.1763372\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nContrition\n1.7486898\nabsent\nOrth_sp\n1\n15\n62\n33\n39\n27\n30\n26\n126\nContrition\n0\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n-0.5745381\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nDebacle\n2.9008386\npresent\nOrth_sp\n0\n15\n62\n33\n39\n27\n30\n26\n126\ndibarcle\n2\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n0.5733869\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nDormancy\n1.6263089\nabsent\nOrth_sp\n0\n15\n62\n33\n39\n27\n30\n26\n126\ndoormensy\n3\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n-0.6964704\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nEpigram\n1.3822337\npresent\nOrth_sp\n1\n15\n62\n33\n39\n27\n30\n26\n126\nEpigram\n0\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n-0.9396508\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nFoible\n2.7051987\npresent\nOrth_sp\n1\n15\n62\n33\n39\n27\n30\n26\n126\nFoible\n0\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n0.3784641\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nFracas\n3.1443345\nabsent\nOrth_sp\n0\n15\n62\n33\n39\n27\n30\n26\n126\nfracar\n1\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n0.8159901\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nLassitude\n0.9048202\npresent\nOrth_sp\n0\n15\n62\n33\n39\n27\n30\n26\n126\nlacitude\n2\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n-1.4153141\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nLuminary\n1.0985931\nabsent\nOrth_sp\n0\n15\n62\n33\n39\n27\n30\n26\n126\nloomenery\n4\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n-1.2222516\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nNonentity\n3.9681391\nabsent\nOrth_sp\n0\n15\n62\n33\n39\n27\n30\n26\n126\nnonenterty\n2\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n1.6367746\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nPlatitude\n0.9048202\npresent\nOrth_sp\n1\n15\n62\n33\n39\n27\n30\n26\n126\nPlatitude\n0\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n-1.4153141\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nPropensity\n1.6861898\nabsent\nOrth_sp\n0\n15\n62\n33\n39\n27\n30\n26\n126\npropencity\n1\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n-0.6368090\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nRaconteur\n3.8245334\nabsent\nOrth_sp\n0\n15\n62\n33\n39\n27\n30\n26\n126\nraconter\n1\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n1.4936954\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nSyncopation\n3.0436450\npresent\nOrth_sp\n0\n15\n62\n33\n39\n27\n30\n26\n126\nsincipation\n2\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n0.7156697\n\n\nEOF001\n1\nStudy1&2\nexplicit\na\nVeracity\n2.8693837\npresent\nOrth_sp\n0\n15\n62\n33\n39\n27\n30\n26\n126\nvaracity\n1\n-1.409869\n-0.8895032\n0.7889916\n0.4771563\n-0.3286222\n-0.6258886\n-0.3484719\n-0.4871803\n-0.2723693\n0.5420473\n\n\n\n\n\n\n\nYou can see that for EOF001, as for every participant, we have information on the conditions under which we observed their responses (Instructions, Orthography), as well as information about the stimuli that we asked participants to respond to (e.g., Word, Consistency_H), information about the responses or outcomes we recorded (Measure, Score, Spelling.transcription,  Levenshtein.Score), and information about the participants themselves (e.g., TOWREsweRS, TOWREpdeRS).\n\n\n4.7.4 Process the data\nWe almost always need to process data in order to render the information ready for discovery or communication data visualization.\n\n4.7.4.1 Specify column data types\nYou will have seen that data processing began when we first read the data in for use. Let’s go back and take a look at the code steps.\n\nconc.orth &lt;- read_csv(\"concurrent.orth_2020-08-11.csv\",\n\n                      col_types = cols(\n\n                        Participant = col_factor(),\n                        Time = col_factor(),\n                        Study = col_factor(),\n                        Instructions = col_factor(),\n                        Version = col_factor(),\n                        Word = col_factor(),\n                        Orthography = col_factor(),\n                        Measure = col_factor(),\n                        Spelling.transcription = col_factor()\n\n                        )\n                      )\n\nThe chunk of code is doing two things: first, we tell R what .csv file we want to read into the environment, and what we want to call the dataset; and then we tell R how we want to classify the data variable columns.\n\nconc.orth &lt;- read_csv(\"concurrent.orth_2020-08-11.csv\" first reads the named .csv file, creating an object I will call conc.orth: a dataset or tibble we can now work with in R.\n\n\nYou have been using the read.csv() function to read in data files.\nThe read_csv() function is the more modern tidyverse form of the function you were introduced to.\nBoth versions work in similar ways but read_csv() is a bit more efficient, and it allows us to do what we do next.\n\n\ncol_types = cols( ... ) tells R how to interpret some of the columns in the .csv.\n\n\nThe read_csv() function is excellent at working out what types of data are held in each column but sometimes we have to tell it what to do.\nHere, I am specifying with e.g. Participant = col_factor() that the Participant column should be treated as a categorical or nominal variable, a factor.\n\nUsing the col_types = cols( ... ) argument saves me from having to first read the data in then using code like the following to require, technically, coerce R into recognizing the nominal nature of variables like Participant with code like\n\nconc.orth$Participant &lt;- as.factor(conc.orth$Participant)\n\n\n4.7.4.1.1 Exercise\nI do not have to do step 2 of the read-in process, here. What happens if we use just read_csv()? Try it.\n\nconc.orth &lt;- read_csv(\"concurrent.orth_2020-08-11.csv\")\n\n\n\n4.7.4.1.2 Further information\nYou can read more about read_csv() here\nYou can read more about col_types = cols() here\n\n\n\n4.7.4.2 Extract information from the dataset\nThe Ricketts et al. (2021) dataset orth.conc is a moderately sized and rich dataset with several observations, on multiple variables, for each of many participants. Sometimes, we want to extract information from a more complex dataset because we want to understand or present a part of it, or a relatively simple account of it. We look at an example of how you might do that now.\nAs you saw when you looked at the summary of the orth.conc dataset, we have multiple rows of data for each participant. Recall the design of the study. For each participant, we recorded their response to a stimulus word, in a test of word learning, for 16 words.\nFor each participant, we have a separate row for each response the participant made to each word. But you will have noticed that information about the participant is repeated. So, for participant EOF001, we have data about their performance e.g. on the BPVSRS vocabulary test (they scored 126). Notice that that score is repeated: the same value is copied for each row, for this participant, in the BPVSRS column. The reason the data are structured like this are not relevant here 1 but it does require us to do some data processing, as I explain next.\nIt is a very common task to want to present a summary of the attributes of your participants or stimuli when you are reporting data in a report of a psychological research project. We could get a summary of the participant attributes using the psych library describe function as follows.\n\nconc.orth %&gt;%\n  select(WASImRS:BPVSRS) %&gt;%\n  describe(ranges = FALSE, skew = FALSE)\n\n           vars    n   mean    sd   se\nWASImRS       1 1167  16.00  4.30 0.13\nTOWREsweRS    2 1167  74.23  8.67 0.25\nTOWREpdeRS    3 1167  41.59  9.66 0.28\nCC2regRS      4 1167  36.91  2.65 0.08\nCC2irregRS    5 1167  25.24  3.70 0.11\nCC2nwRS       6 1167  32.01  6.12 0.18\nWASIvRS       7 1167  29.12  4.99 0.15\nBPVSRS        8 1167 130.87 13.97 0.41\n\n\nBut you can see that part of the information in the summary does not appear to make sense at first glance. We do not have 1167 participants in this dataset, as Ricketts et al. (2021) report.\nHow do we extract the participant attribute variable data for each unique participant code for the participants in our dataset?\n\nconc.orth.subjs &lt;- conc.orth %&gt;%\n  group_by(Participant) %&gt;%\n  mutate(mean.score = mean(Levenshtein.Score)) %&gt;%\n  ungroup() %&gt;%\n  distinct(Participant, .keep_all = TRUE) %&gt;%\n  select(WASImRS:BPVSRS, mean.score, Participant)\n\nWe create a new dataset conc.orth.subjs by taking conc.orth and piping it through a series of processing steps. As part of the process, we want to extract the data for each unique unique Participant identity code using distinct(). Along the way, we want to calculate the mean accuracy of response on the outcome measure (Score), that is, the average number of edits separating a child’s spelling of a target word from the correct spelling.\nThis is how we do it.\n\nconc.orth.subjs &lt;- ... tells R to create a new dataset conc.orth.subjs.\nconc.orth %&gt;% ... we do this by telling R to take conc.orth and pipe it through the following steps.\ngroup_by(Participant) %&gt;% first we group the data by Participant identity code.\nmutate(mean.score = mean(Score)) %&gt;% then we use mutate() to create the new variable mean.score by calculating the mean() of the Score variable values (i.e. the average score) for each participant. We then pipe to the next step.\nungroup() %&gt;% we tell R to ungroup the data because we want to work with all rows for what comes next, and we then pipe to the next step.\ndistinct(Participant, .keep_all = TRUE) %&gt;% requires R to extract from the full orth.conc dataset the set of (here, 16) data rows we have for each distinct (uniquely identified) Participant. We use the argument .keep_all = TRUE to tell R that we want to keep all columns. This requires the next step, so we tell R to pipe %&gt;% the data.\nselect(WASImRS:BPVSRS, mean.score, Participant) then tells R to select just the columns with information about participant attributes. (WASImRS:BPVSRS tells R to select every column between WASImRS and BPVSRS inclusive. mean.score, Participant tells R we also want those columns, specified by name, including the mean.score column of average response scores we calculated just earlier.\n\nWe can now get a sensible summary of the descriptive statistics for the participants in Study 2 of the Ricketts et al. (2021) investigation.\n\nconc.orth.subjs %&gt;%\n  select(-Participant) %&gt;%\n  describe(ranges = FALSE, skew = FALSE)\n\n           vars  n   mean    sd   se\nWASImRS       1 73  16.00  4.33 0.51\nTOWREsweRS    2 73  74.22  8.73 1.02\nTOWREpdeRS    3 73  41.58  9.73 1.14\nCC2regRS      4 73  36.90  2.67 0.31\nCC2irregRS    5 73  25.23  3.72 0.44\nCC2nwRS       6 73  32.00  6.17 0.72\nWASIvRS       7 73  29.12  5.02 0.59\nBPVSRS        8 73 130.88 14.06 1.65\nmean.score    9 73   1.38  0.62 0.07\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is exactly the kind of tabled summary of descriptive statistics we would expect to produce in a report, in a presentation of the participant characteristics for a study sample (in e.g., the Methods section).\nNotice:\n\nThe table has not yet been formatted according to APA rules.\nWe would prefer to use real words for row name labels instead of dataset variable column labels, e.g, replace TOWREsweRS with: “TOWRE word reading score”.\n\n\n\n\n4.7.4.2.1 Exercise {sec-vis-ex-1}\nIn these bits of demonstration code, we extract information relating just to participants. However, in this study, we recorded the responses participants made to 16 stimulus words, and we include in the dataset information about the word properties Consistency_H.\n\nCan you adapt the code you see here in order to calculate a mean score for each word, and then extract the word-level information for each distinct stimulus word identity?\n\n\n\n4.7.4.2.2 Further information {sec-vis-ex-1-further}\nYou can read more about the psych library, which is often useful, here. You can read more about the distinct() function here.\n\n\n\n\n4.7.5 Visualize the data: introduction\nIt has taken us a while but now we are ready to examine the data using visualizations. Remember, we are engaging in visualization to (1.) do discovery, to get a sense of our data, and maybe reveal unexpected aspects, and (2.) potentially to communicate to ourselves and others what we have observed or perhaps what insights we can gain.\nWe have been learning to use histograms, in other classes, so let’s start there.\n\n\n4.7.6 Examine the distributions of numeric variables\nWe can use histograms to visualize the distribution of observed values for a numeric variable. Let’s start simple, and then explore how to elaborate the plotting code, in a series of edits, to polish the plot presentation.\n\nggplot(data = conc.orth.subjs, aes(x = WASImRS)) +\n  geom_histogram()\n\n\n\n\n\n\n\nFigure 4.5: Distribution of WASImRS intelligence scores\n\n\n\n\n\nThis is how the code works.\n\nggplot(data = conc.orth.subjs, ... tells R what function to use ggplot() and what data to work with data = conc.orth.subjs.\naes(x = WASImRS) tells R what aesthetic mapping to use: we want to map values on the WASImRS variable (small to large) to locations on the x-axis (left to right).\ngeom_histogram() tells R to construct a histogram, presenting a statistical summary of the distribution of intelligence scores.\n\nWith histograms, we are visualizing the distribution of a single continuous variable by dividing the variable values into bins (i.e. subsets) and counting the number of observations in each bin. Histograms display the counts with bars.\nYou can see more information about geom_histogram here.\nFigure 4.5 shows how intelligence (WASImRS) scores vary in the Ricketts Study 2 dataset. Scores peak around 17, with a long tail of lower scores towards 5, and a maximum around 25.\n\nWhere I use the word “peak” I am talking about the tallest bar in the plot (or, later the highest point in a density curve). At this point, we have the most observations of the value under the bar. Here, we observed the score WASImRS \\(= 17\\) for the most children in this sample.\n\nA primary function of discovery visualization is to assess whether the distribution of scores on a variable is consistent with expectations, granted assumptions about a sample (e.g., that the children are typically developing). We would normally use research area knowledge to assess whether this distribution fits expectations for a sample of typically developing school-aged children in the UK. However, I shall leave that concern aside, here, so that we can focus on enriching the plot presentation, next.\nThere are two main problems with the plot:\n\nThe bars are “gappy” in the histogram, suggesting we have not grouped observed values in sufficiently wide subsets (bins). This is a problem because it weakens our ability to gain or communicate a visual sense of the distribution of scores.\nThe axis labeling uses the dataset variable name WASImRS but if we were to present the plot to others we could not expect them to know what that means.\n\nWe can fix both these problems, and polish the plot for presentation, through the following code steps.\n\nggplot(data = conc.orth.subjs, aes(x = WASImRS)) +\n  geom_histogram(binwidth = 2) +\n  labs(x = \"Scores on the Wechsler Abbreviated Scale of Intelligence\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 4.6: Distribution of WASImRS intelligence scores\n\n\n\n\n\nFigure 4.6 shows the same data, and furnishes us with the same picture of the distribution of intelligence scores but it is a bit easier to read. We achieve this by making three edits.\n\ngeom_histogram(binwidth = 2) + we change the binwidth.\n\n\nThis is so that more different observed values of the data variable are included in bins (subsets corresponding to bars) so that the bars correspond to information about a wider range of values.\nThis makes the bars bigger, wider, and closes the gaps.\nAnd this means we can focus the eyes of the audience for our plot on the visual impression we wish to communicate: the skewed distribution of intelligence scores.\n\n\nlabs(x = \"Scores on the Wechsler Abbreviated Scale of Intelligence\") + changes the label to something that should be understandable by people, in our audience, who do not have access to variable information (as we do) about the dataset.\ntheme_bw() we change the overall appearance of the plot by changing the theme.\n\n\n4.7.6.1 Exercise {sec-vis-ex-2}\nWe could, if we wanted, add a line and annotation to indicate the mean value, as you saw in Figure 4.2.\n\nCan you add the necessary code to indicate the mean value of WASI scores, for this plot?\n\nWe can, of course, plot histograms to indicate the distributions of other variables.\n\nCan you apply the histogram code to plot histograms of other variables?\n\n\n\n\n4.7.7 Comparing the distributions of numeric variables\nWe may wish to discover or communicate how values vary on dataset variables in two different ways. Sometimes, we need to examine how values vary on different variables. And sometimes, we need to examine how values vary on the same variable but in different groups of participants (or stimuli) or under different conditions. We look at this next. We begin by looking at how you might compare how values vary on different variables.\n\n4.7.7.1 Compare how values vary on different variables\nIt can be useful to compare the distributions of different variables. Why?\nConsider the Ricketts et al. (2021) investigation dataset. Like many developmental investigations (see also clinical investigations), we tested children and recorded their scores on a series of standardized measures, here, measures of ability on a range of dimensions. We did this, in part, to establish that the children in our sample are operating at about the level one might expect for typically developing children in cognitive ability dimensions of interest: dimensions like intelligence, reading ability or spelling ability. So, one of the aspects of the data we are considering is whether scores on these dimensions are higher or lower than typical threshold levels. But we also want to examine the distributions of scores because we want to find out:\n\nif participants are varied in ability (wide distribution) or if maybe they are all similar (narrow distribution) as would be the case if the ability measures are too easy (so all scores are at ceiling) or too hard (so all scores are at floor);\nif there are subgroups within the sample, maybe reflected by two or more peaks;\nif there are unusual scores, maybe reflected by small peaks at very low or very high scores.\n\nWe could look at each variable, one plot at a time. Instead, next, I will show you how to produce a set of histogram plots, and present them all as a single grid of plots.\n\n\n\n\n\n\nWarning\n\n\n\nI have to warn you that the way I write the code is not good practice. The code is written with repeats of the ggplot() block of code to produce each plot. This repetition is inefficient and leaves the coding vulnerable to errors because it is hard to spot a mistake in more code. What I should do is encapsulate the code as a function (see here). The reason I do not, here, is because I want to focus our attention on just the plotting.\n\n\nFigure 4.7 presents a grid of plots showing how scores vary for each ability test measure, for the children in the Ricketts et al. (2021) investigation dataset. We need to go through the code steps, next, and discuss what the plots show us (discovery and communication).\n\np.WASImRS &lt;- ggplot(data = conc.orth.subjs, aes(x = WASImRS)) +\n  geom_histogram(binwidth = 2) +\n  labs(x = \"WASI matrix\") +\n  theme_bw()\n\np.TOWREsweRS &lt;- ggplot(data = conc.orth.subjs, aes(x = TOWREsweRS)) +\n  geom_histogram(binwidth = 5) +\n  labs(x = \"TOWRE words\") +\n  theme_bw()\n\np.TOWREpdeRS &lt;- ggplot(data = conc.orth.subjs, aes(x = TOWREpdeRS)) +\n  geom_histogram(binwidth = 5) +\n  labs(x = \"TOWRE phonemic\") +\n  theme_bw()\n\np.CC2regRS &lt;- ggplot(data = conc.orth.subjs, aes(x = CC2regRS)) +\n  geom_histogram(binwidth = 2) +\n  labs(x = \"CC regular words\") +\n  theme_bw()\n\np.CC2irregRS &lt;- ggplot(data = conc.orth.subjs, aes(x = CC2irregRS)) +\n  geom_histogram(binwidth = 2) +\n  labs(x = \"CC irregular words\") +\n  theme_bw()\n\np.CC2nwRS &lt;- ggplot(data = conc.orth.subjs, aes(x = CC2nwRS)) +\n  geom_histogram(binwidth = 2) +\n  labs(x = \"CC nonwords\") +\n  theme_bw()\n\np.WASIvRS &lt;- ggplot(data = conc.orth.subjs, aes(x = WASIvRS)) +\n  geom_histogram(binwidth = 2) +\n  labs(x = \"WASI vocabulary\") +\n  theme_bw()\n\np.BPVSRS &lt;- ggplot(data = conc.orth.subjs, aes(x = BPVSRS)) +\n  geom_histogram(binwidth = 3) +\n  labs(x = \"BPVS vocabulary\") +\n  theme_bw()\n\np.mean.score &lt;- ggplot(data = conc.orth.subjs, aes(x = mean.score)) +\n  geom_histogram(binwidth = .25) +\n  labs(x = \"Mean orthographic test score\") +\n  theme_bw()\n\np.mean.score + p.BPVSRS + p.WASIvRS + p.WASImRS +\n  p.CC2nwRS + p.CC2irregRS + p.CC2regRS + \n  p.TOWREpdeRS + p.TOWREsweRS + plot_layout(ncol = 3)\n\n\n\n\n\n\n\nFigure 4.7: Distribution of childrens’ scores on ability measures\n\n\n\n\n\nThis is how the code works, step by step:\n\np.WASImRS &lt;- ggplot(...) first creates a plot object, which we call p.WASImRS.\nggplot(data = conc.orth.subjs, aes(x = WASImRS)) + tells R what data to use, and what aesthetic mapping to work with mapping the variable WASImRS here to the x-axis location.\ngeom_histogram(binwidth = 2) + tells R to sort the values of WASImRS scores into bins and create a histogram to show how many children in the sample present scores of different sizes.\nlabs(x = \"WASI matrix\") + changes the x-axis label to make it more informative.\ntheme_bw() changes the theme to make it a bit cleaner looking.\n\nWe do this bit of code separately for each variable. We change the plot object name, the x = variable specification, and the axis label text for each variable. We adjust the binwidth where it appears to be necessary.\nWe then use the following plot code to put all the plots together in a single grid.\n\np.mean.score + p.BPVSRS + p.WASIvRS + p.WASImRS +\n  p.CC2nwRS + p.CC2irregRS + p.CC2regRS + \n  p.TOWREpdeRS + p.TOWREsweRS + plot_layout(ncol = 3)\n\n\nIn the code, we add a series of plots together e.g. p.mean.score + p.BPVSRS + p.WASIvRS ...\nand then specify we want a grid of plots with a layout of three columns plot_layout(ncol = 3).\n\nThis syntax requires the library(patchwork) and more information about this very useful library can be found here.\nWhat do the plots show us?\nFigure 4.7 shows a grid of 9 histogram plots. Each plot presents the distribution of scores for the Ricketts et al. (2021) Study 2 participant sample on a separate ability measure, including scores on the BPVS vocabulary, WASI vocabulary, TOWRE words and TOWRE nonwords reading tests, as well as scores on the Castles and Coltheart regular words, irregular words and nonwords reading tests, and the mean Levenshtein distance (spelling score) outcome measure of performance for the experimental word learning post-test.\nTake a look, you may notice the following features.\n\nThe mean orthographic test score suggests that many children produced spellings to the words they learned in the Ricketts et al. (2021) study that, on average, were correct (0 edits) or were one or two edits (e.g., a letter deletion or replacement) away from the target word spelling. The children were learning the words, and most of the time, they learned the spellings of the words effectively. However, one or two children tended to produce spellings that were 2-3 edits distant from the target spelling.\n\n\nWe can see these features because we can see that the histogram peaks around 1 (at Levenshtein distance score \\(= 1\\)) but that there is a small bar of scores at around 3.\n\n\nWe can see that there are two peaks on the BPVS and WASI measures of vocabulary. What is going on there?\n\n\nIs it the case that we have two sub-groups of children within the overall sample? For example, on the BPVS test, maybe one sub-group of children has a distribution of vocabulary scores with a peak around 120 (the peak shows where most children have scores) while another sub-group of children has a distribution of vocabulary scores with a peak around 140.\n\n\nIf we look at the CC nonwords and CC regular words tests of reading ability, we may notice that while most children present relatively high scores on these tests (CC nonwords peak around 35, CC regular words peak around 37) there is a skewed distribution. Many of the children’s scores are piled up towards the maximum value in the data on the measures. But we can also see that, on both measures, there are long tails in the distributions because relatively small numbers of children have substantially lower scores.\n\n\nDevelopmental samples are often highly varied (just like clinical samples). Are all the children in the sample at the same developmental stage, or are they all typically developing?\n\n\n\n\n\n\n\nTip\n\n\n\nNotice that in presenting a grid of plots like this, we offer a compact visual way to present the same summary information we might otherwise present using a table of descriptive statistics. In some ways, this grid of plots is more informative than the descriptive statistics because the mean and SD values do not tell you what you can see:\n\nthe characteristics of the variation in values, like the presence of two peaks;\nor the presence of unusually high or low scores (for this sample).\n\n\n\nGrids of plots like this can be helpful to inspect the distributions of variables in a concise approach. They are not really too useful for comparing the distributions because they require your eyes to move between plots, repeatedly, to do the comparison.\nHere is a more compact way to code the grid of histograms using the library(ggridges) function geom_density_ridges(). I do not discuss it in detail because I want to focus your attention on core tidyverse functions (I show you more information in the Notes tab).\nNotice that if you produce all the plots so that the are in line in the same column with a shared x-axis it becomes much easier to compare the distributions of scores. You lose some of the fine detail, discussed in relation to Figure 4.7, but this style allows you to gain an impression, quickly, of how for distributions of scores compare between measures. For example, we can see that within the Castles and Coltheart (CC) measures of reading ability, children do better on regular words than on nonwords, and on nonwords better than on irregular words.\n\nPlotNotes\n\n\n\nlibrary(ggridges)\nconc.orth.subjs %&gt;%\n  pivot_longer(names_to = \"task\", values_to = \"score\", cols = WASImRS:mean.score) %&gt;% \n  ggplot(aes(y = task, x = score)) +\n  geom_density_ridges(stat = \"binline\", bins = 20, scale = 0.95, draw_baseline = FALSE) +\n  theme_ridges()\n\n\n\n\n\n\n\nFigure 4.8: Distribution of childrens’ scores on ability measures\n\n\n\n\n\n\n\n\nlibrary(ggridges) get the library we need.\nconc.orth.subjs %&gt;% pipe the dataset for processing.\npivot_longer(names_to = \"task\", values_to = \"score\", cols = WASImRS:mean.score) %&gt;% pivot the data so all test scores are in the same column, “scores” wwith coding for “task” name, and pipe to the next step for plotting.\nggplot(aes(y = task, x = score)) + create a plot for the scores on each task.\ngeom_density_ridges(stat = \"binline\", bins = 20, scale = 0.95, draw_baseline = FALSE) + show the plots as histograms.\ntheme_ridges() change the theme to the specific theme suitable for showing a grid of ridges.\n\nYou can find more information on ggridges here.\n\n\n\n\n\n4.7.7.2 Compare between groups how values vary on different variables\nWe will often want to compare the distributions of variable values between groups or between conditions. This need may appear when, for example, we are conducting a between-groups manipulation of some condition and we want to check that the groups are approximately matched on dimensions that are potentially linked to outcomes (i.e., on potential confounds). The need may appear when, alternatively, we have recruited or selected participant (or stimulus) samples and we want to check that the sample sub-groups are approximately matched or detectably different on one or more dimensions of interest or of concern.\nAs a demonstration of the visualization work we can do in such contexts, let’s pick up on an observation we made earlier, that there are two peaks on the BPVS and WASI measures of vocabulary. I asked: Is it the case that we have two sub-groups of children within the overall sample? Actually, we know the answer to that question because Ricketts et al. (2021) state that they recruited one set of children for their Study 1 and then, for Study 2:\n\nThirty-three children from an additional three socially mixed schools in the South-East of England were added to the Study 1 sample (total N = 74). These additional children were older (\\(M_{age}\\) = 12.57, SD = 0.29, 17 female)\n\nDo the younger (Study 1) children differ in any way from the older (additional) children?\nWe can check this through data visualization. Our aim is to present the distributions of variables side-by-side or superimposed to ensure easy comparison. We can do this in different ways, so I will demonstrate one approach with an outline explanation of the actions, and offer suggestions for further approaches.\nI am going to process the data before I do the plotting. I will re-use the code I used before (see Section 4.7.4.2) with one additional change. I will add a line to create a group coding variable. This addition shows you how to do an action that is very often useful in the data processing part of your workflow.\n\n4.7.7.2.1 Data processing\nYou have seen that the Ricketts et al. (2021) report states that an additional group of children was recruited for the investigation’s second study. How do we know who they are? If you recall the summary view of the complete dataset, there is one variable we can use to code group identity.\n\nsummary(conc.orth$Study)\n\nStudy1&2   Study2 \n     655      512 \n\n\nThis summary tells us that we have 512 observations concerning the additional group of children recruited for Study 2, and 655 observations for the (younger) children whose data were analyzed for both Study 1 and Study 2 (i.e., coded as Study1&2 in the Study variable column). We can use this information to create a coding variable. (If we had age data, we could use that instead but we do not.) This is how we do that.\n\nconc.orth.subjs &lt;- conc.orth %&gt;%\n  group_by(Participant) %&gt;%\n  mutate(mean.score = mean(Levenshtein.Score)) %&gt;%\n  ungroup() %&gt;%\n  distinct(Participant, .keep_all = TRUE) %&gt;%\n  mutate(age.group = fct_recode(Study,\n    \n    \"young\" = \"Study1&2\",\n    \"old\" = \"Study2\"\n    \n  )) %&gt;%\n  select(WASImRS:BPVSRS, mean.score, Participant, age.group)\n\nThe code block is mostly the same as the code I used in Section Section 4.7.4.2 to extract the data for each participant, with two changes:\n\nFirst, mutate(age.group = fct_recode(...) tells R that I want to create a new variable age.group through the process of recoding, with fct_recode(...) the variable I specify next, in the way that I specify.\nfct_recode(Study, ...) tells R I want to recode the variable Study.\n\"young\" = \"Study1&2\", \"old\" = \"Study2\" specifies what I want recoded.\n\n\nI am telling R to look in the Study column and (a.) whenever it finds the value Study1&2 replace it with young whereas (b.) whenever it finds the value Study2 replace it with old.\nNotice that the syntax in recoding is fct_recode: “new name” = “old name”.\nHaving done that, I tell R to pipe the data, including the recoded variable, to the next step.\n\n\nselect(WASImRS:BPVSRS, mean.score, Participant, age.group) where I add the new recoded variable to the selection of variables I want to include in the new dataset conc.orth.subjs.\n\n\n\n\n\n\n\nTip\n\n\n\nNotice that R handles categorical or nominal variables like Study (or, in other data, variables e.g. gender, education or ethnicity) as factors.\n\nWithin a classification scheme like education, we may have different classes or categories or groups e.g. “further, higher, school”. We can code these different classes with numbers (e.g. \\(school = 1\\)) or with words “further, higher, school”. Whatever we use, the different classes or groups are referred to as levels and each level has a name.\nIn factor recoding, we are changing level names while keeping the underlying data the same.\n\n\n\nThe tidyverse collection includes the forcats library of functions for working with categorical variables (forcats = factors). These functions are often very useful and you can read more about them here.\nChanging factors level coding by hand is, for many, a common task, and the fct_recode() function makes it easy. You can find the technical information on the function, with further examples, here.\n\n\n4.7.7.2.2 Group comparison visualization\nThere are different ways to examine the distributions of variables so that we can compare the distributions of the same variable between groups.\nFigure 4.9 presents some alternatives as a grid of 4 different kinds of plots designed to enable the same comparison. Each plot presents the distribution of scores for the Ricketts et al. (2021) Study 2 participant sample on the BPVS vocabulary measure so that we can compare the distribution of vocabulary scores between age groups.\nThe plots differ in method using:\n\nfacetted histograms showing the distribution of vocabulary scores, separately for each group, in side-by-side histograms for comparison;\nboxplots, showing the distribution of scores for each group, indicated by the y-axis locations of the edges of the boxes (25% and 75% quartiles) and the middle lines (medians);\nsuperimposed histograms, where the histograms for the separate groups are laid on top of each other but given different colours to allow comparison; and\nsuperimposed density plots where the densities for the separate groups are laid on top of each other but given different colours to allow comparison.\n\n\n\n\n\n\n\nTip\n\n\n\nThere is one thing you should notice about all these plots.\n\nIt looks like the BPVS vocabulary scores have their peak – most children show this value – at around 120 for the young group and at around 140 for the old group.\n\nWe return to this shortly.\n\n\n\nI am going to hide the coding and the explanation of the coding behind the Notes tab. Click on the tab to get a step-by-step explanation. Of these alternatives, I focus on one which I explain in more depth, following: d. Superimposed density plots.\n\nPlotNotes\n\n\n\n\n\n\n\n\n\n\nFigure 4.9: Distribution of childrens’ scores on the BPVS vocabulary measure: distributions are compared between the younger and older age groups\n\n\n\n\n\n\n\n\np.facet.hist &lt;- ggplot(data = conc.orth.subjs, aes(x = BPVSRS)) +\n  geom_histogram(binwidth = 5) +\n  labs(x = \"BPVS vocabulary score\", title = \"a. Faceted histograms\") +\n  facet_wrap(~ age.group) +\n  theme_bw()\n\np.colour.boxplot &lt;- ggplot(data = conc.orth.subjs, aes(y = BPVSRS, colour = age.group)) +\n  geom_boxplot() +\n  labs(x = \"BPVS vocabulary score\", title = \"b. Boxplots\") +\n  theme_bw()\n\np.colour.hist &lt;- ggplot(data = conc.orth.subjs, aes(x = BPVSRS, colour = age.group, fill = age.group)) +\n  geom_histogram(binwidth = 5) +\n  labs(x = \"BPVS vocabulary score\", title = \"c. Superimposed histograms\") +\n  theme_bw()\n\np.colour.density &lt;- ggplot(data = conc.orth.subjs, aes(x = BPVSRS, colour = age.group, fill = age.group)) +\n  geom_density(alpha = .5, size = 1.5) +\n  labs(x = \"BPVS vocabulary score\", title = \"d. Superimposed density plots\") +\n  theme_bw()\n\np.facet.hist + p.colour.boxplot + p.colour.hist + p.colour.density\n\n\nIn plot “a. Faceted histograms”, we use the code to construct a histogram but the difference is we use:\n\n\nfacet_wrap(~ age.group) to tell R to split the data by age.group then present the histograms indicating vocabulary score distributions separately for each group.\n\n\nIn plot “b. Boxplots”, we use the geom_boxplot() code to construct a boxplot to summarize the distributions of vocabulary scores – as you have seen previously – but the difference is we use:\n\n\naes(y = BPVSRS, colour = age.group) to tell R to assign different colours to different levels of age.group to help distinguish the data from each group.\n\n\nIn plot “c. Superimposed histograms”, we use the code to construct a histogram but the difference is we use:\n\n\naes(x = BPVSRS, colour = age.group, fill = age.group) to tell R to assign different colours to different levels of age.group to help distinguish the data from each group.\nNotice that the fill gives the colour inside the bars and colour gives the colour of the outline edges of the bars.\n\n\nIn plot “d. Superimposed density plots”, we use the code geom_density(...) to construct what is called a density plot.\n\n\nA density plot presents a smoothed histogram to show the distribution of variable values.\nWe add arguments in geom_density(alpha = .5, size = 1.5) to adjust the thickness of the line (size = 1.5) drawn to show the shape of the distribution and adjust the transparency of the colour fill inside the line alpha = .5).\nWe useaes(x = BPVSRS, colour = age.group, fill = age.group) to tell R to assign different colours to different levels of age.group to help distinguish the data from each group.\nNotice that the fill gives the colour inside the density plots and colour gives the colour of the outline edges of the densities.\n\n\n\n\nDensity plots can be helpful when we wish to compare distributions. This is because we can superimpose distribution plots on top of each other, enabling us or our audience to directly compare the distributions: directly because the distributions are shown on the same scale, in the same image.\nWe can (roughly) understand a density plot as working like a smoothed version of the histogram. Imagine how the heights of the bars in the histogram represent how many observations we have of the values in a particular bin. If we draw a smooth curving line through the tops of the bars then we are representing the chances that an observation in our sample has a value (the value under the curve) at any specific location on the x-axis. You can see that in Figure 4.10.\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\nFigure 4.10: Distribution of childrens’ scores on the BPVS vocabulary measure. The figure shows the histogram versus density plot representation of the same data distribution\n\n\n\n\n\nYou can find the ggplot2 reference information on the geom_density() function, with further examples, here. You can find technical information on density functions here and here.\nWe can develop the density plot to enrich the information we can discover or communicate through the plot. Figure 4.11 shows the distribution of scores on both the BPVS and WASI vocabulary knowledge measures.\n\np.BPVSRS.density &lt;- ggplot(data = conc.orth.subjs, aes(x = BPVSRS, colour = age.group, fill = age.group)) +\n  geom_density(alpha = .5, size = 1.5) +\n  geom_rug(alpha = .5) +\n  geom_vline(xintercept = 120, linetype = \"dashed\") +\n  geom_vline(xintercept = 140, linetype = \"dotted\") +\n  labs(x = \"BPVSRS vocabulary score\") +\n  theme_bw()\n\np.WASIvRS.density &lt;- ggplot(data = conc.orth.subjs, aes(x = WASIvRS, colour = age.group, fill = age.group)) +\n  geom_density(alpha = .5, size = 1.5) +\n  geom_rug(alpha = .5) +\n  labs(x = \"WASI vocabulary score\") +\n  theme_bw()\n\np.BPVSRS.density + p.WASIvRS.density + plot_layout(guides = 'collect')\n\n\n\n\n\n\n\nFigure 4.11: Distribution of childrens’ scores on the BPVS and WASI vocabulary measures.\n\n\n\n\n\nHere is what the code does:\n\np.BPVRS.density &lt;- ggplot(...) creates a plot object called p.BPVRS.density.\ndata = conc.orth.subjs, ... says we use the conc.orth.subjs dataset to do this.\naes(x = BPVRS, colour = age.group, fill = age.group)) + says we want to map BPVRS scores to x-axis location, and age.group level coding (young, old) to both colour and fill.\ngeom_density(alpha = .5, size = 1.5) + draws a density plot; note that we said earlier what we want for colour and fill but here we also say that:\n\n\nalpha = .5 we want the fill to be transparent;\nsize = 1.5 we want the density curve line to be thicker than usual.\n\n\ngeom_rug(alpha = .5) + adds a one-dimensional plot, a series of tick marks, to show where we have observations of BPVRS scores for specific children. We ask R to make the tick marks semi-transparent.\ngeom_vline(xintercept = 120, linetype = \"dashed\") + draws a vertical dashed line where BPVRS = 120.\ngeom_vline(xintercept = 140, linetype = \"dotted\") + draws a vertical dotted line where BPVRS = 140.\nlabs(x = \"BPVS vocabulary score\") + makes the x-axis label something understandable to someone who does not know about the study.\ntheme_bw() changes the theme.\n\n\n\n4.7.7.2.3 Critical evaluation: discovery and communication\nAs we work with visualization, we should aim to develop skills in reading plots, so:\n\nWhat do we see?\n\nWhen we look at Figure 4.11, we can see that the younger and older children in the Ricketts et al. (2021) sample have broadly overlapping distributions of vocabulary scores. However, as we have noticed previously, the peak of the distribution is a bit lower for the younger children compared to the older children. This appears to be the case whether we are looking at the BPVS or at the WASI measures of vocabulary, suggesting that the observation does not depend on the particular vocabulary test. Is this observation unexpected? Probably not, as we should hope to see vocabulary knowledge increase as children get older. Is this observation a problem for our analysis? You need to read the paper to find out what we decided.\n\n\n4.7.7.2.4 Exercise {sec-vis-ex-3}\nIn the demonstration examples, I focused on comparing age groups on vocabulary, what about the other measures?\nI used superimposed density plots: are other plotting styles more effective, for you? Try using boxplots or superimposed or faceted histograms instead.\n\n\n\n\n4.7.8 Summary: Visualizing distributions\nSo far, we have looked at how and why we may examine the distributions of numeric variables. We have used histograms to visualize the distribution of variable values. We have explored the construction of grids of plots to enable the quick examination or concise communication of information about the distributions of multiple variables at the same time. And we have used histograms, boxplots and density plots to examine how the distributions of variables may differ between groups.\nThe comparison of the distributions of variable values in different groups (or, similarly, between different conditions) may be the kind of work we would need to do, in data visualization, as part of an analysis ending in, for example, a t-test comparison of mean values.\nWhile boxplots, density plots and histograms are typically used to examine how the values of a numeric variable vary, scatterplots are typically used when we wish to examine, to make sense of or communicate potential associations or relations between two (or more) numeric variables. We turn to scatterplots, next.\n\n\n4.7.9 Examine the associations between numeric variables\nMany of us start learning about scatterplots in high school math classes. Using the modern tools made available to us through the ggplot2 library (as part of tidyverse), we can produce effective, nice-looking, scatterplots for a range of discovery or communication scenarios.\nWe continue working with the Ricketts et al. (2021) dataset. In the context of the Ricketts et al. (2021) investigation, there is interest in how children vary in the reading, spelling and vocabulary abilities that may influence the capacity of children to learn new words. So, in this context, we can begin to progress our development in visualization skills by usefully considering the potential association between participant attributes in the Study 2 sample.\nLater on, we will look at more advanced plots that help us to communicate the impact of the experimental manipulations implemented by Ricketts et al. (2021), and also to discover the ways that these impacts may vary between children.\n\n4.7.9.1 Getting started: Scatterplot basics\nWe can begin by asking a simple research question we can guess the answer to:\n\nDo vocabulary knowledge scores on two alternative measures, the BPVS and the WASI, relate to each other?\n\nIf two measurement instruments or tests are intended to measure individual differences in the same psychological attribute, here, vocabulary knowledge, then we would reasonably expect that scores on one test should covary with scores on the second test.\n\nggplot(data = conc.orth.subjs, aes(x = WASIvRS, y = BPVSRS)) +\n  geom_point() +\n  labs(x = \"WASI vocabulary score\", \n       y = \"BPVSRS vocabulary score\",\n       title = \"Are WASI and BPVS vocabulary scores associated?\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 4.12: Scatterplot indicating the potential association of childrens’ scores on the BPVS and WASI vocabulary measures.\n\n\n\n\n\nWhat does the plot show us?\nAs a reminder of how scatterplots work, we can recall that they present integrated information. Each point, for the Ricketts et al. (2021) data, represents information about both the BPVS and the WASI score for each child.\n\nThe vertical height of a point tells us the BPVS score recorded for a child: higher points represent higher scores.\nThe left-to-right horizontal position of the same point tells us the WASI score for the same child: points located more on the right represent higher scores.\n\nFigure 4.12 is a scatterplot comparing variation in childrens’ scores on the BPVS and WASI vocabulary measures: variation in BPVS scores are shown on the y-axis and variation in WASI scores are shown on the x-axis. Critically, the scientific insight the plot gives us is this: higher WASI scores are associated with higher BPVS scores.\nHow does the code work? We have seen scatterplots before but, to ensure we are comfortable with the coding, we can go through them step by step.\n\nggplot(data = conc.orth.subjs...) + tells R we want to produce a plot using ggplot() with the conc.orth.subjs dataset.\naes(x = WASIvRS, y = BPVSRS) tells R that, in the plot, WASIvRS values are mapped to x-axis (horizontal) position and BPVSRS values are mapped to y-axis (vertical) position.\ngeom_point() + constructs a scatterplot, using these data and these position mappings.\nlabs(x = \"WASI vocabulary score\", ... fixes the x-axis label.\ny = \"BPVSRS vocabulary score\",... fixes the y-axis label.\ntitle = \"Are WASI and BPVS vocabulary scores associated?\") + fixes the title.\ntheme_bw() changes the theme.\n\n\n\n4.7.9.2 Building complexity: adding information step by step\nFor this pair of variables in this dataset, the potential association in the variation of scores is quite obvious. However, sometimes it is helpful to guide the audience by imposing a smoother. There are different ways to do this, for different objectives and in different contexts. Here, we look at two different approaches. In addition, as we go, we examine how to adjust the appearance of the plot to address different potential discovery or communication needs.\nWe begin by adding what is called a LOESS smoother.\n\nggplot(data = conc.orth.subjs, aes(x = WASIvRS, y = BPVSRS)) +\n  geom_point() +\n  geom_smooth() +\n  labs(x = \"WASI vocabulary score\", \n       y = \"BPVSRS vocabulary score\",\n       title = \"Are WASI and BPVS vocabulary scores associated?\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 4.13: Scatterplot indicating the potential association of childrens’ scores on the BPVS and WASI vocabulary measures.\n\n\n\n\n\nThe only coding difference between this plot Figure 4.13 and the previous plot Figure 4.12 appears at line 3:\n\ngeom_smooth()\n\nThe addition of this bit of code results in the addition of the curving line you see in Figure 4.13. The blue line is curving, and visually suggests that the relation between BPVS and WASI scores is different – sometimes more sometimes less steep – for different values of WASI vocabulary score.\nThis line is generated by the geom_smooth() code, by default, in an approach in which the dataset is effectively split into sub-sets, dividing the data up into sub-sets from the lowest to the highest WASI scores, and the predicted association between the y-axis variable (here, BPVS score) and the x-axis variable (here, WASI score) is calculated bit by bit, in a series of regression analyses, working in order through sub-sets of the data. This calculation of what is called the LOESS (locally estimated scatterplot smoothing) trend is done by ggplot for us. And this approach to visualizing the trend in a potential association between variables is often a helpful way to discover curved or non-linear relations.\nYou can find technical information on geom_smooth() here and an explanation of LOESS here.\nFor us, this default visualization is not helpful for two reasons:\n\nWe have not yet learned about linear models, so learning about LOESS comes a bit early in our development.\nIt is hard to look at Figure 4.13 and identify a convincing curvilinear relation between the two variables. A lot of the curve for low WASI scores appears to be linked to the presence of a small number of data points.\n\nAt this stage, it is more helpful to adjust the addition of the smoother. We can do that by adding an argument to the geom_smooth() function code.\n\nggplot(data = conc.orth.subjs, aes(x = WASIvRS, y = BPVSRS)) +\n  geom_point() +\n  geom_smooth(method = 'lm') +\n  labs(x = \"WASI vocabulary score\", \n       y = \"BPVSRS vocabulary score\",\n       title = \"Are WASI and BPVS vocabulary scores associated?\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 4.14: Scatterplot indicating the potential association of childrens’ scores on the BPVS and WASI vocabulary measures.\n\n\n\n\n\nNotice the difference between Figure 4.13 and Figure 4.14:\n\ngeom_smooth(method = 'lm') tells R to draw a trend line, a smoother, using the lm method.\n\nThe lm method requires R to estimate the association between the two variables, here, BPVS and WASI, assuming a linear model. Of course, we are going to learn about linear models but, in short, right now, what we need to know is that we assume a “straight line” relationship between the variables. This assumption requires that for any interval of WASI scores – e.g., whether we are talking about WASI scores between 20-25 or about WASI scores between 30-35 – the relation between BPVS and WASI scores has the same shape: the direction and steepness of the slope of the line is the same.\n\n\n4.7.9.3 Exercise {sec-vis-ex-4}\n\nDeveloping skill in working with data visualizations is not just about developing coding skills, it is also about developing skills in reading, and critically evaluating, the information the plots we produce show us.\n\nStop and take a good look at the scatterplot in Figure 4.14. Use the visual representation of data to critically evaluate the potential association between the BPVS and WASI variables. What can you see?\nYou can train your critical evaluation by asking yourself questions like the following:\n\nHow does variation in the x-axis variable relate to variation in values of the y-axis variable?\n\n\nWe can see, here, that higher WASI scores are associated with higher BPVS scores.\n\n\nHow strong is the relation?\n\n\nThe strength of the relation can be indicated by the steepness of the trend indicated by the smoother, here, the blue line.\nIf you track the position of the line, you can see, for example, that going from a WASI score of 20 to a WASI score of 40 is associated with going from a BPVS score of a little over 110 to a BPVS score of about a 150.\nThat seems like a big difference.\n\n\nHow well does the trend we are looking at capture the data in our sample?\n\n\nHere, we are concerned with how close the points are to the trend line.\nIf the trend line represents a set of predictions about how the BPVS scores vary (in height) given variation in WASI scores, we can see that in places the prediction is not very good.\nTake a look at the points located at WASI 25. We can see that there there are points indicating that different children have the same WASI score of 25 but BPVS scores ranging from about 115 to 140.\n\n\n\n4.7.9.4 Polish the appearance of a plot for presentation\nFigure 4.14 presents a satisfactory looking plot but it is worth checking what edits we can make to the appearance of the plot, to indicate some of the ways that you can exercise choice in determining what a plot looks like. This will be helpful to you when you are constructing plots for presentation and report and you want to ensure the plots are as effective as possible.\n\nggplot(data = conc.orth.subjs, aes(x = WASIvRS, y = BPVSRS)) +\n  geom_point(alpha = .5, size = 2) +\n  geom_smooth(method = 'lm', colour = \"red\", size = 1.5) +\n  labs(x = \"WASI vocabulary score\", \n       y = \"BPVSRS vocabulary score\",\n       title = \"Are WASI and BPVS vocabulary scores associated?\") +\n  xlim(0, 40) + ylim(0, 160) +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 4.15: Scatterplot indicating the potential association of childrens’ scores on the BPVS and WASI vocabulary measures.\n\n\n\n\n\nIf you inspect the code, you can see that I have made three changes:\n\ngeom_point(alpha = .5, size = 2 changes the size of the points and their transparency (using alpha).\ngeom_smooth(method = 'lm', colour = \"red\", size = 1.5) change the colour of the smoother line, and the thickness (size) of the line.\nxlim(0, 40) + ylim(0, 160) changes the axis limits.\n\nThe last step — changing the axis limits — reveals how the sample data can be understood in the context of possible scores on these ability measures. Children could get BPVS scores of 0 or WASI scores of 0. By showing the start of the axes we get a more realistic sense of how our sample compares to the possible ranges of scores we could see in the wider population of children. This perhaps offers a more honest or realistic visualization of the potential association between BPVS and WASI vocabulary scores.\n\n\n4.7.9.5 Examining associations among multiple variables\nAs we have seen previously, we can construct a series of plots and present them all at once in a grid or lattice. Figure 4.16 presents just such a grid: of scatterplots, indicating a series of potential associations.\nLet’s suppose that we are primarily interested in what factors influence the extent to which children in the Ricketts et al. (2021) word learning experiment are able to correctly spell the target words they were given to learn. As explained earlier, in Section 4.7.2, Ricketts et al. (2021) examined the spellings produced by participant children in response to target words, counting how many string edits (i.e., letter deletions etc.) separated the spelling each child produced from the target spelling they should have produced.\nWe can calculate the mean spelling accuracy score for each child, over all the target words we observed their response to. We can identify mean spelling score as the outcome variable. We can then examine whether the outcome spelling scores are or are not influenced by participant attributes like vocabulary knowledge.\nFigure 4.16 presents a grid of scatterplots indicating the potential association between mean spelling score and each of the variables we have in the conc.orth dataset, including the Castles and Coltheart (CC) and TOWRE measures of word or nonword reading skill, WASI and BPVS measures of vocabulary knowledge, and the WASI matrix measure of intelligence, as well as (our newly coded) age group factor.\nI hide an explanation of the coding behind the Notes tab, because we have seen how to produce grids of plots, but you can take a look if you want to learn how the plot is produced.\n\nPlotNotes\n\n\n\n\n\n\n\n\n\n\nFigure 4.16: Grid of scatterplots showing the potential association between mean spelling score, for each child, and variation in the Castles and Coltheart (CC) and TOWRE measures of word or nonword reading skill, WASI and BPVS measures of vocabulary knowledge, the WASI matrix measure of intelligence, and age group factor\n\n\n\n\n\n\n\nThe code to produce the figure is set out as follows.\n\np.wordsvsmean.score &lt;- ggplot(data = conc.orth.subjs, \n                              aes(x = TOWREsweRS, \n                              y = mean.score)) +\n  geom_point(alpha = .5, size = 3) +\n  geom_smooth(method = 'lm', size = 1.5) +\n  labs(x = \"Word reading\", \n       y = \"Spelling score\",\n       title = \"(a.)\") +\n  theme_bw()\n\np.nonwordsvsmean.score &lt;- ggplot(data = conc.orth.subjs, \n                              aes(x = TOWREsweRS, \n                                  y = mean.score)) +\n  geom_point(alpha = .5, size = 3) +\n  geom_smooth(method = 'lm', size = 1.5) +\n  labs(x = \"Nonword reading\", \n       y = \"Spelling score\",\n       title = \"(b.)\") +\n  theme_bw()\n\np.WASIvRSvsmean.score &lt;- ggplot(data = conc.orth.subjs, \n                              aes(x = WASIvRS, \n                                  y = mean.score)) +\n  geom_point(alpha = .5, size = 3) +\n  geom_smooth(method = 'lm', size = 1.5) +\n  labs(x = \"WASI vocabulary\", \n       y = \"Spelling score\",\n       title = \"(c.)\") +\n  theme_bw()\n\np.BPVSRSvsmean.score &lt;- ggplot(data = conc.orth.subjs, \n                              aes(x = BPVSRS, \n                                  y = mean.score)) +\n  geom_point(alpha = .5, size = 3) +\n  geom_smooth(method = 'lm', size = 1.5) +\n  labs(x = \"BPVS vocabulary score\", \n       y = \"Spelling score\",\n       title = \"(d.)\") +\n  theme_bw()\n\np.WASImRSvsmean.score &lt;- ggplot(data = conc.orth.subjs, \n                              aes(x = WASImRS, \n                                  y = mean.score)) +\n  geom_point(alpha = .5, size = 3) +\n  geom_smooth(method = 'lm', size = 1.5) +\n  labs(x = \"WASI matrix\", \n       y = \"Spelling score\",\n       title = \"(e.)\") +\n  theme_bw()\n\np.CC2regRSvsmean.score &lt;- ggplot(data = conc.orth.subjs, \n                              aes(x = CC2regRS, \n                                  y = mean.score)) +\n  geom_point(alpha = .5, size = 3) +\n  geom_smooth(method = 'lm', size = 1.5) +\n  labs(x = \"CC regular words\", \n       y = \"Spelling score\",\n       title = \"(f.)\") +\n  theme_bw()\n\np.CC2irregRSvsmean.score &lt;- ggplot(data = conc.orth.subjs, \n                              aes(x = CC2irregRS, \n                                  y = mean.score)) +\n  geom_point(alpha = .5, size = 3) +\n  geom_smooth(method = 'lm', size = 1.5) +\n  labs(x = \"CC irregular words\", \n       y = \"Spelling score\",\n       title = \"(g.)\") +\n  theme_bw()\n\np.CC2nwRSvsmean.score &lt;- ggplot(data = conc.orth.subjs, \n                              aes(x = CC2nwRS, \n                                  y = mean.score)) +\n  geom_point(alpha = .5, size = 3) +\n  geom_smooth(method = 'lm', size = 1.5) +\n  labs(x = \"CC nonwords\", \n       y = \"Spelling score\",\n       title = \"(h.)\") +\n  theme_bw()\n\np.age.groupvsmean.score &lt;- ggplot(data = conc.orth.subjs, \n                              aes(x = age.group, \n                                  y = mean.score)) +\n  geom_boxplot() +\n  labs(x = \"Age group\", \n       y = \"Spelling score\",\n       title = \"(i.)\") +\n  theme_bw()\n\np.wordsvsmean.score + p.nonwordsvsmean.score + p.WASIvRSvsmean.score +\n  p.BPVSRSvsmean.score + p.WASImRSvsmean.score + p.CC2regRSvsmean.score +\n  p.CC2irregRSvsmean.score + p.CC2nwRSvsmean.score + p.age.groupvsmean.score\n\n\nTo produce the grid of plots, we first create a series of plot objects using code like that shown in the chunk.\n\n\np.wordsvsmean.score &lt;- ggplot(data = conc.orth.subjs, \n                              aes(x = TOWREsweRS, \n                              y = mean.score)) +\n  geom_point(alpha = .5, size = 3) +\n  geom_smooth(method = 'lm', size = 1.5) +\n  labs(x = \"Word reading\", \n       y = \"Spelling score\",\n       title = \"(a.)\") +\n  theme_bw()\n\n\np.wordsvsmean.score &lt;- ggplot(...) creates the plot.\ndata = conc.orth.subjs tells R what data to work with.\naes(x = TOWREsweRS, y = mean.score) specifies the aesthetic data mappings.\ngeom_point(alpha = .5, size = 3) tells R to produce a scatterplot, specifying the size and transparency of the points.\ngeom_smooth(method = 'lm', size = 1.5) tells R to add a smoother, specifying the method and the thickness of the line.\nlabs(x = \"Word reading\", y = \"Spelling score\", title = \"(a.)\") fixes the labels.\ntheme_bw() adjusts the theme.\n\n\nWe then put the plots together, using the patchwork syntax where we list the plot objects by name, separating each name by a +.\n\n\np.BPVSRSvsmean.score + p.WASImRSvsmean.score + p.CC2regRSvsmean.score +\n  p.CC2irregRSvsmean.score + p.CC2nwRSvsmean.score + p.age.groupvsmean.score\n\n\n\n\nFigure 4.16 allows us to visually represent the potential association between an outcome measure, the average spelling score, and a series of other variables that may or may not have an influence on that outcome. Using a grid in this fashion allows us to compare the extent to which different variables appear to have an influence on the outcome. We can see, for example, that measures of variation in word reading skill appear to have stronger association (the trend lines are more steeply slowed) than measures of vocabulary knowledge or intelligence, or age group.\nUsing grids of plots like this allow us to compactly communicate these potential associations in a single figure.\n\n\n\n\n\n\nWarning\n\n\n\nLevenshtein distance scores are higher if a child makes more errors in producing the letters in a spelling response.\n\nThis means that if we want to see what factors help a child to learn a word, including its spelling, then we want to see that helpful factors are associated with lower Levenshtein scores.\n\n\n\n\n\n\n4.7.10 Answering a scientific question: Visualize the effects of experimental conditions\nAs explained in Section 4.7.2, in the Ricketts et al. (2021) study, we taught children taught 16 novel words in a study with a 2 x 2 factorial design. The presence of orthography (orthography absent vs. orthography present) was manipulated within participants: for all children, eight of the words were taught with orthography (the word spelling) present and eight with orthography absent. Instructions (incidental vs. explicit) were manipulated between participants such that children in the explicit condition were alerted to the presence of orthography whereas children in the incidental condition were not. The Ricketts et al. (2021) investigation was primarily concerned with the effects on word learning of presenting words for learning with or without showing the words with their spellings, with or without instructing students explicitly that they would be helped by the presence of the spellings.\nWe can analyze the effects of orthography and instruction using a linear model.\n\nmodel &lt;- lm(Levenshtein.Score ~ Instructions*Orthography, data = conc.orth)\n\nThe model code estimates variation in spelling score (values of the Levenshtein.Score) variable, given variation in the levels of the Instructions and Orthography factors, and their interaction.\nThis model is a limited approximation of the analysis we would need to do with these data to estimate the effects of orthography and instruction; see Ricketts et al. (2021) for more information on what analysis is required (in our view). However, it is good enough as a basis for exploring the kind of data visualization work — in terms of both discovery and communication — that you can do when you are working with data from an experimental study.\nWe can get a summary of the model results which presents the estimated effect of each experimental factor. These estimates represent the predicted change in spelling score, given variation in Orthography (present, absent) or Instruction (explicit, incidental), and given the possibility that the effect of the presence of orthography is different for different levels of instruction.\nNotice that some of the p-values are incorrectly shown as 0.000. This is a result of using functions to automatically take a model summary and generate a table. I am going to leave this error with a warning because our focus is on visualization, next.\n\n\n\nModel summary\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n1.584\n0.072\n21.857\n0.000\n\n\nInstructionsincidental\n-0.041\n0.103\n-0.396\n0.692\n\n\nOrthographypresent\n-0.409\n0.103\n-3.987\n0.000\n\n\nInstructionsincidental:Orthographypresent\n0.060\n0.146\n0.409\n0.683\n\n\n\n\n\n\n\nVery often, when we complete a statistical analysis of outcome data, in which we estimate or test the effects on outcomes of variation in some variables or of variation in experimental conditions, then we present a table summary of the analysis results. However, these estimates are typically difficult to interpret (it gets easier with practice) and talk about. Take a look at the summary table. We are often to focus on whether effects are significant or not significant. But, really, what we should consider is how much the outcome changes given the different experimental conditions.\nHow do we get that information from the analysis results? We can communicate results — to ourselves or to an audience — by constructing plots from the model information. The ggeffects library extends ggplot2 to enable us to do this quite efficiently.\nWhen we write code to fit a linear model like:\n\nmodel &lt;- lm(Levenshtein.Score ~ Instructions*Orthography, data = conc.orth)\n\nWe record the results as an object called model because we specify model &lt;- lm(...). We can take these results and ask R to create a plot showing predicted change in outcome (spelling) given our model. We can then present the effects of the variables, as shown in Figure 4.17.\n\ndat &lt;- ggpredict(model, terms = c(\"Instructions\", \"Orthography\"))\nplot(dat, facet = TRUE) + ylim(0, 3)\n\n\n\n\n\n\n\nFigure 4.17: Dot and whisker plots showing the predicted effect on outcome spelling (Levenshtein) score, given different experimental conditions: Orthography (present, absent) x Instruction (explicit, incidental).\n\n\n\n\n\nThe code works as follows:\n\ndat &lt;- ggpredict(model, terms = c(\"Instructions\", \"Orthography\")) tells R to calculate predicted outcomes, given our model information, for the factors \"Instructions\", \"Orthography\".\nplot(dat, facet = TRUE) plot the effects, given the predictions, showing the effect of different instruction conditions in different plot facets (the left and right panels).\nylim(0, 3) fix the y-axis to show a more honest indication of the effect on outcomes, given the potential range of spelling scores can start at 0.\n\nIn Figure 4.17, the dots represent the linear model estimates of outcome spelling, predicted under different conditions. The plots indicate that spelling scores are predicted to be lower when orthography is present. There appears to be little or no effect associated with different kinds of instruction.\nThe vertical lines (often termed “whiskers”) indicate the 95% confidence interval about these estimates. Confidence intervals (CIs) are often mis-interpreted so I will give the quick definition outlined by Hoekstra et al. (2014) here:\n\nA CI is a numerical interval constructed around the estimate of a parameter [i.e. the model estimate of the effect]. Such an interval does not, however, directly indicate a property of the parameter; instead, it indicates a property of the procedure, as is typical for a frequentist technique. Specifically, we may find that a particular procedure, when used repeatedly across a series of hypothetical data sets (i.e., the sample space), yields intervals that contain the true parameter value in 95 % of the cases.\n\nIn short, the interval shows us the range of values within which we can expect to capture the effects of interest, in the long run, if we were to run our experiment over and over again.\nGiven our data and our model, these intervals indicate where the outcome might be expected to vary, given different conditions, and that is quite useful information. If you look at Figure 4.17, you can see that the presence of orthography (present versus absent) appears to shift outcome spelling, on average, by about a quarter of a letter edit: from over 1.5 to about 1.25. This is about one quarter of the difference, on average, between getting a target spelling correct and getting it wrong by one letter (e.g., the response ‘epegram’ for the target ‘epigram’). This is a relatively small effect but we may consider how such small effects add up, over a child’s development, cumulatively, in making the difference between wrong or nearly right spellings to correct spellings.\nIn the Ricketts et al. (2021) paper, we conducted Bayesian analyses which allow us to plot the estimated effects of experimental conditions along with what are called credible intervals indicating our uncertainty about the estimates. In a Bayesian analysis, we can indicate the probable or plausible effect of conditions, or range of plausible effects, given our data and our model. (This intuitive sense of the probable location of effects is, sometimes, what researchers and students mis-interpret confidence intervals as showing; Hoekstra et al. (2014).) Accounting for our uncertainty is a productive approach to considering how much we learn from the evidence we collect in experiments.\nBut this gets ahead of where we are now in our development of skills and understanding. There is another way to discover how uncertain we may be about the results of our analysis. This is an approach we have already experienced: plotting trends or estimates together with the observed data points. We present an example in Figure 4.18.\n\nplot(dat, add.data = TRUE)\n\n\n\n\n\n\n\nFigure 4.18: Dot and whisker plots showing the predicted effect on outcome spelling (Levenshtein) score, given different experimental conditions: Orthography (present, absent) x Instruction (explicit, incidental). The estimates are shown as dot-whisker points. In addition, the plot shows as points the spelling score observed for each child for each response recorded in the conc.orth dataset.\n\n\n\n\n\nFigure 4.18 reveals the usefulness of plotting model estimates of effects alongside the raw observed outcomes. We can make two critical observations.\n\nWe can see that the observed scores clearly cluster around outcome spelling values of 0, 1, 2, 3, 4, and 5.\n\n\nThis is not a surprise because Ricketts et al. (2021) scored each response in their test of spelling knowledge by counting the number of letter edits (letter deletions, additions etc.) separating a spelling response from a target response.\nBut the plot does suggest that the linear model is missing something about the outcome data because there is no recognition in the model or the results of this bunching or clustering around whole number values of the outcome variable. (This is why Ricketts et al. (2021) use a different analysis approach.)\n\n\nWe can also see that it is actually quite difficult to distinguish the effects of the experimental condition differences on the observed spelling responses. There is a lot of variation in the responses.\n\nHow can we make sense of this variation?\nAnother approach we can take to experimental data is to examine visually how the effects of experimental conditions vary between individual participants. Usually, in teaching, learning and doing foundation or introductory statistical analyses we think about the average impact on outcomes of the experimental conditions or some set of predictor variables. It often makes sense, also, or instead, to consider the ways that the impact on outcomes vary between individuals.\nHere, it might be worthwhile to look at the effect of the conditions for each child. We can do that in different ways. In the following, we will look at a couple of approaches that are often useful. We will focus on the effect of variation in the Orthography condition (present, absent)\nTo begin our work, we first calculate the average outcome (Levenshtein.Score) spelling score for each child in each of the experimental conditions (Orthography, present versus absent):\nWe do this in a series of steps.\n\nscore.by.subj &lt;- conc.orth %&gt;%\n  group_by(Participant, Orthography) %&gt;%\n  summarise(mean.score = mean(Levenshtein.Score))\n\n\nscore.by.subj &lt;- conc.orth %&gt;% create a new dataset score.by.subj by taking the original data conc.orth and piping it through a series of processing steps, to follow.\ngroup_by(Participant, Orthography) %&gt;% first group the rows of the original dataset and piped the grouped data to the next bit. We group the data by participant identity code and by Orthography condition\nsummarise(mean.score = mean(Levenshtein.Score)) then calculate the mean Levenshtein.Score for each participant, for their responses in the Orthography present and in the Orthography absent conditions.\n\nThis first step produces a summary version of the original dataset, with two mean outcome spelling scores for each child, for their responses in the Orthography present and in the Orthography absent conditions. This arranges the summary mean scores in rows, with two rows per child: one for the absent, one for the present condition. You can see what we get in the extract from the dataset, shown next.\n\n\n\n\n\nParticipant\nOrthography\nmean.score\n\n\n\n\nEOF001\nabsent\n1.750\n\n\nEOF001\npresent\n0.875\n\n\nEOF002\nabsent\n1.375\n\n\nEOF002\npresent\n2.125\n\n\nEOF004\nabsent\n1.625\n\n\nEOF004\npresent\n1.000\n\n\nEOF006\nabsent\n0.750\n\n\nEOF006\npresent\n0.500\n\n\nEOF007\nabsent\n1.500\n\n\nEOF007\npresent\n0.625\n\n\n\n\n\n\n\nIn the second step, we also calculate the difference between spelling scores in the different Orthography conditions. We do this because Ricketts et al. (2021) were interested in whether spelling responses were different in the different conditions.\n\nscore.by.subj.diff &lt;- score.by.subj %&gt;%\n  pivot_wider(names_from = Orthography, values_from = mean.score) %&gt;%\n  mutate(difference.score = absent - present) %&gt;%\n  pivot_longer(cols = c(absent, present), \n               names_to = 'Orthography',\n               values_to = 'mean.score') \n\n\nscore.by.subj.diff &lt;- score.by.subj %&gt;% creates a new version of the summary dataset from the dataset we just produced.\npivot_wider(names_from = Orthography, values_from = mean.score) %&gt;% re-arranges the dataset so that the absent, present mean scores are side-by-side, in different columns, for each child.\nmutate(difference.score = absent - present) %&gt;% calculates the difference between the absent, present mean scores, creating a new variable, difference.score.\npivot_longer(cols = c(absent, present) ...) re-arranges the data back again so that the dataset is in tidy format, with one column of mean spelling scores, with two rows for each participant for the absent, present mean scores.\n\nThis code arranges the summary mean scores in rows, with two rows per child: one for the absent, one for the present condition — plus a difference score.\n\n\n\n\n\nParticipant\ndifference.score\nOrthography\nmean.score\n\n\n\n\nEOF001\n0.875\nabsent\n1.750\n\n\nEOF001\n0.875\npresent\n0.875\n\n\nEOF002\n-0.750\nabsent\n1.375\n\n\nEOF002\n-0.750\npresent\n2.125\n\n\nEOF004\n0.625\nabsent\n1.625\n\n\nEOF004\n0.625\npresent\n1.000\n\n\nEOF006\n0.250\nabsent\n0.750\n\n\nEOF006\n0.250\npresent\n0.500\n\n\nEOF007\n0.875\nabsent\n1.500\n\n\nEOF007\n0.875\npresent\n0.625\n\n\n\n\n\n\n\nNow we can use these data to consider how the impact of the experimental condition (Orthography: present versus absent) varies between individual participants. We do this by showing the mean outcome spelling score, separately for each participant, in each condition.\nFigure 4.19 shows dot plots indicating the different outcome spelling (Levenshtein) scores, for each participant, in the different experimental conditions: Orthography (present, absent). Plots are ordered, from top left to bottom right, by the difference between mean spelling scores in the absent versus present conditions. The plots indicate that some children show higher spelling scores in the present than in the absent condition (top left plots), some children show little difference between conditions (middle rows), while some children show higher spelling scores in the absent than in the present condition (bottom rows).\n\nggplot(data = score.by.subj.diff, \n       aes(x = Orthography, y = mean.score,\n           colour = Orthography)) +\n  geom_point() +\n  facet_wrap(~ reorder(Participant, difference.score)) +\n  theme(axis.text.x = element_blank())\n\n\n\n\n\n\n\nFigure 4.19: Dot plots showing the different outcome spelling (Levenshtein) scores, for each participant, in the different experimental conditions: Orthography (present, absent). Plots are ordered, from top left to bottom right, by the difference between mean spelling scores in the absent versus present conditions.\n\n\n\n\n\nOnce we have done the data processing in preparation, the code to produce the plot is fairly compact.\n\nggplot(data = score.by.subj.diff ... tells R to produce a plot, using ggplot() and the newly created score.by.subj.diff dataset.\naes(x = Orthography, y = mean.score,... specifies the aesthetic mappings: we tell R to locate mean.score on the y-axis and Orthography condition on the x-axis/\naes(...colour = Orthography)) + specifies a further aesthetic mapping: we tell R to map different Orthography conditions to different colours.\ngeom_point() + tells R to take the data and produce a scatterplot, given our mapping specifications.\nfacet_wrap(...) + tells to split the dataset into sub-sets (facets).\nfacet_wrap(~ reorder(Participant, difference.score)) tells R that we want the sub-sets to be organized by Participant, and we want the facets to be ordered by the difference.score calculated for each participant.\ntheme(axis.text.x = element_blank()) removes the x-axis labels because it is too crowded with the axis labels left in, and the information is already present in the colour guide legend shown on the right of the plot.\n\n\n\n4.7.11 Summary: Visualizing associations\nVisualizing associations between variables encompasses a wide range of the things we have to do, in terms of both discovery and communication, when we work with data from psychological experiments.\nThe conventional method to visualize how the distribution of values in one variable covaries with the distribution of values in another variable is through using a scatterplot. However, the construction of a scatterplot can be elaborated in various ways to enrich the information we present or communicate to our audiences, or to ourselves.\n\nWe can add elements like smoothers to indicate trends.\nWe can add annotation, as with the histograms, to highlight specific thresholds.\nWe can facet the plots to indicate how trends may vary between sub-sets of the data.\n\nIn the final phases of our practical work, we started by presenting model-based predictions of the effects of experimental manipulations. However, you will have noticed that presenting plots of effects is not where we stop when we engage with a dataset. Further plotting indicates quite marked variation between participants in the effects of the conditions. This kind of insight is something we can and should seek to reveal through our visualization work.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#sec-vis-next-steps",
    "href": "visualization.html#sec-vis-next-steps",
    "title": "4  Data visualization",
    "section": "4.8 Next steps for development",
    "text": "4.8 Next steps for development\nTo take your development further, take a look at the resources listed in Section 4.9.\nIn my experience, the most productive way to learn about visualization and about coding the production of plots, is by doing. And this work is most interesting if you have a dataset you care about: for your research report, or for your dissertation study.\nAs you have the alternate datasets described in Section 4.7.1.2.1, you can start with the data from the other task or the other study in Ricketts et al. (2021). Ricketts et al. (2021) recorded children’s responses in two different outcome tasks, the orthographic spelling task we have looked at, and a semantic or meaning-based task. It would be a fairly short step to adapt the code you see in the example code chunks to work with the semantic datasets.\nAlternatively, you can look at the data reported by Rodríguez-Ferreiro et al. (2020). Rodríguez-Ferreiro et al. (2020) present both measures of individual differences (on schizotypyal traits) and experimental manipulations (of semantic priming) so you can do similar things with those data as we have explored here.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#sec-vis-resources",
    "href": "visualization.html#sec-vis-resources",
    "title": "4  Data visualization",
    "section": "4.9 Helpful resources",
    "text": "4.9 Helpful resources\n\n4.9.1 Some helpful websites\n\nWe typically use the ggplot library (part of the tidyverse) to produce plots. Clear technical information, with useful examples you can copy and run, can be found in the reference webpages:\n\nhttps://ggplot2.tidyverse.org/reference/index.html\n\nA source of inspiration can be found here:\n\nhttps://r-graph-gallery.com\nIf you are trying to work out how to do things by searching for information online, you often find yourself at tutorial webpages. You will develop a sense of quality and usefulness with experience. Most often, what you are looking for is a tutorial that provides some explanation, and example code you can adapt for your own purposes. Here are some examples.\n\nCedric Scherer on producing raincloud plots:\n\nhttps://www.cedricscherer.com/2021/06/06/visualizing-distributions-with-raincloud-plots-and-how-to-create-them-with-ggplot2/\n\nWinston Chang on colours and colour blind palettes:\n\nhttp://www.cookbook-r.com/Graphs/Colors_(ggplot2)/\n\nThomas Lin Pedersen (and others) on putting together plots into a single presentation using the patchwork library functions:\n\nhttps://patchwork.data-imaginist.com/articles/patchwork.html\n\n\n4.9.2 Some helpful books\n\nThe book “R for Data Science” (Wickham & Grolemund, 2016) will guide you through the data analysis workflow, including data visualization, and the latest version can be accessed in an online free version here:\n\nhttps://r4ds.hadley.nz\n\nThe “ggplot2: Elegant Graphics for Data Analysis” book (Wickham, 2016) corresponding to the ggplot library was written by Hadley Wickham in its first edition, it is now in its third edition (as a work in progress, co-authored by Wickham, Danielle Navarro and Thomas Lin Pedersen) and this latest version can be accessed in an online free version here:\n\nhttps://ggplot2-book.org/index.html\n\nThe “R graphics cookbook” (Chang, 2013), and the latest version can be accessed in an online free version here:\n\nhttps://r-graphics.org\n\nThe book “Fundamentals of Data Visualization” (Wilke, n.d.) is about different aspects of visualization, and can be accessed in an online free version here:\n\nhttps://clauswilke.com/dataviz/\n\n\n\n\nBates, D., Mächler, M., Bolker, B., & Walker, S. (2015). Fitting linear mixed-effects models using lme4. Journal of Statistical Software, 67(1), 1–48. https://doi.org/10.18637/jss.v067.i01\n\n\nBelenky, G., Wesensten, N. J., Thorne, D. R., Thomas, M. L., Sing, H. C., Redmond, D. P., Russo, M. B., & Balkin, T. J. (2003). Patterns of performance degradation and restoration during sleep restriction and subsequent recovery: a sleep dose-response study. Journal of Sleep Research, 12(1), 1–12. https://doi.org/10.1046/j.1365-2869.2003.00337.x\n\n\nChang, W. (2013). R graphics cookbook. o’Reilly Media.\n\n\nFranconeri, S. L., Padilla, L. M., Shah, P., Zacks, J. M., & Hullman, J. (2021). The Science of Visual Data Communication: What Works. Psychological Science in the Public Interest, 22(3), 110–161. https://doi.org/10.1177/15291006211051956\n\n\nGelman, A., & Unwin, A. (2013). Infovis and Statistical Graphics: Different Goals, Different Looks. Journal of Computational and Graphical Statistics, 22(1), 2–28. https://doi.org/10.1080/10618600.2012.761137\n\n\nHoekstra, R., Morey, R. D., Rouder, J. N., & Wagenmakers, E.-J. (2014). Robust misinterpretation of confidence intervals. Psychonomic Bulletin & Review, 21(5), 1157–1164. https://doi.org/10.3758/s13423-013-0572-3\n\n\nPinheiro, J. C., & Bates, D. M. (2000). Mixed-effects models in s and s-plus (statistics and computing). Springer.\n\n\nRicketts, J., Dawson, N., & Davies, R. (2021). The hidden depths of new word knowledge: Using graded measures of orthographic and semantic learning to measure vocabulary acquisition. Learning and Instruction, 74, 101468. https://doi.org/10.1016/j.learninstruc.2021.101468\n\n\nRodríguez-Ferreiro, J., Aguilera, M., & Davies, R. (2020). Semantic priming and schizotypal personality: reassessing the link between thought disorder and enhanced spreading of semantic activation. PeerJ, 8, e9511. https://doi.org/10.7717/peerj.9511\n\n\nVasishth, S., & Gelman, A. (2021). How to embrace variation and accept uncertainty in linguistic and psycholinguistic data analysis. Linguistics, 59(5), 1311–1342. https://doi.org/10.1515/ling-2019-0051\n\n\nWickham, H. (2016). ggplot2: Elegant graphics for data analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org\n\n\nWickham, H. (2017). Tidyverse: Easily install and load the ’tidyverse’. https://cran.r-project.org/package=tidyverse\n\n\nWickham, H., & Grolemund, G. (2016). R for data science: Import, tidy, transform, visualize, and model data. \" O’Reilly Media, Inc.\".\n\n\nWilke, C. O. (n.d.). Fundamentals of data visualization. https://clauswilke.com/dataviz/\n\n\nWilkinson, L. (2013). The Grammar of Graphics. Springer Science & Business Media.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#footnotes",
    "href": "visualization.html#footnotes",
    "title": "4  Data visualization",
    "section": "",
    "text": "As you can see if you read the Ricketts et al. (2021) paper, and the associated guide to the data and analysis on the OSF repository, we analysed the word learning data using Generalized Linear Mixed-effects Models (GLMM). GLMMs are used when we are analyzing data with a multilevel structure. These structures are very common and can be identified whenever we have groups or clusters observations: here, we have multiple observations of the test response, for each participant and for each stimulus word. When we fit GLMMs, the functions we use to do the analysis require the data to be structured in this tidy fashion, with different rows for each response or outcome observation, and repeated information for each participant or stimulus (if present).↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data visualization</span>"
    ]
  },
  {
    "objectID": "lm-intro.html",
    "href": "lm-intro.html",
    "title": "5  Introduction to the linear model",
    "section": "",
    "text": "5.1 Overview\nWelcome to our overview of the materials you will work with in our class introducing you to the linear model in PSYC401 Week 9.\nWe will return to locate our learning in the context of the Clearly understood project. We will present our PSYC401 lessons in the context of this research project because we think that this context will help you to make sense of the data (as we explain in Chapter 1).\nRemember: it is unclear how to make health communication more effective. The problem is that we are not sure how health information should be communicated so that everyone can understand it. This is why we ask the research questions:\nAs we work together, we will be revisiting some of the ideas and techniques you have seen in previous classes. This is to give you the opportunity to consolidate your learning. We will be extending your development with some new ideas to strengthen your skills.\nYou can read a bit more about the project and the project data in Chapter 2.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to the linear model</span>"
    ]
  },
  {
    "objectID": "lm-intro.html#sec-lm-intro-overview",
    "href": "lm-intro.html#sec-lm-intro-overview",
    "title": "5  Introduction to the linear model",
    "section": "",
    "text": "What person attributes predict success in understanding?\nCan people accurately evaluate whether they correctly understand written health information?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to the linear model</span>"
    ]
  },
  {
    "objectID": "lm-intro.html#sec-lm-intro-goals",
    "href": "lm-intro.html#sec-lm-intro-goals",
    "title": "5  Introduction to the linear model",
    "section": "5.2 Our learning goals",
    "text": "5.2 Our learning goals\nThis week, we focus on learning how to predict people: predicting observations about us (e.g., our attributes) or about the things we make or do. To do this, we will learn to think about and work with linear models.\nOur learning objectives: — what are we learning about?\n\n5.2.1 Using the linear model to answer research questions\n\nWe will learn how to:\n\n\ncode linear models\nidentify and interpret model statistics\ncritically evaluate the results\ncommunicate the results\n\n\nBecause we often want to know about relationships, asking questions like:\n\n\nDoes variation in X predict variation in Y?\nWhat are the factors that influence outcome Y?\nIs a theoretical model consistent with observed behaviour?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to the linear model</span>"
    ]
  },
  {
    "objectID": "lm-intro.html#sec-lm-intro-resources",
    "href": "lm-intro.html#sec-lm-intro-resources",
    "title": "5  Introduction to the linear model",
    "section": "5.3 Learning resources",
    "text": "5.3 Learning resources\nYou will see in the next section links to the lectures we created both to explain the concepts we want to help you to learn about, and to explain the practical data analysis skills we want to help you to develop (Section 5.3.1). We then share links to information about the practical materials we have provided to help you to practise those skills (Section 5.3.2).\nAll the links to the lecture videos, the lecture slides, and everything you need for your practical work can also be found in the Week 9 files folder on Moodle here\nIn Section 5.4, we present the lecture slide points.\n\n\n\n\n\n\nTip\n\n\n\nLinked resources include:\n\nIn Chapter 2, we present an overview of the materials we have shared to support your development of critical thinking in relation to associations, and to support your learning about conducting correlation-based analyses of associations.\nIt may help you to revise these materials.\n\n\n\n\n5.3.1 Lectures\nThe lecture material for this week is presented in four short parts. Click on a link and your browser should open a tab showing the Panopto video for the lecture part.\nPart 1 of 4\nPart 2 of 4\nPart 3 of 4\nPart 4 of 4\nYou can download the slides we presented in the lecture in two different formats, depending on what you think will be most useful to you:\n\nDownload the slides exactly as they appear in the lecture from this link. The .html file can be opened and viewed in any web browser (e.g., Chrome, Firefox, Safari).\nOr you can download a printable Word .docx presentation of the slides from this link. The .docx can be opened in Microsoft Word. The figures will not appear exactly as they do in the lecture recording because Word cannot cope with images so well but the trade-off is that you get a document you can print and edit to add notes.\n\n\n\n5.3.2 Practical materials\nWe have collected the practical materials together into a folder.\nThe folder includes the data files:\n\nstudy-one-general-participants.csv\nstudy-two-general-participants.csv\n\nand .R code files:\n\n401-lm-intro-how-to.R\n401-lm-intro-workbook.R\n\nYou will use these files for your practical learning.\nYou can download the .R files and the data .csv files in a single folder, using the link here.\nOr you can download the files as individual files from the module Moodle page for PSYC401.\nOnce you have downloaded the file folder, you will need to upload it to the R-Studio server to access and use the R files.\n\n5.3.2.1 The how-to guide\nIn the how-to guide:\n\n401-lm-intro-how-to.R\n\nwe show you how to do everything you need to do in the practical workbook (see Section 5.3.3). The guide comprises an .R file 401-lm-intro-how-to.R with code and advice.\nThe code in the .R file was written to work with the data file\n\nstudy-one-general-participants.csv.\n\n\n\n\n\n\n\nTip\n\n\n\n\nWork through the steps in the how-to guide first, this practice will help you to understand what you need to do for the workbook tasks.\nThe how-to guide and the workbook have similar structures. This is intentional: so that you can copy and adapt code from the how-to guide to do the practical tasks in the workbook.\n\n\n\n\n\n\n5.3.3 The workbook\nIn the workbook:\n\n401-lm-intro-workbook.R\n\nyou will work with the data file\n\nstudy-two-general-participants.csv\n\nWe split .R scripts into parts, tasks and questions.\nFor this class on linear models, our practical materials have two aims:\n\nHelping you to learn how to use linear models to address questions about associations, questions like “What person attributes predict success in understanding?”;\nHelping you to learn how to interpret linear model results, including how to work with data visualizations displaying distributions or associations.\n\nWe progress a series of parts, each designed to enable learning or developing concepts or skills:\n\nPart 3 refreshes your development of skills for working with histogram-based visualizations of data distributions. Here, we are aiming to advance your skills so that you can develop nicer looking histograms that present more accurate accounts of distributions (by showing the full range of potential values), and so that you can annotate histograms to direct the attention of your audience as we discuss in 3.\nPart 4 helps you to learn how you can present grids of histograms for easy comparison of variable distributions.\nPart 5 refreshes your development of skills for working with scatterplot-based visualizations of associations or of the relationships between two or more numeric variables. We look at how you can edit the appearance of the plots, element by element. And we look at how you can produce grids of scatterplots, again, to enable comparisons — here, of the potential relationships between one outcome and multiple other variables.\nPart 6 offers the opportunity to revise the calculation and interpretation of correlation analyses.\nPart 7 provides exercises designed to help you to learn how to conduct linear model analyses.\nPart 8 helps you to develop skills in calculating and presenting model predictions.\nPart 9 is optional and focuses on working with R-community information to find out how to annotate plots using geom_vline() or geom_hline().\n\nThe activity 401-lm-intro-workbook.R file takes you through the tasks, one by one.\nIf you are unsure about what you need to do, check the advice in 401-lm-intro-how-to.R.\nYou will see that you can match a task in the activity to the same task in the how-to. The how-to shows you what function you need and how you should write the function code. You will need to change the names of the data-set or the variables to complete the tasks in the activity.\n\n\n5.3.4 The data files\nEach of the data files we will work with has a similar structure.\nHere are what the first few rows in the data file study-two-general-participants.csv looks like:\n\n\n\n\n\nparticipant_ID\nmean.acc\nmean.self\nstudy\nAGE\nSHIPLEY\nHLVA\nFACTOR3\nQRITOTAL\nGENDER\nEDUCATION\nETHNICITY\n\n\n\n\nstudytwo.1\n0.4107143\n6.071429\nstudytwo\n26\n27\n6\n50\n9\nFemale\nHigher\nAsian\n\n\nstudytwo.10\n0.6071429\n8.500000\nstudytwo\n38\n24\n9\n58\n15\nFemale\nSecondary\nWhite\n\n\nstudytwo.100\n0.8750000\n8.928571\nstudytwo\n66\n40\n13\n60\n20\nFemale\nHigher\nWhite\n\n\nstudytwo.101\n0.9642857\n8.500000\nstudytwo\n21\n31\n11\n59\n14\nFemale\nHigher\nWhite\n\n\nstudytwo.102\n0.7142857\n7.071429\nstudytwo\n74\n35\n7\n52\n18\nMale\nHigher\nWhite\n\n\nstudytwo.103\n0.7678571\n5.071429\nstudytwo\n18\n40\n11\n54\n15\nFemale\nFurther\nWhite\n\n\n\n\n\n\n\nYou can see the columns:\n\nparticipant_ID participant code\nmean.acc average accuracy of response to questions testing understanding of health guidance\nmean.self average self-rated accuracy of understanding of health guidance\nstudy variable coding for what study the data were collected in\nAGE age in years\nHLVA health literacy test score\nSHIPLEY vocabulary knowledge test score\nFACTOR3 reading strategy survey score\nGENDER gender code\nEDUCATION education level code\nETHNICITY ethnicity (Office National Statistics categories) code\n\n\n\n5.3.5 The answers\nAfter the practical class, you will be able to download the answers version of the workbook here.\n\nThe answers version will present my answers for questions, and some extra information where that is helpful.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to the linear model</span>"
    ]
  },
  {
    "objectID": "lm-intro.html#sec-lm-intro-notes",
    "href": "lm-intro.html#sec-lm-intro-notes",
    "title": "5  Introduction to the linear model",
    "section": "5.4 Lecture notes",
    "text": "5.4 Lecture notes\nSome people find it easier to read notes than to watch video recordings. This is why we also include the lecture notes here.\n\n\n\n\n\n5.4.1 Week 9: Focus on the linear model\n\n\n\n\n\n\n\n\n\n\n\n5.4.2 Analyze + visualize + present\n\n\n\n\n\n\n\n\nQ\n\n\ncluster_R\n\n\n\n\nnd_1\n\nGet raw data\n\n\n\nnd_2\n\nTidy data\n\n\n\nnd_1-&gt;nd_2\n\n\n\n\n\nnd_3_l\n\nVisualize\n\n\n\nnd_2-&gt;nd_3_l\n\n\n\n\n\nnd_3\n\nAnalyze\n\n\n\nnd_2-&gt;nd_3\n\n\n\n\n\nnd_3_r\n\nExplore\n\n\n\nnd_2-&gt;nd_3_r\n\n\n\n\n\nnd_3_a\n\nAssumptions\n\n\n\nnd_3_a-&gt;nd_3_l\n\n\n\n\n\nnd_3_a-&gt;nd_3\n\n\n\n\n\nnd_3_a-&gt;nd_3_r\n\n\n\n\n\nnd_3_l-&gt;nd_3\n\n\n\n\nnd_4\n\nPresent\n\n\n\nnd_3_l-&gt;nd_4\n\n\n\n\n\nnd_3-&gt;nd_4\n\n\n\n\n\n\n\n\nFigure 5.1: The data analysis pipeline or workflow: we focus on the linear model\n\n\n\n\n\n\n\n5.4.3 The linear model: our aims\n\nUnderstand how to code: lm(mean.acc ~ SHIPLEY)\nTo answer questions like: Is comprehension success influenced by vocabulary knowledge?\n\n\n\n\n\n\n\n\n\nFigure 5.2: Scatterplot showing the potential association between accuracy of comprehension and vocabulary scores\n\n\n\n\n\n\n\n5.4.4 Using the linear model to answer research questions\n\nWe will learn how to:\n\n\ncode linear models\nidentify and interpret model statistics\ncritically evaluate the results\ncommunicate the results\n\n\n\n5.4.5 Thinking about relationships in psychologial science\nWe often want to know about relationships\n\nDoes variation in X predict variation in Y?\nWhat are the factors that influence outcome Y?\nIs a theoretical model consistent with observed behaviour?\n\n\n\n5.4.6 Now consider our research aims in the context of the health comprehension project\n\nBecause public health impacts depend on giving people information they can understand\nWe want to know: What makes it easy or difficult to understand written health information?\n\n\n\n\n\n\nflickr: Sasin Tipchair ‘Senior woman in wheelchair talking to a nurse in a hospital’\n\n\n\n\n\n\n5.4.7 Health comprehension project: questions and analyses\n\nWe want to know: What makes it easy or difficult to understand written health information?\nSo our research questions are:\n\n\nWhat person attributes predict success in understanding?\nCan people accurately evaluate whether they correctly understand written health information?\n\n\nThese kinds of research questions can be answered using methods like linear models\n\n\n\n5.4.8 Context: Individual differences theory of comprehension success\n\nUnderstanding text depends on (1.) language experience and (2.) reasoning ability (Freed et al., 2017)\n\n\n\n\n\n\n\n\n\nQ\n\n\n\nnd_1_l\n\nLanguage experience\n\n\n\nnd_2\n\nComprehension outcome\n\n\n\nnd_1_l-&gt;nd_2\n\n\n\n\n\nnd_1_r\n\nReasoning capacity\n\n\n\nnd_1_r-&gt;nd_2\n\n\n\n\n\n\n\n\nFigure 5.3: The potential drivers of comprehension success\n\n\n\n\n\n\n\n\n5.4.9 The measurement context: Where the data come from\n\nWe measure reading comprehension: asking people to read text and then answer multiple choice questions\nWe measure background knowledge: vocabulary knowledge (Shipley); health literacy (HLVA)\n\n\n\n5.4.10 Reflect: The kinds of critical evaluation questions you can ask yourself\n\nAre multiple choice questions good ways to probe understanding? What alternatives are there?\nAre tests like the Shipley good measures of language knowledge? What do we miss?\n\n\n\n5.4.11 Reflect: As we move into thinking about the data analysis, we need to identify our assumptions\n\nvalidity: that differences in knowledge or ability cause differences in test scores\nmeasurement: that this is equally true across the different kinds of people we tested\ngeneralizability: that the sample of people we recruited looks like the population\n\n\n\n5.4.12 We need to think about the derivation chain\n\n\n\n\n\n\n\n\nQ\n\n\ncluster_R\n\n\n\n\nnd_1_l\n\nConcept formation\n\n\n\nnd_1_r\n\nCausal model\n\n\n\nnd_2_l\n\nMeasurement\n\n\n\nnd_1_l-&gt;nd_2_l\n\n\n\n\n\nnd_3\n\nStatistical predictions\n\n\n\nnd_2_l-&gt;nd_3\n\n\n\n\n\nnd_2_r\n\nAuxiliary assumptions\n\n\n\nnd_2_r-&gt;nd_3\n\n\n\n\n\nnd_4\n\nTesting hypotheses\n\n\n\nnd_3-&gt;nd_4\n\n\n\n\n\n\n\n\nFigure 5.4: The derivation chain\n\n\n\n\n\n\n\n5.4.13 Questions, assumptions, predictions\nLink: concepts, questions \\(\\rightarrow\\) assumptions \\(\\rightarrow\\) testable predictions\n\nconcepts, questions: Can people accurately understand health guidance? \\(\\rightarrow\\)\nassumptions: People who know more about language should also present more accurate understanding \\(\\rightarrow\\)\ntestable predictions: Higher levels of vocabulary should be associated with higher levels of comprehension accuracy: we expect to estimate a positive coefficient\n\n\n\n5.4.14 One way of thinking about the association is to visualize it\n\nFor each value of the predictor vocabulary\nDoes the the value of the outcome accuracy\nIncrease or decrease?\n\n\n\n\n\n\n\n\n\nFigure 5.5: The association between comprehension accuracy and vocabulary\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.4.15 Predicted association as expected change in average outcome\n\nFigure 5.6 shows the distribution curve of mean (comprehension) accuracy scores observed at each value of vocabulary\nYou can see that the middle – the average – of each distribution increases\nas we go from left (low scores) to right (high scores) on vocabulary\n\n\n\n\n\n\n\n\n\nFigure 5.6: Association between accuracy and vocabulary\n\n\n\n\n\n\n\n5.4.16 How do we estimate the association between two variables?\n\nmodel &lt;- lm(mean.acc ~ SHIPLEY,\n            data = clearly.one.subjects)\nsummary(model)\n\n\nSpecify the lm function and the model mean.acc ~ SHIPLEY\nSpecify what data we use data = clearly.one.subjects\nGet the results summary(model)\n\n\n\n5.4.17 The sentence structure of models in R\nTake a good look:\n\nlm(mean.acc ~ SHIPLEY ...)\n\nYou will see this sentence structure in coding for many different analysis types\n\nmethod(outcome ~ predictors)\nmethod could be aov, brm, lm, glm, glmm, lmer, t.test, cor.test\n\n\n\n5.4.18 Results: How does the outcome vary in relation to the predictor?\n\nmodel &lt;- lm(mean.acc ~ SHIPLEY,\n            data = clearly.one.subjects)\nsummary(model)\n\n\nCall:\nlm(formula = mean.acc ~ SHIPLEY, data = clearly.one.subjects)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.42871 -0.04921  0.02079  0.07480  0.18430 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.44914    0.08053   5.577 9.67e-08 ***\nSHIPLEY      0.01050    0.00229   4.585 8.85e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1115 on 167 degrees of freedom\nMultiple R-squared:  0.1118,    Adjusted R-squared:  0.1065 \nF-statistic: 21.03 on 1 and 167 DF,  p-value: 8.846e-06\n\n\n\nA model summary gives us estimates of:\nThe coefficient \\(= 0.44914\\) for the intercept\nThe coefficient \\(= 0.01050\\) for the slope of the SHIPLEY ‘effect’\n\n\n\n5.4.19 These coefficients build a line\n\nThe line represents:\nour prediction for how the outcome varies on average\ngiven change in the predictor\n\n\n\n5.4.20 So now we need to think about straight lines\n\n\nYou may remember from school that to draw a straight line you need four numbers:\n\n\\[y = a + bx\\]\n\nWe calculate the height \\(y\\) by adding\n\n\n\\(a\\) the intercept, the value of y when \\(x = 0\\)\nto the product of \\(b\\) the coefficient for the slope of the line\nmultiplied by \\(x\\) the value of the predictor variable\n\n\n\n5.4.21 Let’s draw it\n\n\nLook at what we get if we draw the line using the linear model coefficients:\n\\(= 0.449\\) for the intercept, \\(a\\)\n\\(= 0.011\\) for the slope, \\(b\\)\nIn the formula: \\(y = 0.449 + 0.011x\\)\n(I round the numbers to three decimal places.)\n\n\n\n\n\n\n\n\n\nFigure 5.7: The predicted association between comprehension accuracy and vocabulary\n\n\n\n\n\n\n\n5.4.22 We can understand the line as representing a set of predictions\n\n\nTo see how — we use the coefficients to predict just one potential outcome:\nthe expected accuracy for someone with a vocabulary score of 20\nWe do this using the formula:\n\n\\(\\text{predicted y} = 0.449 + \\text{0.011 } \\times \\text{ Shipley score of } 20\\)\n\n\n\n\n\n\n\n\nFigure 5.8: Predicted outcome in red",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to the linear model</span>"
    ]
  },
  {
    "objectID": "lm-intro.html#we-can-understand-the-line-as-representing-a-set-of-predictions-1",
    "href": "lm-intro.html#we-can-understand-the-line-as-representing-a-set-of-predictions-1",
    "title": "5  Introduction to the linear model",
    "section": "5.5 We can understand the line as representing a set of predictions",
    "text": "5.5 We can understand the line as representing a set of predictions\n\n\nLet’s expand our predictions\n\n\nPredict accuracy given a Shipley score of 20\n\n\n\\(y = 0.449 + 0.011 \\times 20\\)\n\n\nPredict accuracy given a Shipley score of 30\n\n\n\\(y = 0.449 + 0.011 \\times 30\\)\n\n\n\n\n\n\n\n\n\nFigure 5.9: Predicted outcome in red\n\n\n\n\n\n\n5.5.1 The linear model allows us to predict the average outcome we can expect given any value of the predictor\n\n\n\n\n\n\n\n\nFigure 5.10: The predicted change in mean comprehension accuracy, given variation in vocabulary scores\n\n\n\n\n\n\n\n5.5.2 We could draw a variety of different model prediction lines: how do we pick the right one?\n\n\n\n\n\n\n\n\n\n\n\n5.5.3 We could draw a variety of different model prediction lines: how do we pick the right one?\nWe need to go back to the prediction model\n\nTo calculate a predicted outcome value, we calculated it as: \\(\\text{predicted y} = 0.449 + 0.011 \\times \\text{ Shipley score } 20\\)\nAssuming the linear model \\(\\text{predicted y} = intercept + \\text{slope } \\times \\text{ vocabulary}\\)\nBut we missed a bit: error\n\n\n\n5.5.4 Linear models are typically estimated given sample data\n\nMaybe you noticed that I talked about how the model allows us to predict\nhow the outcome varies on average given different values of the predictor\nWhen we use a linear model to estimate the intercept and slope – to build the predictions – we fit a model to the sample data\nAnd no model will fit sample data perfectly\n\n\n\n\n5.5.5 Linear models are typically estimated given sample data\n\nUsually, this means there are differences between the expected outcomes that the model predicts and the observed outcomes\n\nSo we often write the linear model like this: \\(y = a + bx + \\epsilon\\)\n\n\nThe observed outcome \\(y\\) equals\nthe intercept \\(a\\)\nplus the difference associated with a specific predictor value \\(bx\\)\nplus some amount of mismatch or error \\(\\epsilon\\), the difference between the observed outcome and the predicted outcome\n\n\n\n5.5.6 We can derive the formulas used to calculate the estimates using calculus\n\nBut we won’t\nBecause the linear model calculations are done using matrix solution algorithms in R so we don’t have to \n\n\n\n5.5.7 What do the prediction errors – the residuals – look like?\n\n\n\n\n\n\n\n\nFigure 5.11: The predicted change in mean comprehension accuracy, given variation in vocabulary scores. Observed values are shown in orange-red. Predicted values are shown in blue\n\n\n\n\n\n\n\n5.5.8 What the prediction errors look like\n\nThe model expectation: higher vocabulary predicts higher mean comprehension accuracy\nThe predicted points are shown by the blue line\nThe prediction line increases in height for higher values of vocabulary\nLook at the differences in height between the observed points (in orange-red) and predicted points\n\n\n\n\n\n\n\n\n\nFigure 5.12: The predicted change in mean comprehension accuracy, given variation in vocabulary scores. Observed values are shown in orange-red. Predicted values are shown in blue",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to the linear model</span>"
    ]
  },
  {
    "objectID": "lm-intro.html#what-the-prediction-errors-look-like-1",
    "href": "lm-intro.html#what-the-prediction-errors-look-like-1",
    "title": "5  Introduction to the linear model",
    "section": "5.6 What the prediction errors look like",
    "text": "5.6 What the prediction errors look like\n\nIf the regression model were perfect then all the observed points would lie on the prediction line\nThey do not\n\n\n\n\n\n\n\n\n\nFigure 5.13: The predicted change in mean comprehension accuracy, given variation in vocabulary scores. Observed values are shown in orange-red. Predicted values are shown in blue\n\n\n\n\n\n\n5.6.1 What the prediction errors look like\n\nDifferences between observed and predicted outcomes are shown by the vertical lines\nBetter models should show smaller differences between observed and predicted outcome values\nNotice: some participants had same vocabulary scores but different outcomes\n\n\n\n\n\n\n\n\n\nFigure 5.14: The predicted change in mean comprehension accuracy, given variation in vocabulary scores. Observed values are shown in orange-red. Predicted values are shown in blue\n\n\n\n\n\n\n\n5.6.2 We typically assume that the residuals are normally distributed\n\nSome are positive: observed outcome larger than predicted outcome\nSome are negative: observed outcome smaller than predicted outcome\nThe average of the residuals will be zero overall\n\n\n\n\n\n\n\n\n\nFigure 5.15: Plot showing the distribution of prediction errors – residuals – for the linear model of comprehension accuracy\n\n\n\n\n\n\n\n5.6.3 So: We pick the line that minimizes the residuals – the mismatch between predicted and observed outcomes\n\n\n\n\n\n\n\n\n\n\n\n5.6.4 Identifying the key information in the linear model results\n\nThe summary() of the linear model shows …\nThe Estimate of the Coefficient of the effect of individual differences in vocabulary (SHIPLEY)\nhow much the outcome mean.acc value changes, given differences in SHIPLEY score\nAssociated t value and Pr(&gt; |t|) statistics for the coefficient t-test\nModel fit statistics: R-squared and F-statistic\n\n\nmodel &lt;- lm(mean.acc ~ SHIPLEY, data = clearly.one.subjects)\nsummary(model)\n\n\nCall:\nlm(formula = mean.acc ~ SHIPLEY, data = clearly.one.subjects)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.42871 -0.04921  0.02079  0.07480  0.18430 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.44914    0.08053   5.577 9.67e-08 ***\nSHIPLEY      0.01050    0.00229   4.585 8.85e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1115 on 167 degrees of freedom\nMultiple R-squared:  0.1118,    Adjusted R-squared:  0.1065 \nF-statistic: 21.03 on 1 and 167 DF,  p-value: 8.846e-06\n\n\n\n\n5.6.5 For the effect of vocabulary (SHIPLEY), we have:\n\nThe coefficient for the slope of the effect of variation in vocabulary scores: 0.01050\nThe Std. Error (standard error) 0.00229 for that estimate\nThe tvalue 4.585 and associated Pr(&gt;|t|) p-value 8.85e-06 for the null hypothesis test of the coefficient\n\n\nmodel &lt;- lm(mean.acc ~ SHIPLEY, data = clearly.one.subjects)\nsummary(model)\n\n\nCall:\nlm(formula = mean.acc ~ SHIPLEY, data = clearly.one.subjects)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.42871 -0.04921  0.02079  0.07480  0.18430 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.44914    0.08053   5.577 9.67e-08 ***\nSHIPLEY      0.01050    0.00229   4.585 8.85e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1115 on 167 degrees of freedom\nMultiple R-squared:  0.1118,    Adjusted R-squared:  0.1065 \nF-statistic: 21.03 on 1 and 167 DF,  p-value: 8.846e-06\n\n\n\n\n5.6.6 Identifying the key information in the results\n\nPay attention to the sign and the size of the coefficient estimate:\nIs the coefficient (e.g., SHIPLEY 0.01050) a positive or a negative number?\nIs it relatively large or small?\nWe come back to this, shortly, in the context of interpretation and reporting\n\n\nmodel &lt;- lm(mean.acc ~ SHIPLEY, data = clearly.one.subjects)\nsummary(model)\n\n\nCall:\nlm(formula = mean.acc ~ SHIPLEY, data = clearly.one.subjects)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.42871 -0.04921  0.02079  0.07480  0.18430 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.44914    0.08053   5.577 9.67e-08 ***\nSHIPLEY      0.01050    0.00229   4.585 8.85e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1115 on 167 degrees of freedom\nMultiple R-squared:  0.1118,    Adjusted R-squared:  0.1065 \nF-statistic: 21.03 on 1 and 167 DF,  p-value: 8.846e-06\n\n\n\n\n5.6.7 The t-tests in the linear model\n\n\n\n\\[t = \\frac{\\beta_j}{s_{\\beta_j}}\\]\n\n\nFor each coefficient, the t-test is used to evaluate if the coefficient \\(\\beta_j\\) is significantly different from zero\nWe assume the null hypothesis that the coefficient \\(\\beta_j\\) is zero\nWe do the test by comparing the estimated coefficient \\(\\beta_j\\) with the standard error of the estimate\n\n\n\n5.6.8 The t-tests in the linear model\n\n\n\n\\[t = \\frac{\\beta_j}{s_{\\beta_j}}\\]\n\nThe standard error \\(s_{\\beta_j}\\) indicates our uncertainty about the estimate\nLarger standard errors represent greater uncertainty\n\n\n\n5.6.9 The t-tests in the linear model\n\n\n\n\\[t = \\frac{\\beta_j}{s_{\\beta_j}}\\]\n\nStandard errors can be calculated using information about:\nError in the model — think of the distribution of residuals\nVariation of values in the predictor — how widely they range\nThe sample size\nStandard errors will be smaller for the coefficients of effects that appear to have bigger impacts, in models that describe outcomes better, in larger samples\n\n\n\n5.6.10 Identifying the key information in the results\n\nPay attention to R-squared:\nThe model summary gives us the Multiple R-squared and Adjusted R-squared\nThese numbers represent how much of the variation in the outcome can be predicted by the model\nWe usually report Adjusted R-squared because it tends to be more accurate\n\n\nmodel &lt;- lm(mean.acc ~ SHIPLEY, data = clearly.one.subjects)\nsummary(model)\n\n\nCall:\nlm(formula = mean.acc ~ SHIPLEY, data = clearly.one.subjects)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.42871 -0.04921  0.02079  0.07480  0.18430 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.44914    0.08053   5.577 9.67e-08 ***\nSHIPLEY      0.01050    0.00229   4.585 8.85e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1115 on 167 degrees of freedom\nMultiple R-squared:  0.1118,    Adjusted R-squared:  0.1065 \nF-statistic: 21.03 on 1 and 167 DF,  p-value: 8.846e-06\n\n\n\n\n5.6.11 R-squared – what is it? – an indicator of the proportion of outcome variation we can predict\n\nBetter models should show smaller differences between observed and predicted outcomes\nR-squared (\\(R^2\\)) gives the proportion of outcome variance\nwe can predict given information about differences in vocabulary\n\n\n\n\n\n\n\n\n\nFigure 5.16: The difference between predicted and observed outcomes, given variation in vocabulary\n\n\n\n\n\n\n\n5.6.12 R-squared as an indicator of the proportion of the outcome variation we can predict\n\nTo understand what this means, look at the scatterplot\nOn average, values in outcome (accuracy) increase with increasing values in the predictor (vocabulary)\nBut different people got different outcomes even with same vocabulary scores\n\n\n\n\n\n\n\n\n\nFigure 5.17: The difference between predicted and observed outcomes, given variation in vocabulary\n\n\n\n\n\n\n\n5.6.13 R-squared as an indicator of the proportion of the outcome variation we can predict\n\nSo: we have variation in the outcome that is related to variation in the predictor\nAnd: we have variation in the outcome that seems unrelated to the predictor\n\\(R^2\\) tells us how much variation in the outcome is explained by the model\n\\(R^2\\) gives us a proportion where \\(R^2 = \\frac{\\text{predicted outcome variation}}{\\text{total outcome variation}}\\)\n\n\n\n\n\n\n\n\n\nFigure 5.18: The difference between predicted and observed outcomes, given variation in vocabulary\n\n\n\n\n\n\n\n5.6.14 Identifying the key information in the results\n\nPay attention to F:\nThe model summary gives us the F-statistic:\nThis is the test statistic for the test of the null hypothesis that the model does not predict the outcome\n\n\nmodel &lt;- lm(mean.acc ~ SHIPLEY, data = clearly.one.subjects)\nsummary(model)\n\n\nCall:\nlm(formula = mean.acc ~ SHIPLEY, data = clearly.one.subjects)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.42871 -0.04921  0.02079  0.07480  0.18430 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.44914    0.08053   5.577 9.67e-08 ***\nSHIPLEY      0.01050    0.00229   4.585 8.85e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1115 on 167 degrees of freedom\nMultiple R-squared:  0.1118,    Adjusted R-squared:  0.1065 \nF-statistic: 21.03 on 1 and 167 DF,  p-value: 8.846e-06\n\n\n\n\n5.6.15 Reporting the results of a linear model\n\nYou will need to report three bits of information:\n\n\n\\(R^2\\) how much outcome variation is explained by the model\n\\(F\\) test for the null hypothesis that none of the predictors actually predict the outcome\nCoefficient estimates with the t-tests for the null hypothesis for each coefficient\n\n\nmodel &lt;- lm(mean.acc ~ SHIPLEY, data = clearly.one.subjects)\nsummary(model)\n\n\nCall:\nlm(formula = mean.acc ~ SHIPLEY, data = clearly.one.subjects)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.42871 -0.04921  0.02079  0.07480  0.18430 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.44914    0.08053   5.577 9.67e-08 ***\nSHIPLEY      0.01050    0.00229   4.585 8.85e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1115 on 167 degrees of freedom\nMultiple R-squared:  0.1118,    Adjusted R-squared:  0.1065 \nF-statistic: 21.03 on 1 and 167 DF,  p-value: 8.846e-06\n\n\n\n\n5.6.16 The language and style of reporting linear model results\nHere is an example of results reporting text that is conventional:\n\nWe fitted a linear model with mean comprehension accuracy as the outcome and vocabulary (Shipley) as the predictor. Our analysis indicated a significant effect of vocabulary knowledge. The model is significant overall, with \\(F(1, 167) = 21.03, p &lt; .001\\), and explains 11% of variance (\\(\\text{adjusted } R^2 = 0.11\\)). The model estimates showed that the accuracy of comprehension increased with increasing levels of participant vocabulary knowledge (\\(\\beta = .011, t = 4.59, p &lt;.001\\)).\n\n\n\n5.6.17 Look at what we do with the text\n\nExplain what I did, specifying the method (linear model), the outcome variable (accuracy) and the predictor variables (health literacy, reading strategy, reading skill and vocabulary)\nReport the model fit statistics overall (\\(F, R^2\\))\nReport the significant effects (\\(\\beta, t, p\\)) and describe the nature of the effects (does the outcome increase or decrease?)\n\n\nWe fitted a linear model with mean comprehension accuracy as the outcome and vocabulary (Shipley) as the predictor. Our analysis indicated a significant effect of vocabulary knowledge. The model is significant overall, with \\(F(1, 167) = 21.03, p &lt; .001\\), and explains 11% of variance (\\(\\text{adjusted } R^2 = 0.11\\)). The model estimates showed that the accuracy of comprehension increased with increasing levels of participant vocabulary knowledge (\\(\\beta = .011, t = 4.59, p &lt;.001\\)).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to the linear model</span>"
    ]
  },
  {
    "objectID": "lm-intro.html#summary",
    "href": "lm-intro.html#summary",
    "title": "5  Introduction to the linear model",
    "section": "5.7 Summary",
    "text": "5.7 Summary\n\nIn psychological science, we often ask questions like:\n\n\nDoes variation in X predict variation in Y?\nWhat are the factors that influence outcome Y?\nIs a theoretical model consistent with observed behaviour?\n\n\nWe can answer these questions using the linear model\nGiven sample data, we can predict the average difference in outcome values, for different levels of a predictor variable\nWe (or the math engine R uses) calculate the predictions so that they minimize the residuals, the errors of prediction or the mismatch between predicted and observed outcomes\nOur results report tells the reader about the model and the estimated effects\n\n\n\n\n\nFreed, E. M., Hamilton, S. T., & Long, D. L. (2017). Comprehension in proficient readers: The nature of individual variation. Journal of Memory and Language, 97, 135–153. https://doi.org/10.1016/j.jml.2017.07.008",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to the linear model</span>"
    ]
  },
  {
    "objectID": "lm-dev.html",
    "href": "lm-dev.html",
    "title": "6  Developing the linear model",
    "section": "",
    "text": "6.1 Overview\nWelcome to our overview of the materials for our class on developing the linear model in PSYC401 Week 10.\nWe will continue to locate our learning in the context of the Clearly understood project. We present our PSYC401 lessons in the context of this research project because we think that this context will help you to make sense of the data (as we explain in Chapter 1).\nIn this chapter, we focus on extending your understanding and skills so that you can apply the linear model analysis approach to a wider range of research questions.\nIn the context of the Clearly understood project, we frame our analysis concerns and methods in relation to two example research questions:\nIt will be seen that to answer these research questions, we need to think about how we analyze data when multiple different predictor variables could be included in our model of outcome data, and when these variables may consist of different kinds of observations.\nYou can read a bit more about the project and the project data in Chapter 2.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Developing the linear model</span>"
    ]
  },
  {
    "objectID": "lm-dev.html#sec-lm-dev-overview",
    "href": "lm-dev.html#sec-lm-dev-overview",
    "title": "6  Developing the linear model",
    "section": "",
    "text": "What person attributes predict success in understanding?\nCan people accurately evaluate whether they correctly understand written health information?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Developing the linear model</span>"
    ]
  },
  {
    "objectID": "lm-dev.html#sec-lm-dev-goals",
    "href": "lm-dev.html#sec-lm-dev-goals",
    "title": "6  Developing the linear model",
    "section": "6.2 Our learning goals",
    "text": "6.2 Our learning goals\nThis week, we build on what we have learnt so far about how we can analyze data to predict data about people (e.g., our attributes) or about the things we make or do. To do this, we will learn to think about and work with linear models.\nOur learning objectives: — what are we learning about?\n\n6.2.1 Develop the linear model: our aims\n\nWe will learn how to:\n\n\nExtend our capacity to code models so that we can incorporate multiple predictors\nDevelop the thought processes required to make decisions about what predictors to include\nDevelop the skills required to critically evaluate results\n\nEspecially considering potential variation across samples\n\nWe will revise how to:\n\n\nIdentify and interpret model statistics\nCritically evaluate the results\nCommunicate the results\n\n\nWe will learn how to: explore extensions or generalisations of the linear model",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Developing the linear model</span>"
    ]
  },
  {
    "objectID": "lm-dev.html#sec-lm-dev-resources",
    "href": "lm-dev.html#sec-lm-dev-resources",
    "title": "6  Developing the linear model",
    "section": "6.3 Learning resources",
    "text": "6.3 Learning resources\nYou will see in the next section links to the lectures we created both to explain the concepts we want to help you to learn about, and to explain the practical data analysis skills we want to help you to develop (Section 6.3.1). We then share links to information about the practical materials we have provided to help you to practise those skills (Section 6.3.2).\nAll the links to the lecture videos, the lecture slides, and everything you need for your practical work can also be found in the Week 10 files folder on Moodle here\nIn Section 6.4, we present the lecture slide points.\n\n\n\n\n\n\nTip\n\n\n\nLinked resources include:\n\nIn Chapter 2, we present an overview of our materials in relation to thinking about associations, and working with correlation-based analyses of associations.\nIn Chapter 5, we introduce you to the main ideas and practical steps involved in conducting linear model analyses.\n\n\n\n\n6.3.1 Lectures\nThe lecture material for this week is presented in four short parts. Click on a link and your browser should open a tab showing the Panopto video for the lecture part.\nPart 1 of 4\nPart 2 of 4\nPart 3 of 4\nPart 4 of 4\nYou can download the slides we presented in the lecture in two different formats, depending on what you think will be most useful to you:\n\nDownload the slides exactly as they appear in the lecture from this link. The .html file can be opened and viewed in any web browser (e.g., Chrome, Firefox, Safari).\nOr you can download a printable Word .docx presentation of the slides from this link. The .docx can be opened in Microsoft Word. The figures will not appear exactly as they do in the lecture recording because Word cannot cope with images so well but the trade-off is that you get a document you can print and edit to add notes.\n\n\n\n6.3.2 Practical materials\nWe have collected the practical materials together into a folder.\nThe folder includes the data file:\n\n2022-12-08_all-studies-subject-scores.csv\n\nand .R code files:\n\n401-lm-dev-how-to.R\n401-lm-dev-workbook.R\n\nYou will use these files for your practical learning.\nYou can download the .R files and the data .csv files in a single folder, using the link here.\nOr you can download the files as individual files from the module Moodle page for PSYC401.\nOnce you have downloaded the file folder, you will need to upload it to the R-Studio server to access and use the R files.\n\n6.3.2.1 The how-to guide\nIn the how-to guide:\n\n401-lm-dev-how-to.R\n\nwe show you how to do everything you need to do in the practical workbook (see Section 6.3.3). The guide comprises an .R file 401-lm-dev-how-to.R with code and advice.\nThe code in the .R file was written to work with the data file\n\n2022-12-08_all-studies-subject-scores.csv.\n\n\n\n\n\n\n\nTip\n\n\n\n\nWork through the steps in the how-to guide first, this practice will help you to understand what you need to do for the workbook tasks.\nThe how-to guide and the workbook have similar structures.\nThis is intentional: so that you can copy and adapt code from the how-to guide to do the practical tasks in the workbook.\n\n\n\n\n\n\n6.3.3 The workbook\nIn the workbook:\n\n401-lm-dev-workbook.R\n\nyou will work with the data file\n\n2022-12-08_all-studies-subject-scores.csv\n\nWe split .R scripts into parts, tasks and questions.\nFor this class on developing the linear model, our practical materials have two aims:\n\nHelping you to consolidate your learning on how to use linear models to estimate and to visualize the hypothetical association between outcome and predictor variables.\n\n\nYou will work to code linear models, to identify key statistical information in model outputs, and to interpret and report the results of the models.\nWe refresh your learning by working with a data-set you have not encountered before\nWe extend your skills by using a new function to generate predictions from fitted models.\n\n\nHelping you to learn how to extend your capacity to work with data to answer research questions by developing linear models that include multiple predictor variables.\n\n\nWe extend your skills by looking at how you work with categorical predictor variables: factors.\nBecause factors are so important to research in Psychology, we examine how to code or recode factor levels, and how to visualize the effects on outcomes of differences between factor levels.\n\nTo meet these aims, we progress through a series of parts:\n\nPart 2 shows you how you can read in data and at the same time ensure that different kinds of variables (e.g., factors versus numeric variables) are handled differently by R.\nPart 3 consolidates your learning on how to work with linear models when there is one outcome variable and just one predictor variable. Learning to work with linear models involves not just coding models but also being able to identify and interpret the results of the models you fit.\nPart 5 extends your capacities by helping you to learn how to code linear models that include multiple predictor variables.\nPart 6 builds your understanding of what linear models do, and what model estimates mean, by demonstrating a key point: linear models are coded to fit sample outcome data. When you look at model results, your interpretation is based on how the outcome is predicted to change, on average, given differences in values of one or more predictor variables.\nPart 7 builds your skills by helping you to learn how to code, visualize and interpret the impact on outcomes of the differences between factor levels.\n\nThroughout, we help you to develop skills in calculating and presenting model predictions.\n\nParts optional are designed to help you to examine the ways in which the association between variables may, itself, differ between different samples, and to help you to consolidate skills on exporting plots for use in reports.\n\nThe activity 401-lm-dev-workbook.R file takes you through the tasks, one by one.\nIf you are unsure about what you need to do, check the advice in 401-lm-dev-how-to.R.\nYou will see that you can match a task in the activity to the same task in the how-to. The how-to shows you what function you need and how you should write the function code. You will need to change the names of the data-set or the variables to complete the tasks in the activity.\n\n\n6.3.4 The data files\nThe data file we will work with has a similar structure to the structure you have seen before.\nHere are what the first few rows in the data file 2022-12-08_all-studies-subject-scores.csv looks like:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResponseId\nmean.acc\nmean.self\nAGE\nGENDER\nEDUCATION\nETHNICITY\nSHIPLEY\nHLVA\nFACTOR3\nNATIVE.LANGUAGE\nstudy\n\n\n\n\nR_1lcaBAGJNNI2kju\n1.00\n7.6\n18\nFemale\nFurther\nWhite\n31\n10\n48\nEnglish\nPSYC122\n\n\nR_AG4jiTm8oxmuOOZ\n0.90\n7.6\n18\nFemale\nFurther\nWhite\n35\n10\n40\nEnglish\nPSYC122\n\n\nR_2Ckb6YXLPGwYSvg\n0.95\n7.2\n18\nMale\nFurther\nAsian\n35\n9\n47\nOther\nPSYC122\n\n\nR_27JY5xHHcMs7jGi\n0.90\n6.8\n18\nFemale\nFurther\nWhite\n35\n8\n52\nEnglish\nPSYC122\n\n\nR_1DtJ4mrOXmxre01\n0.85\n6.4\n19\nFemale\nFurther\nWhite\n33\n9\n41\nEnglish\nPSYC122\n\n\nR_PRFQFInzSS6T8e5\n0.90\n6.2\n19\nFemale\nFurther\nMixed\n36\n5\n52\nEnglish\nPSYC122\n\n\n\n\n\nThere are two new columns:\n\nNATIVE.LANGUAGE self reported language status, whether the participant reports whether they are or are not a native speaker of English\nstudy codes for which study participant data were collected in\n\nYou can also see the columns you have seen before:\n\nResponseId participant code\nmean.acc average accuracy of response to questions testing understanding of health guidance\nmean.self average self-rated accuracy of understanding of health guidance\nstudy variable coding for what study the data were collected in\nAGE age in years\nGENDER gender code\nEDUCATION education level code\nETHNICITY ethnicity (Office National Statistics categories) code\nSHIPLEY vocabulary knowledge test score\nHLVA health literacy test score\nFACTOR3 reading strategy survey score\n\n\n\n6.3.5 The answers\nAfter the practical class, you will be able to download the answers version of the workbook here.\n\nThe answers version will present my answers for questions, and some extra information where that is helpful.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Developing the linear model</span>"
    ]
  },
  {
    "objectID": "lm-dev.html#sec-lm-dev-notes",
    "href": "lm-dev.html#sec-lm-dev-notes",
    "title": "6  Developing the linear model",
    "section": "6.4 Lecture notes",
    "text": "6.4 Lecture notes\nSome people find it easier to read notes than to watch video recordings. This is why we also include the lecture notes here.\n\n\n\n\n\n6.4.1 Analyze + visualize + present\n\n\n\n\n\n\n\n\nQ\n\n\ncluster_R\n\n\n\n\nnd_1\n\nGet raw data\n\n\n\nnd_2\n\nTidy data\n\n\n\nnd_1-&gt;nd_2\n\n\n\n\n\nnd_3_l\n\nVisualize\n\n\n\nnd_2-&gt;nd_3_l\n\n\n\n\n\nnd_3\n\nAnalyze\n\n\n\nnd_2-&gt;nd_3\n\n\n\n\n\nnd_3_r\n\nExplore\n\n\n\nnd_2-&gt;nd_3_r\n\n\n\n\n\nnd_3_a\n\nAssumptions\n\n\n\nnd_3_a-&gt;nd_3_l\n\n\n\n\n\nnd_3_a-&gt;nd_3\n\n\n\n\n\nnd_3_a-&gt;nd_3_r\n\n\n\n\n\nnd_3_l-&gt;nd_3\n\n\n\n\nnd_4\n\nPresent\n\n\n\nnd_3_l-&gt;nd_4\n\n\n\n\n\nnd_3-&gt;nd_4\n\n\n\n\n\n\n\n\nFigure 6.1: The data analysis pipeline or workflow: we focus on the linear model\n\n\n\n\n\n\n\n6.4.2 Develop the linear model: our aims\n\nWe will learn how to:\n\n\nExtend our capacity to code models so that we can incorporate multiple predictors\nDevelop the thought processes required to make decisions about what predictors to include\nDevelop the skills required to critically evaluate results\n\n\nEspecially considering potential variation across samples\n\n\n\n6.4.3 Develop the linear model: our aims\n\nWe will revise how to:\n\n\nIdentify and interpret model statistics\nCritically evaluate the results\nCommunicate the results\n\n\nWe will learn how to: explore extensions of the linear model\n\n\n\n6.4.4 We close the loop: Our context, the health comprehension project\n\nBecause public health impacts depend on giving people information they can understand\nWe want to know: What makes it easy or difficult to understand written health information?\n\n\n\n\n\n\nflickr: Sasin Tipchair ‘Senior woman in wheelchair talking to a nurse in a hospital’\n\n\n\n\n\n\n6.4.5 We close the loop: Health comprehension project, questions and analyses\n\nWe want to know: What makes it easy or difficult to understand written health information?\nSo our research questions are:\n\n\nWhat person attributes predict success in understanding?\nCan people accurately evaluate whether they correctly understand written health information?\n\n\n\n6.4.6 Extensions to the linear model: Multiple predictors\n\nWe need only a limited change to R code\nTo specify a model with multiple predictors\n\n\n\n6.4.7 How we estimate the association between two variables: One outcome and one predictor\n\nmodel &lt;- lm(mean.acc ~ SHIPLEY,\n            data = all.studies.subjects)\nsummary(model)\n\n\nSpecify the lm function and the model mean.acc ~ ...\nSpecify what data we use data = all.studies.subjects\nGet the results summary(model)\n\n\n\n6.4.8 How we estimate the association between multiple variables: One outcome and multiple predictors\n\nmodel &lt;- lm(mean.acc ~ SHIPLEY + HLVA + FACTOR3 + AGE + NATIVE.LANGUAGE,\n            data = all.studies.subjects)\nsummary(model)\n\n\nSpecify the lm function and the model:\n\n\nmean.acc ~ SHIPLEY + HLVA + FACTOR3 + AGE + NATIVE.LANGUAGE\n\n\n\n6.4.9 The sentence structure of model code in R\nTake a good look:\n\nlm(mean.acc ~ SHIPLEY + HLVA + FACTOR3 + AGE + NATIVE.LANGUAGE, ...)\n\nYou will see this sentence structure in coding for many different analysis types\n\nmethod(outcome ~ predictors)\npredictors could be SHIPLEY + HLVA + FACTOR3 + AGE + NATIVE.LANGUAGE ...\n\n\n\n6.4.10 Extensions to the linear model: Multiple predictors\n\nWe assume that the outcome prediction errors residuals are normally distributed\nWe do not assume that the distributions of predictor variables are normal\n\n\n\n6.4.11 Revision: What differences between observed and predicted outcome values look like\n\nDifferences between observed and predicted outcomes are shown by the vertical lines – outcome prediction errors: residuals\nBetter models should show smaller differences between observed and predicted outcome values\n\n\n\n\n\n\n\n\n\nFigure 6.2: The predicted change in mean comprehension accuracy, given variation in vocabulary scores. Observed values are shown in orange-red. Predicted values are shown in blue\n\n\n\n\n\n\n\n6.4.12 Revision: We typically assume that the residuals are normally distributed\n\nSome outcome prediction errors – residuals – are positive\nSome residuals are negative\nThe average of the residuals will be zero overall\n\n\n\n\n\n\n\n\n\nFigure 6.3: Plot showing the distribution of prediction errors – residuals – for the linear model of comprehension accuracy\n\n\n\n\n\n\n\n6.4.13 Multiple candidate predictor variables\n\n\n\n\n\n\n\n\nFigure 6.4: Scatterplot showing the potential association between accuracy of comprehension and variation on each of a series of potential predictor variables. Data from 8 studies\n\n\n\n\n\n\n\n6.4.14 We do not assume normal predictors\n\n\n\n\n\n\n\n\nFigure 6.5: Grid of plots showing the distribution of potential predictor variables. Data from 8 studies\n\n\n\n\n\n\n\n6.4.15 Extensions to the linear model: Multiple predictors\n\n\n\n\n\n\nTip\n\n\n\nWe can try to model anything using linear models: that is the real challenge we face\n\nAny analysis you have learned can instead be done using a linear model: ANOVA, t-test, correlation, \\(\\chi^2\\) test, …\nWe can work with any kind of dependent or independent variable you can think of\n\nThis is why we need to be careful\n\n\n\n\n6.4.16 Analyses are done in context so when we conduct analyses we must use contextual information\nClosing the loop: The health comprehension project questions\n\nWe want to know: What makes it easy or difficult to understand written health information?\nSo our research questions include:\n\n\nWhat person attributes predict success in understanding?\n\n\n\n6.4.17 We must use contextual information: theory of comprehension\n\n\n\n\n\n\n\n\nQ\n\n\n\nnd_1_l\n\nLanguage experience\n\n\n\nnd_2\n\nComprehension outcome\n\n\n\nnd_1_l-&gt;nd_2\n\n\n\n\n\nnd_1_r\n\nReasoning capacity\n\n\n\nnd_1_r-&gt;nd_2\n\n\n\n\n\n\n\n\nFigure 6.6: Understanding text depends on (1.) language experience and (2.) reasoning ability (Freed et al., 2017)\n\n\n\n\n\n\n\n6.4.18 Given theory, model of comprehension accuracy should include measures of\n(1.) experience (HLVA, SHIPLEY) and (2.) reasoning ability (reading strategy)\n\n\n\n\n\n\n\n\nQ\n\n\n\nnd_1_l\n\nLanguage experience\n\n\n\nnd_2\n\nComprehension outcome\n\n\n\nnd_1_l-&gt;nd_2\n\n\n\n\n\nnd_1_r\n\nReasoning capacity\n\n\n\nnd_1_r-&gt;nd_2\n\n\n\n\n\n\n\n\nFigure 6.7: Understanding text depends on (1.) language experience and (2.) reasoning ability (Freed et al., 2017)\n\n\n\n\n\n\n\n6.4.19 The flexibility and power of linear models requires us to be aware of the garden of forking paths\n\nWhich variables should be included in an analysis?\nAll of them; some of them; why?\nWill others disagree with reason?\n\n\n\n\n\n\n\n\n\nD\n\n\n\nA\n\nA\n\n\n\nB1\n\nB1\n\n\n\nA-&gt;B1\n\n\n\n\n\nB2\n\nB2\n\n\n\nA-&gt;B2\n\n\n\n\n\nC1\n\nC1\n\n\n\nB1-&gt;C1\n\n\n\n\n\nC2\n\nC2\n\n\n\nB1-&gt;C2\n\n\n\n\n\nC3\n\nC3\n\n\n\nB1-&gt;C3\n\n\n\n\n\nC4\n\nC4\n\n\n\nB2-&gt;C4\n\n\n\n\n\nC5\n\nC5\n\n\n\nB2-&gt;C5\n\n\n\n\n\nC6\n\nC6\n\n\n\nB2-&gt;C6\n\n\n\n\n\n\n\n\nFigure 6.8: Forking paths in data analysis\n\n\n\n\n\n\n\n6.4.20 Different researchers can reasonably make different choices\nThis is why we care about open science\n\nTheory- and evidence-based selection of critical variables for analysis \\(\\rightarrow\\) literature review\nShare usable data and analysis code in open repositories \\(\\rightarrow\\) research report exercise, PSYC403 data archiving\n\n\n\n6.4.21 Coding, thinking about, and reporting linear models with multiple predictors\n\nlm(mean.acc ~ SHIPLEY + HLVA + FACTOR3 + AGE + NATIVE.LANGUAGE, ...)\n\n\n\n6.4.22 Coding the linear model with multiple predictors\n\nlm(mean.acc ~ SHIPLEY + HLVA + FACTOR3 + AGE + NATIVE.LANGUAGE, ...)\n\n\nThe code represents a linear model with multiple predictors:\n\\(y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots + \\epsilon\\)\n\n\n\n6.4.23 Thinking about the linear model with multiple predictors\n\\(y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots + \\epsilon\\)\nOutcome \\(y\\) is calculated as the sum of:\n\nThe intercept \\(\\beta_0\\) plus\nThe product of the coefficient of the effect of e.g. AGE \\(\\beta_1\\) multiplied by \\(x_1\\) a person’s age +\n+ any number of other variables +\nThe error \\(\\epsilon\\): mismatches between observed and predicted outcomes\n\n\n\n6.4.24 Identifying key information in results\n\n\n\nCall:\nlm(formula = mean.acc ~ SHIPLEY + HLVA + FACTOR3 + AGE + NATIVE.LANGUAGE, \n    data = all.studies.subjects)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.55939 -0.08115  0.02056  0.10633  0.41598 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           0.1873086  0.0472991   3.960 8.47e-05 ***\nSHIPLEY               0.0073947  0.0011144   6.635 7.70e-11 ***\nHLVA                  0.0242787  0.0031769   7.642 9.44e-14 ***\nFACTOR3               0.0053455  0.0008947   5.975 4.12e-09 ***\nAGE                  -0.0026434  0.0004905  -5.390 1.05e-07 ***\nNATIVE.LANGUAGEOther -0.0900035  0.0141356  -6.367 4.04e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1612 on 555 degrees of freedom\n  (54 observations deleted due to missingness)\nMultiple R-squared:  0.4221,    Adjusted R-squared:  0.4169 \nF-statistic: 81.09 on 5 and 555 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n6.4.25 Identifying key information in results\n\nThe summary() of the linear model shows:\nEstimates of the coefficients of the effects of the predictors we included, with null hypothesis significance tests of those estimates\nModel fit statistics including R-squared and F-statistic estimates\n\n\n\n6.4.26 For each predictor, e.g. HLVA, we see\n\n\n\nCall:\nlm(formula = mean.acc ~ SHIPLEY + HLVA + FACTOR3 + AGE + NATIVE.LANGUAGE, \n    data = all.studies.subjects)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.55939 -0.08115  0.02056  0.10633  0.41598 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           0.1873086  0.0472991   3.960 8.47e-05 ***\nSHIPLEY               0.0073947  0.0011144   6.635 7.70e-11 ***\nHLVA                  0.0242787  0.0031769   7.642 9.44e-14 ***\nFACTOR3               0.0053455  0.0008947   5.975 4.12e-09 ***\nAGE                  -0.0026434  0.0004905  -5.390 1.05e-07 ***\nNATIVE.LANGUAGEOther -0.0900035  0.0141356  -6.367 4.04e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1612 on 555 degrees of freedom\n  (54 observations deleted due to missingness)\nMultiple R-squared:  0.4221,    Adjusted R-squared:  0.4169 \nF-statistic: 81.09 on 5 and 555 DF,  p-value: &lt; 2.2e-16\n\n\n\nThe Coefficient Estimate: 0.0242787 for the slope of the effect of variation in HLVA scores\nThe Std. Error (standard error) 0.0031769 for the estimate\nThe t value of 7.642 and associated Pr(&gt;|t|) p-value 9.44e-14 for the null hypothesis test of the coefficient\n\n\n\n6.4.27 Identifying the key information in the linear model results: Coefficients\n\n\n\nCall:\nlm(formula = mean.acc ~ SHIPLEY + HLVA + FACTOR3 + AGE + NATIVE.LANGUAGE, \n    data = all.studies.subjects)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.55939 -0.08115  0.02056  0.10633  0.41598 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           0.1873086  0.0472991   3.960 8.47e-05 ***\nSHIPLEY               0.0073947  0.0011144   6.635 7.70e-11 ***\nHLVA                  0.0242787  0.0031769   7.642 9.44e-14 ***\nFACTOR3               0.0053455  0.0008947   5.975 4.12e-09 ***\nAGE                  -0.0026434  0.0004905  -5.390 1.05e-07 ***\nNATIVE.LANGUAGEOther -0.0900035  0.0141356  -6.367 4.04e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1612 on 555 degrees of freedom\n  (54 observations deleted due to missingness)\nMultiple R-squared:  0.4221,    Adjusted R-squared:  0.4169 \nF-statistic: 81.09 on 5 and 555 DF,  p-value: &lt; 2.2e-16\n\n\n\nPay attention to sign and the size of coefficient estimate:\nIs the coefficient (e.g., HLVA 0.0242787) a positive or a negative number? is it relatively large or small?\n\n\n\n6.4.28 Identifying the key information in the linear model results: R-squared\n\n\n\nCall:\nlm(formula = mean.acc ~ SHIPLEY + HLVA + FACTOR3 + AGE + NATIVE.LANGUAGE, \n    data = all.studies.subjects)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.55939 -0.08115  0.02056  0.10633  0.41598 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           0.1873086  0.0472991   3.960 8.47e-05 ***\nSHIPLEY               0.0073947  0.0011144   6.635 7.70e-11 ***\nHLVA                  0.0242787  0.0031769   7.642 9.44e-14 ***\nFACTOR3               0.0053455  0.0008947   5.975 4.12e-09 ***\nAGE                  -0.0026434  0.0004905  -5.390 1.05e-07 ***\nNATIVE.LANGUAGEOther -0.0900035  0.0141356  -6.367 4.04e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1612 on 555 degrees of freedom\n  (54 observations deleted due to missingness)\nMultiple R-squared:  0.4221,    Adjusted R-squared:  0.4169 \nF-statistic: 81.09 on 5 and 555 DF,  p-value: &lt; 2.2e-16\n\n\n\nRevision: Pay attention to R-squared\nR-squared indicates how much outcome variation we can predict, given our model\nRevision: we report Adjusted R-squared because it tends to be more accurate\n\n\n\n6.4.29 Identifying the key information in the linear model results: F\n\n\n\nCall:\nlm(formula = mean.acc ~ SHIPLEY + HLVA + FACTOR3 + AGE + NATIVE.LANGUAGE, \n    data = all.studies.subjects)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.55939 -0.08115  0.02056  0.10633  0.41598 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           0.1873086  0.0472991   3.960 8.47e-05 ***\nSHIPLEY               0.0073947  0.0011144   6.635 7.70e-11 ***\nHLVA                  0.0242787  0.0031769   7.642 9.44e-14 ***\nFACTOR3               0.0053455  0.0008947   5.975 4.12e-09 ***\nAGE                  -0.0026434  0.0004905  -5.390 1.05e-07 ***\nNATIVE.LANGUAGEOther -0.0900035  0.0141356  -6.367 4.04e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1612 on 555 degrees of freedom\n  (54 observations deleted due to missingness)\nMultiple R-squared:  0.4221,    Adjusted R-squared:  0.4169 \nF-statistic: 81.09 on 5 and 555 DF,  p-value: &lt; 2.2e-16\n\n\n\nThe model summary gives us the F-statistic:\nRevision: the F-test of the null hypothesis that the model does not predict the outcome\n\n\n\n6.4.30 Plot predictions to interpret effects\n\n\n\n\n\n\n\n\nFigure 6.9: A grid of plots showing model predictions, for outcome accuracy, given variation in (a.) age, (b.) vocabulary, (c.) health literacy, (d) reading strategy and (e.) native language. Data from eight studies\n\n\n\n\n\n\n\n6.4.31 Compare estimates with effects plots\n\n\n\n\n\n\n\n\n\n\nCoefficients estimates in the summary match what we see\nPositive coefficients show upward slopes\nLarger coefficients show steeper slopes\n\n\n\n6.4.32 The language and style of reporting linear model results\n\nWe fitted a linear model with mean comprehension accuracy as the outcome and, as predictors: vocabulary knowledge (Shipley), health literacy (HLVA), reading strategy (FACTOR3), age (years) and native language status. Our analysis indicated significant effects of all predictor variables. The model is significant overall, with \\(F(5, 555) = 81.09, p &lt; .001\\), and explains 42% of variance (\\(\\text{adjusted } R^2 = 0.42\\)). The model estimates showed that the accuracy of comprehension increased with higher levels of participant vocabulary knowledge (\\(\\beta = .007, t = 6.64, p &lt;.001\\)), health literacy (\\(\\beta = .024, t = 7.64, p &lt;.001\\)), and reading strategy (\\(\\beta = .005, t = 5.98, p = &lt; .001\\)). Younger participants (\\(\\beta = -0.003, t = -5.39, p &lt;.001\\)) and native speakers of English as another language (\\(\\beta = -.090, t = -6.37, p &lt;.001\\)) tended to show lower levels of accuracy.\n\n\n\n6.4.33 Look at what we do with the text\n\nWe fitted a linear model with mean comprehension accuracy as the outcome and, as predictors: vocabulary knowledge (Shipley), health literacy (HLVA), reading strategy (FACTOR3), age (years) and native language status. Our analysis indicated significant effects of all predictor variables. The model is significant overall, with \\(F(5, 555) = 81.09, p &lt; .001\\), and explains 42% of variance (\\(\\text{adjusted } R^2 = 0.42\\)). The model estimates showed that the accuracy of comprehension increased with higher levels of participant vocabulary knowledge (\\(\\beta = .007, t = 6.64, p &lt;.001\\)), health literacy (\\(\\beta = .024, t = 7.64, p &lt;.001\\)), and reading strategy (\\(\\beta = .005, t = 5.98, p = &lt; .001\\)). Younger participants (\\(\\beta = -0.003, t = -5.39, p &lt;.001\\)) and native speakers of English as another language (\\(\\beta = -.090, t = -6.37, p &lt;.001\\)) tended to show lower levels of accuracy.\n\n\nExplain: the method (linear model); the outcome (accuracy) and the predictors\nReport the model fit statistics overall (\\(F, R^2\\))\nReport the significant effects (\\(\\beta, t, p\\)) and describe the nature of the effects\n\n\n\n6.4.34 Critically evaluating the results of analyses involving linear models\nThere are three levels of uncertainty when we look at sample data (mcelreath2020?) – uncertainty over:\n\nThe nature of the expected change in outcome\nThe ways that expected changes might vary between individual participants or between groups of participants\nThe random ways that specific responses can be produced\n\n\n\n6.4.35 Critically evaluating the results of analyses involving linear models\n\nThese uncertainties require us to carefully qualify the conclusions we draw from data analyses\nThis does not mean we should avoid causal language when we think that psychological processes cause the behaviours we examine (Grosz2020?)\nBut it does mean we can be careful to identify the limits in the evidence we analyse\n\n\n\n6.4.36 Revision: As we move into thinking about the data analysis, we need to identify our assumptions\n\nvalidity: that differences in knowledge or ability cause differences in test scores\nmeasurement: that this is equally true across the different kinds of people we tested\ngeneralizability: that the sample of people we recruited resembles the population\n\n\n\n6.4.37 How do you do this work?\n\nvalidity\n\n\nWe want to work with valid measures but validity requires explaining (Borsboom et al., 2004):\n\n\nDoes the thing exist in the world?\nIs variation in that thing be reflected in variation in our measurement?\n\n\nWhat you can do: literature review \\(\\rightarrow\\) to identify your reasoning in answer to these questions\n\n\n\n6.4.38 How do you do this work?\n\nmeasurement\ngeneralizability\n\n\nIt is most helpful to assume from the start that effects estimates will vary (Gelman, 2015; Vasishth & Gelman, 2021)\nSo then we ask ourselves: will this test work in the same way in different groups?\nAnd we ask: how will these effects estimates vary across different groups\n\n\n\n6.4.39 Why we need replication studies\n\n\n\n\n\n\n\n\nFigure 6.10: Scatterplot showing the potential association between accuracy of comprehension and vocabulary scores: Data from eight studies. Effects will vary between different samples so: expect the variation (Gelman, 2015; Vasishth & Gelman, 2021) &gt;&gt;&gt; important to evaluating claims in the literature, and to evaluation of your own results\n\n\n\n\n\n\n\n6.4.40 Why we need replication studies\n\n\n\n\n\n\n\n\nFigure 6.11: Effects will vary between samples so expect the variation (Gelman, 2015; Vasishth & Gelman, 2021) &gt;&gt;&gt; ask what variation may result from systematic differences between groups\n\n\n\n\n\n\n\n6.4.41 Why we need to consider the generalizability of sample data\n\n\n\n\n\n\n\n\nFigure 6.12: Grid of plots showing the distribution of potential predictor variables\n\n\n\n\n\n\n\n6.4.42 Convenience samples are common in Psychology\n\nWe test who we can – convenience sampling – and who we can test has an impact on the quality of evidence (Bornstein et al., 2013)\nIf age, ethnicity or gender are not balanced \\(\\rightarrow\\) does this matter to your research question?\nIf samples are limited in size \\(\\rightarrow\\) how does that affect our uncertainty over effects estimates?\n\n\n\n6.4.43 The linear model is very flexible, powerful and general\n\nMost introductory statistics classes teach each statistical test as if they are independent\n\n\n\n\n\n\n\nTip\n\n\n\nMost common statistical tests are special cases of linear models, or are close approximations\n\n\n\n\n6.4.44 The t-test as linear model\n\\(y_i = \\beta_0 + \\beta_1X\\)\n\nIf you have two groups, with a variable X coding for group membership\nThen the mean outcome for one group \\(= \\beta_0\\)\nThe estimate of the slope \\(\\beta_1\\) tells about the average difference between groups\nAnd we can code the model like this: lm(y ~ group)\n\n\n\n6.4.45 ANOVA as linear model\n\\(y_i = \\beta_0 + \\beta_1X + \\beta_2Z + \\beta_3XZ\\)\n\nIf you have a 2 x 2 factorial design, with two factors factor.1, factor.2, and a dataset with variables X, Z coding for group membership\nThen the mean outcome for baseline conditions \\(= \\beta_0\\)\nThe estimates of the slopes \\(\\beta_1, \\beta_2\\) tells about the average difference between groups\nThe estimate of the slope \\(\\beta_3\\) tells us about the interaction\nAnd we can code the model like this: lm(y ~ factor.1*factor.2)\nOr this Anova(aov(y ~ factor.1*factor.2, data), type='II')\n\n\n\n6.4.46 ANOVA as linear model\n\nIn general, the psychological literature is full of ANOVA\nBut the field is moving away from ANOVA towards mixed-effects models\n\n\n\n\n\n\n\nTip\n\n\n\nWe have to make choices in teaching and, here, we are choosing to focus on a powerful, flexible, and generally applicable method we can explain in depth: linear models\n\nOur aim is for students to better understand how to use a general approach\n\n\n\n\n\n6.4.47 Extensions to the linear model\n\\(outcome ~ predictors + error\\)\n\noutcome can generalize to analyse data that are not metric, do not come from normal distributions\npredictors can be curvilinear, categorical, involve interactions\nerror can be independent; can be non-independent\n\n\n\n6.4.48 Look ahead: extensions to the linear model\n\nWhat if the outcome measurement data cannot be understood to be metric or to come from a normal probability distribution?\n\n\n\n6.4.49 Extensions to the linear model – binary or dichotomous outcomes\n\nBinary outcomes are very common in Psychology: yes or no; correct or incorrect; left or right visual field etc.\nThe change in coding is e.g. glm(ratings ~ predictors, family = \"binomial\")\n\n\n\n6.4.50 Extensions to the linear model – ordinal outcomes\n\nLikert scale or ratings data are best analysed using ordinal models (Liddell & Kruschke, 2018)\nThe change in coding [Christensen (n.d.)] is e.g. clm(ratings ~ predictors)\n\n\n\n6.4.51 Extensions to the linear model – non-independence of observations\n\nMuch – maybe most – psychological data are collected in ways that guarantee the non-independence of observations\n\n\nWe test children in classes, patients in clinics, individuals in regions\nWe test participants in multiple trials in an experiment, recording responses to multiple stimuli\n\n\nThese data should be analysed using linear mixed-effects models (Meteyard & Davies, 2020)\n\n\n\n6.4.52 General advice\nAn old saying goes:\n\nAll models are wrong but some are useful\n\n(attributed to George Box).\n\n\n\n\n\n\nTip\n\n\n\n\nSometimes, it can be useful to adopt a simpler approach as a way to approximate get closer to better methods\nBox also advises “Since all models are wrong the scientist must be alert to what is importantly wrong. It is inappropriate to be concerned about mice when there are tigers abroad.”\nHere, we focus on validity, measurement, generalizability and critical thinking\n\n\n\n\n\n6.4.53 Summary\n\nLinear models\n\n\nLinear models are a very general, flexible, and powerful analysis method\nWe can use assuming that prediction outcomes (residuals) are normally distributed\nWith potentially multiple predictor variables\n\n\nThinking about linear models\n\n\nClosing the loop: when we plan an analysis we should try to use contextual information – theory and measurement understanding – to specify our model\nClosing the loop: when we critically evaluate our or others’ findings, we should consider validity, measurement, and generalizability\n\n\nReporting linear models\n\n\nWhen we report an analysis, we should report:\n\n\nExplain what I did, specifying the method (linear model), the outcome variable (accuracy) and the predictor variables (health literacy, reading strategy, reading skill and vocabulary)\nReport the model fit statistics overall (\\(F, R^2\\))\nReport the significant effects (\\(\\beta, t, p\\)) and describe the nature of the effects (does the outcome increase or decrease?)\n\n\n\n\n\nBornstein, M. H., Jager, J., & Putnick, D. L. (2013). Sampling in developmental science: Situations, shortcomings, solutions, and standards. Developmental Review, 33(4), 357–370. https://doi.org/10.1016/j.dr.2013.08.003\n\n\nBorsboom, D., Mellenbergh, G. J., & Heerden, J. van. (2004). The concept of validity. Psychological Review, 111(4), 1061–1071. https://doi.org/10.1037/0033-295X.111.4.1061\n\n\nChristensen, R. H. B. (n.d.). Cumulative Link Models for Ordinal Regression with the R Package ordinal.\n\n\nFreed, E. M., Hamilton, S. T., & Long, D. L. (2017). Comprehension in proficient readers: The nature of individual variation. Journal of Memory and Language, 97, 135–153. https://doi.org/10.1016/j.jml.2017.07.008\n\n\nGelman, a. (2015). The connection between varying treatment effects and the crisis of unreplicable research: A bayesian perspective. Journal of Management, 41(2), 632–643. https://doi.org/10.1177/0149206314525208\n\n\nLiddell, T. M., & Kruschke, J. K. (2018). Analyzing ordinal data with metric models: What could possibly go wrong? Journal of Experimental Social Psychology, 79(September), 328–348. https://doi.org/10.1016/j.jesp.2018.08.009\n\n\nMeteyard, L., & Davies, R. A. I. (2020). Best practice guidance for linear mixed-effects models in psychological science. Journal of Memory and Language, 112. https://doi.org/10.1016/j.jml.2020.104092\n\n\nVasishth, S., & Gelman, A. (2021). How to embrace variation and accept uncertainty in linguistic and psycholinguistic data analysis. Linguistics, 59(5), 1311–1342. https://doi.org/10.1515/ling-2019-0051",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Developing the linear model</span>"
    ]
  },
  {
    "objectID": "report-preface.html",
    "href": "report-preface.html",
    "title": "7  The structured research report: Quick start",
    "section": "",
    "text": "7.1 What you have to write\nLet’s begin with what you need to write to complete this coursework.\nWhat is a structured report?\nYou will submit short answers to a series of questions:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The structured research report: Quick start</span>"
    ]
  },
  {
    "objectID": "report-preface.html#sec-report-quick-what",
    "href": "report-preface.html#sec-report-quick-what",
    "title": "7  The structured research report: Quick start",
    "section": "",
    "text": "Submission deadline: On Friday 17th January 2025\nYou will submit a structured report.\nPresenting an analysis: what you did; what you found; the context; and the implications.\n\n\n\n\n\n\n\nTip\n\n\n\nYou will be able to find the submission point on Moodle here when it is revealed.\n\n\n\n\n\nDescribe the questions or the predictions you examine in your analysis.\nExplain the research background: you are analyzing previously collected data, so briefly explain why the people who originally collected the data did that work.\nExplain if you are attempting to repeat the analysis that the original researchers did or if you are doing something different.\nExplain the motivation for the questions or the predictions your analysis examines.\nSummarize the methods that were used to collect the data you analyzed.\nIdentify what variables are included in the analysis: what variable is the outcome (or dependent) variable, and what variables are the predictor (or independent) variables.\nDescribe the analysis method you use to address the question or test the predictions.\nExplain your analysis method choices: why are you using this method?\nIdentify the model you use in your analysis.\nPresent a summary of the analysis results: use text and plots to show what you found.\nExplain the implications of the results, in theoretical or in policy terms.\nCritically evaluate your findings: reflect on the strength or the limitations of the evidence you present, as answers to the questions or as tests of the predictions you outlined.\nPredict: how can future research build on the work you have done?\n\n\n\n\n\n\n\nTip\n\n\n\n\nYou will write each answer in sentences, organized in one or more paragraphs.\nYou will submit your analysis code in an appendix.\nWhere appropriate, you will include plots.\nSubmit your report as a series of answers to the questions listed in a single document.\nYou can use each question as a heading in the document.\nWord count limit: no more than 1500 words are allowed for all materials except references, appendices, and the content of tables.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The structured research report: Quick start</span>"
    ]
  },
  {
    "objectID": "report-preface.html#sec-report-quick-how",
    "href": "report-preface.html#sec-report-quick-how",
    "title": "7  The structured research report: Quick start",
    "section": "7.2 How you can do the analysis work",
    "text": "7.2 How you can do the analysis work\nReports will concern, usually, findings from analyses of data collected in previous studies or data accessed from online sources. These data will usually be associated with a published report in a journal like Psychological Science or a pre-print archive like PsyArXiv.\nWe expect students to use one of the analysis methods taught in the module.\n\n\n\n\n\n\nTip\n\n\n\nIf you want to know more about how to do this work, you can read in-depth guidance in Chapter 8.\n\n\nYou can see information on APA formatting of statistics and numbers in the OWL Purdue guide. Though the APA guidelines are the authoritative guide.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The structured research report: Quick start</span>"
    ]
  },
  {
    "objectID": "report-preface.html#sec-report-quick-why",
    "href": "report-preface.html#sec-report-quick-why",
    "title": "7  The structured research report: Quick start",
    "section": "7.3 Why we are asking you to do this",
    "text": "7.3 Why we are asking you to do this\nWe are asking you to do the structured research report because doing the work — the analysis and the writing — will build your awareness and understanding of how psychological science really works, and will strengthen your sense of what best practice looks like in modern science.\n\n7.3.1 Lecture recordings – videos\nClick on a link and your browser should open a tab showing the Panopto video for the lecture part. You should be able to access the videos anywhere; you should not need to be on campus or logged on to the university VPN to view the videos.\n\nOverview (20 minutes): the key ideas, the scientific context\n\n\n\nOverview (20 minutes): the research workflow, multiverse analyses\n\n\n\nOverview (20 minutes): kinds of reproducibility, open data, and doing better science.\n\n\n\n\n7.3.2 Lecture recordings – slides\nYou can download the lecture slides as a single downloadable .html file that you can open in any browser: 411-research-report.html. This can be opened in a browser and presents the slides as they are delivered.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The structured research report: Quick start</span>"
    ]
  },
  {
    "objectID": "report-preface.html#sec-report-quick-how-mark",
    "href": "report-preface.html#sec-report-quick-how-mark",
    "title": "7  The structured research report: Quick start",
    "section": "7.4 How we will mark your work",
    "text": "7.4 How we will mark your work\nA distinction requires:\nA. Background and methods\n\nA concise explanation of the background or context for the research. Why did the original study authors do the research? Why are you doing the analysis you report? The explanation should briefly reference previous research, identifying the limitations of that research, where appropriate.\nA clear statement of the research question that your analysis addresses, or the hypothesis or prediction that your analysis tests. A statement of questions or predictions should be specific and concrete, i.e., not just a statement that some difference is expected: what difference is expected?\nThere should be a coherent, logical, argument for how the background or context leads to the questions or predictions.\n\nB. Analysis and results\n\nA clear account of which variables are included in the analyses, so that the reader can understand what outcome (dependent) variable is being analyzed using what predictor (independent) variables.\nA concise summary of the properties of the variables — how data were collected, how variable values were coded or scored — explaining enough relevant information that the reader can understand the structure of the data, and the reliability or validity of observations.\nA scholarly explanation of the analysis you did, including an account of why you chose to use the method you use, given possible alternatives and, if relevant, referring to justifications for your choice that are identified in the statistical literature.\nA clear identification of what analysis you did, using what method, outlining the elements and structure of the model whose results you present.\nEvidence that the analysis method you chose was used correctly, that the method was appropriate to the question and the data, and that the code you used could do the analysis you say it did.\nA clear presentation of the results, presenting a correct interpretation of statistical information concerning the size, the direction (or sign), and the significance of any effect you estimate or difference you test. The presentation should explain how the outcome is expected to vary, on average, given the differences or the effects estimated using your analyses.\nThere is full and correct use of APA conventions in the presentation of statistical results.\nWhere appropriate, results are presented using both text and relevant, informative, visualizations.\n\nC. Implications and critical reflection\n\nAn accurate account of whether or how the analysis results address the questions, or how the analysis results support or disconfirm the predictions stated.\nA scholarly explanation of the implications of the results: what do the results suggest we now know that we did (or did not) know before?\nA critical analysis of the strengths or limitations of the evidence: how confident can we be, how uncertain should we be, that the results that are presented do or do not suggest what you think they suggest.\nA constructive analysis of how future research could build on your findings to further extend understanding or evaluate methodological concerns.\n\nA merit will be awarded, by comparison, if:\nA. Background and methods\n\nThere is a concise explanation of the background or context for the research. There is limited or partial information on why the original study authors did the research or why you are you doing the analysis you report. We may see statements but we do not see an argument explaining the justifications motivating either the original or your research.\nThere is a clear statement of the research question, or the hypothesis or prediction that your analysis tests. But the question or the prediction may be vague, or stated in general not specific terms.\n\nB. Analysis and results\n\nThere is a clear account of which variables are included in the analyses, so that the reader can understand what outcome (dependent) variable is being analyzed using what predictor (independent) variables.\nThe summary of the properties of the variables may not be sufficiently clear or informative to communicate effectively the structure of the data, or to support evaluations of the reliability or validity of analysis results.\nThere is an explanation of the analysis you did, but there is limited explanation of why you chose to use the method you use. There is no or there is limited awareness of possible alternatives. No informed justification is presented for analysis choices.\nThere is a clear identification of what analysis you did, using what method, outlining the elements and structure of the model whose results you present.\nThere is evidence that the analysis method you chose was used correctly, that the method was appropriate to the question and the data, and that the code you used could do the analysis you say it did.\nThere is a clear presentation of the results, presenting a correct interpretation of statistical information concerning the size, the direction (or sign), and the significance of any effect you estimate or difference you test. The presentation should explain how the outcome is expected to vary, on average, given the differences or the effects estimated using your analyses.\nThere is full and correct use of APA conventions in the presentation of statistical results.\nWhere appropriate, results are presented using both text and relevant, informative, visualizations.\n\nC. Implications and critical reflection\n\nThere is an accurate account of whether or how the analysis results address the questions, or how the analysis results support or disconfirm the predictions stated.\nThere is some explanation of the implications of the results but this account is not linked to previous research (in the literature).\nThere is limited reflection on the strengths or limitations of the evidence. We may see generic but not well informed discussion of the evidence specific to your analysis or your data.\nThere is limited discussion of future research.\n\nA pass will be awarded, by comparison, if:\nA. Background and methods\n\nThere is a concise explanation of the background or context for the research. There is limited or no information on why the original study authors did the research or why you are you doing the analysis you report.\nThere is a statement of the research question, or the hypothesis or prediction that your analysis tests. But the question or the prediction may be vague, or stated in general terms only.\n\nB. Analysis and results\n\nThere is a clear account of which variables are included in the analyses, so that the reader can understand what outcome (dependent) variable is being analyzed using what predictor (independent) variables.\nThe summary of the properties of the variables provides information on the structure of the data but it may be unclear how one or more variables were coded or scored. There is limited engagement with questions of measurement reliability or validity.\nThere is an explanation of the analysis you did, but there is no explanation of why you chose to use the method you use. Limited or no informed justification is presented for analysis choices.\nThere is a clear identification of what analysis you did, using what method, outlining the elements and structure of the model whose results you present.\nThere is evidence that the analysis method you chose was used correctly, that the method was appropriate to the question and the data, and that the code you used could do the analysis you say it did.\nThere is a clear presentation of the results, presenting a correct interpretation of statistical information concerning the direction (or sign) and the significance of any effect you estimate or difference you test. The presentation should explain how the outcome is expected to vary, on average, given the differences or the effects estimated using your analyses.\nThere may be partial use of APA conventions in the presentation of statistical results.\nAppropriate visualizations may be included but without comment or discussion.\n\nC. Implications and critical reflection\n\nThere is a summary account of the results but it may not clearly explain whether or how the analysis results address the questions, or how the analysis results support or do not support the predictions stated.\nThere is limited or no explanation of the implications of some of the results.\nThere is limited or no reflection on the strengths or limitations of the evidence.\nThere is no discussion of future research.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The structured research report: Quick start</span>"
    ]
  },
  {
    "objectID": "how.html",
    "href": "how.html",
    "title": "8  How you can do the analysis work",
    "section": "",
    "text": "8.1 The variety of things students do\nStudents have taken a variety of approaches to the assignment.\nAsk in class or on the Moodle discussion forum for advice about any one of these approaches.\nI consider, first, working with data-sets where an analysis of the data has been presented in the article (see Section 8.2). I then look at working with data-sets where the data are presented without an analysis (see Section 8.3). Our advice on working with data-sets presented without an analysis will overlap in key respects with our advice on working with curated demonstration data.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>How you can do the analysis work</span>"
    ]
  },
  {
    "objectID": "how.html#sec-how-variety",
    "href": "how.html#sec-how-variety",
    "title": "8  How you can do the analysis work",
    "section": "",
    "text": "Using demonstration data — Some students choose to complete an analysis of one of the data-sets used for practical exercises in class: the example or demonstration data we collect together as curated data-sets.\nAnalyzing public data that have been previously analysed — Some students choose to complete an analysis of a publicly available data-set that has been analysed previously, where the analysis report has been published in a journal article.\nAnalyzing public data that have not been previously analysed — Some students choose to complete an analysis of a publicly available data-set where an analysis report has not been published in a journal article.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>How you can do the analysis work</span>"
    ]
  },
  {
    "objectID": "how.html#sec-publishedanalysis",
    "href": "how.html#sec-publishedanalysis",
    "title": "8  How you can do the analysis work",
    "section": "8.2 Working with data associated with a published analysis",
    "text": "8.2 Working with data associated with a published analysis\nIn the following, I split our guidance into two parts.\n\nI look next at the task of locating, accessing and checking the data (Section 8.2.1).\nThen I look at the task of figuring out what analysis you can do with the data (see Section 8.2.2).\n\nObviously, you cannot consider an analysis if you cannot be sure that you can work with the data (Minocher et al., n.d.).\n\n8.2.1 Locate, access and check the data\nAt the start of your work on the assignment, you will need to (1.) locate then (2.) access data for analysis, and then you will need to (3.) check that the data are usable. I set out advice on doing each step, following. Work through the steps: one step at a time.\n\n8.2.1.1 Locate\nIt is usually helpful to find a data-set where the data have been collected in a study within a topic area you care about, or could be interested in. It is helpful because you will need to work with the data and it will be motivating if you are interested in what the data concern. And it is helpful because, often, you will need to do a bit of reading on related research to learn about the context for the data collection, and you will usually want to read research sources that interest you.\n\n\n\n\n\n\nTip\n\n\n\nThe task here is:\n\nDo a search: look for an article with usable data in a topic area that interests you.\n\n\n\nThere are at least two ways you can do this. Both should be reasonably quick methods to get to a usable data-set.\n\nDo a search on Google scholar).\nDo a search on the webpages of a journal.\n\nMost psychological research is published in journals like Psychological Science. If you want, you can look at a list of psychology journals here.\nIn a journal like Psychological Science you can look through lists of previously published articles (in issues, volumes, by year) on the journal webpage. Here is the list of issues for Psychological Science..\n\n8.2.1.1.1 Key words\nIn both methods, you are looking for an article associated with data (and maybe analysis code) you can access and that you are sure you can use. In both methods, you need to first think about some key words to use in your search. Ask yourself:\n\nWhat are you interested in? What population, intervention or effect, comparison, or outcome?\n\nThen:\n\nWhat words do people use, in articles you have seen, when they talk about this thing?\n\nYou can use these words, and maybe consider alternate terms. For example, I am interested in reading comprehension or development reading comprehension but researchers working on reading development might also refer to children reading comprehension.\nYou want to be as efficient as possible so combine your search for articles in an interesting topic area with your search for accessible data. We can learn from the research we discussed on data sharing practices (see Section 9.2.6.2) by looking for specific markers that data associated with an article should be accessible.\nIf you are doing a search (1.) on Google scholar), I would use the key words related to your topic plus words like: open data badge; open science badge. So, I would do a search for the words: reading comprehension open data badge. I have done this: you can try it. The search results will list articles related to the topic of reading comprehension, where the authors claim to have earned the open data badge because they have made data available.\nIf you are doing a search (2.) in a journal list of articles, then what you are looking for are articles that interest you and which are listed with open data badges. In the listing for Psychological Science (here)) a quick read of the journal issue articles index shows that article titles are listed together with symbols representing the open science badges that authors have claimed.\nIn other journals (e.g., PLOS ONE, PeerJ, Collabra), you may be looking for interesting articles with the words Data Availability Statement, Data Accessibility Statement, Supplementary data or Supplementary materials in the article webpage somewhere. Journals like PeerJ or Collabra, in particular, make it easy to locate data associated with published articles on their web pages.\nIn Collabra, you can find published articles through the journal webpage (here). If you click on the title of any article, and look at the article webpage, then on the left of the article text, you can see an index of article contents and that index lists the Data Availability Statement. Click on that and you are often taken to a link to a data repository.\n\n\n\n8.2.1.2 Access\nIf you have located an interesting article with evidence (an open data badge or a data accessibility statement) that the authors have shared their data, you need to check that you can access the data. Most of the time, now, you are looking for a link you can use to go directly to the shared data. The link is often presented as a hyperlink on a webpage, associated with Digital Object Identifiers (DOIs) or Universal resource locators (URLs). Or, increasingly, you are looking for a link to a data repository on a site like the Open Science Framework (OSF).\n\n\n\n\n\n\nTip\n\n\n\nThe task here is:\n\nAccess the data associated with the article you have found.\n\n\n\nHere are some recent examples from my work that you can check, to give you a sense of where or how to find the accessible link to the shared data.\n\nRicketts, J., Dawson, N., & Davies, R. (2021). The hidden depths of new word knowledge: Using graded measures of orthographic and semantic learning to measure vocabulary acquisition. Learning and Instruction, 74, 101468. https://doi.org/10.1016/j.learninstruc.2021.101468\n\n\nRodríguez-Ferreiro, J., Aguilera, M., & Davies, R. (2020). Semantic priming and schizotypal personality: Reassessing the link between thought disorder and enhanced spreading of semantic activation. PeerJ, 8, e9511. https://doi.org/10.7717/peerj.9511\n\nThese are both open access articles.\nIf you look at the webpage for, Rodríguez-Ferreiro et al. (2020), (here)), you can do a search in the article text for the keyword OSF (on the article webpage, use keys CMD-F plus OSF). You are checking to see if you can click on the link and and if clicking on the link takes you to a repository listing the data for the article. The Rodríguez-Ferreiro et al. (2020) article is associated with a data plus analysis code repository (OSF))\nNotice that on the repository webpage, you can see a description of the project plus .pdf files and a folder data-set and Code. If you can click through to the folders, and download the datafiles, you have accessed the data successfully.\nI have guided you, here, through to the Rodríguez-Ferreiro et al. (2020) data repository, can you find the data for the Ricketts et al. (2021) repository?\n\n\n8.2.1.3 Check\nIf you have located an interesting article with data that you can access, and if you have read the introductory notes (see Section 9.2.6.3), then you will next need to make sure that you can use the data.\n\n\n\n\n\n\nTip\n\n\n\nThe task here is:\n\nCheck the data and the data documentation to make sure you can understand what you have got and whether you can use it.\n\n\n\nWhat make data usable are:\n\nInformation in the article, or in the data repository documentation, on the study design and data collection methods: you need to be able to understand where the data came from, how they were collected, and why.\nClear data documentation: you need to find information on the variables, the observations, the scoring, the coding, and whether and how the data were processed to get them from raw data state to the data ready for analysis.\n\nData documentation is often presented as a note or a wiki page or a miniature paper and may be called a codebook, data dictionary, guide to materials or something similar. You will need to check that you can find information on (examples shown are from the Rodríguez-Ferreiro et al. (2020) OSF guide to materials):\n\nwhat the data files are called e.g. PrimDir-111019.csv;\nhow the named data files correspond to the studies presented in the report;\nwhat the data file columns are called and what variables the column data represent e.g. relation, coding for prime-target relatedness condition ...;\nhow scores or responses in columns were collected or calculated e.g. age, giving the age in years ...;\nhow coding was done, if coding was used e.g. biling, giving the bilingualism status;\nwhether data were processed, how missing values were coded, whether participants or observations were excluded before analysis e.g. Missing values in the rt column ... coded as NA\n\n\n\n\n\n\n\nWarning\n\n\n\nIf these information are not presented, or are not clear: walk away.\n\n\n\n\n\n8.2.2 Plan the analysis you want to do\nAfter you have found an interesting article, and have confirmed that you can use the associated data, you will need to plan what analysis you want to do.\n\n\n\n\n\n\nTip\n\n\n\nThe task here is:\n\nIdentify and understand the analysis in the article.\nWork out what analysis you want to do.\n\n\n\nStudents have taken a variety of approaches to the assignment.\n\nSome students choose to complete a reanalysis of the data, in an attempt to reproduce the results presented in the article (see 9.2.5.1).\nSome students choose to complete an alternate analysis of the data, varying elements of the analysis (see the discussion in 9.2.4).\n\nEither way, you will want to first make sure you can identify exactly what the authors of the original study did, how they did it, and why they did it.\nYou can process the key article information efficiently using the QALMRI method we discussed in the class on graduate writing skills (Brosowsky et al., n.d.; Kosslyn & Rosenberg, 2005). You are first aiming to locate information on the broad and the specific question the study addresses, the methods the study authors used to collect data, the results they report, and the conclusions they present given the results. Can you find these bits of information?\n\n8.2.2.1 Completing a reanalysis of the data\nAre you interested in attempting a methods reproducibility test?\nFollowing Hardwicke and colleagues (Hardwicke et al., n.d.; Hardwicke et al., 2018), it would be sensible to focus on identifying the primary or substantive result for a study in an article.\n\nThe result is substantive if the researchers state that it is (e.g., “Our critical analysis is…”), if it is emphasized in the abstract, or if it is presented in a table or figure.\n\nAs we discussed in the class on graduate writing skills, the article authors should signal what they consider to be the primary result for a study by telling you that a result is critical or key or that a result is the or an answer to their research question.\n\n\n\n\n\n\nTip\n\n\n\n\nAn article may present multiple studies: focus on one.\nThe results section of an article, for a study, may list multiple results: identify the primary or substantive result.\n\n\n\nYou will want to identify a result that is both substantive and straightforward (Hardwicke et al., n.d.; Hardwicke et al., 2018).\n\nThe result is Straightforward if the outcome could be calculated using the kind of test you have been learning about or will learn about (e.g., t-test, correlation, the linear model)\n\nPsychological science researchers use a variety of data analysis methods and not all the analyses that you read about will be analyses done using methods that you know about. The use of the methods we teach — t-test, correlation, and the linear model — are very very common; that is why we teach them. But you may also see reports of analyses done using methods like ANOVA, and multilevel or (increasingly) linear mixed-effects models (Meteyard & Davies, 2020).\nIn research on the reproducibility of results in the literature (see Section 9.2.6.3), the researchers attempting to reproduce results often focused on answering the research question the original authors stated using the data the original authors shared. This does not mean that they always tried to exactly reproduce an analysis or an analysis result. Sometimes, that was not possible.\nSometimes, you will encounter an article and a data-set you are interested in but the analysis presented in the article looks a bit complicated, or more complex than the methods you have learned would allow you to do. In this situation, don’t give up.\nWhat you can do – maybe with our advice – is identify a part of the primary result that you can try to reproduce. For example, what if the original study authors report a linear mixed-effects analysis of the effects of both prime relatedness and schizotypy score on response reaction time (Rodríguez-Ferreiro et al., 2020)? Maybe you have not learned about mixed-effects models, or you have not learned about analysing the effects of two variables but you have (you will) learn about analysing the effect of one variable using the linear model method: OK then, do an analysis of the shared data using the method you know.\nYou may be helped, here, by knowing about two good-enough (mostly true) insights from statistical analysis:\n\nMany of the common analysis methods you see used in psychological science can be coded as a linear model.\nMore advanced common analysis methods — (Generalized) Linear Mixed-effects Models (GLMMs) — can be understood as more sophisticated versions of the linear model. (Conversely, the linear model can be understood as an approximation of a GLMM.)\n\nThere is a nice discussion of the idea that common statistical tests are linear models here.\n\n\n\n\n\n\nTip\n\n\n\n\nIdentify the analysis method used to get the result you are interested in.\nIf it is complex or unfamiliar, discuss whether a simpler method can be used.\nIf the result is complex, discuss whether you can attempt to reproduce a part or a simpler result.\n\n\n\n\n\n8.2.2.2 Completing an alternate analysis of the data\nAre you interested in attempting a different analysis than the analysis you see in the journal article?\nIt can be interesting and important work to complete a simpler analysis of shared data. Sometimes, we learn that a simpler analysis provides a good account of the behaviour we observe, perhaps as good an account as that produced using other, more complex, analyses. This can happen if, for example, our theory predicts that two effects should work together but an analysis shows that we can explain behaviour in an account in which the two effects are independent. For example, Ricketts et al. (2021) predicted that children should learn words more effectively if they were shown the spellings of the words and they were told they would be helped by seeing the spelling but, in our data, we found that just seeing the spellings was enough to explain the learning we observed.\nIn completing analyses that vary from original analyses, we are engaging in the kind of work people do when they do multiverse analyses or robustness checks (see Section 9.2.4).\n\n\n\n\n\n\nTip\n\n\n\nIn planning an alternate or multiverse analysis, do not suppose that you need to do multiple analyses: you do not.\n\n\nIn planning an alternate or multiverse analysis, you will want to begin by critically evaluating the analysis you see described in the published article. I talk about how to do this, next.\nBefore we go on, note that I previously discussed an example of how to critically evaluate the results of published research in the context of Rodríguez-Ferreiro et al. (2020). Take a look at the Introduction of that article. There, we summarised the analyses researchers did previously and used the information about the analyses to explain inconsistencies in the research literature. We found limitations in the analyses that people did that had (negative) consequences for the strength of the conclusions we can take from the data.\n\n8.2.2.2.1 Critically evaluate the analysis description\nIf you revisit our discussion of multiverse analyses, you will see that we discussed two things: (1.) analyses of the impact on results of varying how you construct data-sets for analysis (Section 9.2.4.2) and (2.) analyses of the impact on results of varying what analysis method you use, or how you use the method (see Section 9.2.4.3). These are both good ways to approach thinking about the description of the analysis you see in a published article.\nAs we noted in Section 9.2.4.2, you almost always have to process the data you collect (in an experiment or a survey) before you can analyse the data. Often, this means you need to code for responses to survey questions e.g. asking people to self-report their gender, or you need to identify and code for people making errors when they try to do the experimental task you set them, or you need to process the data to exclude participants who took too long to do the task (if taking too long is a problem). Not all of these processing steps will have an impact on the results but some might. This is why you can sometimes do useful and sometimes original research work in reanalysing previously published data.\nYou can begin your analysis planning work by first identifying exactly what data processing the original study authors did then identifying what different data processing they could have done. Remember the research we discussed in relation to reproducibility studies, you need to be prepared for the possibility that it is challenging to identify what researchers did to process their data for analysis Section 9.2.6.3.1. To identify the information you need, look for keywords like code, exclude, process, tidy, transform in the text of the article, or look for words like this in the documentation you find in the data repository.\nWhen you have identified this information, you can then consider three questions:\n\nWhat data processing steps were completed before analysis?\nWhat were the reasons given explaining why these processing steps were completed?\nWhat could happen to the results if different choices were made?\n\nWorking through these questions can then get you to a good plan for an analysis of the data. For example, a simple but useful analysis you can do is to check what happens to the results if you do an analysis with data from all the participants tested, if participants are excluded (for some reason) in the data processing step. Obviously, if the original study authors only share processed data (after exclusions), you cannot do this kind of work. Another simple but useful analysis you can do is to check what happens to the results if you change the coding of variables. Sometimes different coding of categorical variables (e.g., ethnicity) are reasonable. For example, you can ask: what happens if you analyse the impact of the variable given a different coding? (In case you are reading these notes and thinking about recoding a factor, there are some useful functions you can use; read about them here.)\n\n\n\n\n\n\nTip\n\n\n\n\nDo you want to check the impact of varying data processing choices: check, do you need and have access to the raw data? can you see how to recode variables?\n\n\n\nAs we noted in Section 9.2.4.3, when we consider how to answer a research question with a data-set, it is often possible to imagine multiple different analysis methods: reasonable alternatives. Most often, this is most clearly apparent when we are looking at an observational data-set or data collected given a cross-sectional study design.\nIn cross-sectional or observational studies, we typically are not manipulating experimental conditions, and we are often analysed data using some kind of linear model. We often collect data or have access to data on a number of different variables relevant to our interests. For example, in studies I have done on how people read (R. Davies et al., 2013; R. A. I. Davies et al., 2017), we wanted to know what factors would predict or influence how people do basic reading tasks like reading aloud. We collected information on many different kinds of word properties and on the attributes of the participants we tested. (Note: the papers are associated with data repositories in Supplementary Materials.) It is often an open question which variables should be included in a prediction model of the observed outcome (reading response reaction times). Therefore, if you are interested in a study like this, and can access usable data from the study, it will typically be true that you are able to sensibly motivate a different analysis of the study data using a different choice of variables.\nAs discussed in a number of interesting analyses over the years (e.g., Patel et al., 2015), researchers may be interested in the specific impact of one particular predictor variable (e.g., we may be interested in whether it is easier to read words we learned early in life), but will need to include in their analysis that variable plus other variables known to affect the outcome. In that situation, the effect of the variable of interest may appear to be different depending on what other variables are also analysed. This makes it interesting and useful to check the impact of different analysis choices.\nWe will look at data like these, for analyses involving the linear model, in our classes on this method.\n\n\n\n\n\n\nTip\n\n\n\n\nDo you want to check the impact of different analysis choices: check, do you need and have access to a choice of variables?\nCan you think of some reasons to justify using a different choice of variables in your analysis.\n\n\n\n\n\n\n\n8.2.3 Summary: working with data associated with a published analysis\nHere’s a quick summary of the advice we have discussed so far.\n\nAt the start of your work, you will need to (1.) locate then (2.) access data for analysis, and then you will need to (3.) check that the data are usable.\nOnce you have confirmed you have found interesting data you can use, you should plan your analysis.\nStudents do a variety of kinds of analysis. Whatever your interest, you first will want to first make sure you can identify exactly what the authors of the original study did, how they did it, and why they did it.\nIf you are interested in completing a reanalysis, attempting a methods reproducibility test (can you repeat a result, given shared data?) you will perhaps benefit from focusing on a result that is both substantive and straightforward.\nIf you are interested in doing an alternate or multiverse analysis, you can critically evaluate the data processing and the data analysis choices that the original study authors made. You can consider whether other choices would be appropriate, and might sensibly motivate a (limited) investigation of the impact of different analysis choices on the results.\n\nWhat if you access interesting data that were shared but that are not associated with a published analysis? We talk about that situation, next.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>How you can do the analysis work</span>"
    ]
  },
  {
    "objectID": "how.html#sec-noanalysis",
    "href": "how.html#sec-noanalysis",
    "title": "8  How you can do the analysis work",
    "section": "8.3 Working with data that are not associated with a published analysis",
    "text": "8.3 Working with data that are not associated with a published analysis\nA number of data-sets have been published online with information about the data but with no analysis. You can look for data that may be interest you in a number of different places, now, but I would focus on one. I talk about that next. Then I offer some guidance on how you might approach analyzing such data Section 8.3.2.\n\n8.3.1 Looking for open data\nWicherts and colleagues set up the Journal of Open Psychology Data (JOPD) to make it easier for Psychologists to share experimental data. A link to the journal webpage is here) Usually, a data paper reports a study and provides a link to a downloadable data-set.\nSome data-sets that I have looked at in JOPD and other places include the following. I identify these examples because they present interesting, rich, and readily accessible data-sets that could be used in a variety of different kinds of analyses.\n\n8.3.1.1 Wicherts intelligence and personality data\nWicherts did what he recommended and put a large data-set online here\nYou can analyse these data in a number of different interesting ways. You can explore relationships between gender, intelligence and personality differences.\nThe data file and an explanatory document are located at the end of the article. Read the article, it’s worth your time. Wicherts reports:\n\nThe file includes data from our freshman-testing program called “Testweek” (Busato et al., 2000, Smits et al., 2011 and Wicherts and Vorst, 2010) in which 537 students (age: M = 21.0, SD = 4.3) took the Advanced Progressive Matrices ( Raven, Court, & Raven, 1996), a test of Arithmetic, a Number Series test, a Hidden Figures Test, a test of Vocabulary, a test of Verbal Analogies, and a Logical Reasoning test (Elshout, 1976).\nAlso included are data from a Dutch big five personality inventory (Elshout & Akkerman, 1975), the NEO-PI-R (Hoekstra, Ormel, & Fruyt, 1996), scales of social desirability and impression management (based on work by Paulhus, 1984 and Wicherts, 2002), sex of the participants, and grade point averages of the freshmen’s first trimester that may act as outcome variable.\n\n\n\n8.3.1.2 Smits personality data\nSmits and colleagues (including Wicherts) put an even larger data-set online at the Journal of Open Psychology Data here)\nYou will need to register to be able to download the data but the process is simple.\nThe Smits data-set includes Big-5 personality scores for several thousand individuals recorded over a series of years. You can analyse these data in interesting ways including examining changes in personality scores among students over different years.\n\n\n8.3.1.3 Embodied terror management\nTjew A Sin and colleagues shared a data-set at the Journal of Open Psychology Data on an interesting study they did to test the idea that interpersonal touch or simulated interpersonal touch can relieve existential concerns (fear of death) among individuals with low self-esteem. The data can be found here)\nThe Tjew A Sin can be downloaded from a link to a repository location, given at the end of the article. You will likely need to register to download the data. Note that the spreadsheets holding the study data include 999 values to code for missing data. Note also that the data spreadsheets include (in different columns) scores per participant for various measures e.g. mortality anxiety or self-esteem. The measures are explained in the paper. To use the data, you will need to work out the simple process of how to sum the scores across items to get e.g. a measure of self-esteem for each person.\n\n\n8.3.1.4 Demographic influences on disgust\nBerger and Anaki shared data on the disgust sensitivity of a large sample of individuals. The data are from the administration of the Disgust Scale to a set of Hebrew speakers. They can be found here)\nThe experimenters collected data on participants’ characteristics so that analyses of the way in which sensitivity varies in relation to demographic attributes is possible. You will see that the disgust scale is explained in the paper. The different disgust scores, for each item in the disgust scale, can be found in different columns. The disgust scores, for person, are calculated overall as values: Mean_general_ds, Mean_core, Mean_Animal_reminder, Mean_Contamination\nWhen you download the data-set, you may need to change the file name — adding a suffix: .txt (for the tab delimited file), to be opened in Excel, or .sav (for the SPSS data file), to be opened in SPSS — to the file name to allow you to open it in the appropriate application.\n\n\n\n8.3.2 Thinking about analyses of open data\nThe availability of rich, curated, clearly usable data-sets with many variables can make it challenging to decide what to do.\nI would advise beginning with an exploratory analysis of the data you have accessed. You will want to begin by using the data visualization skills we have taught you to examine:\n\nThe distributions of the variables that interest you using histograms, density plots or bar charts.\nThe potential relationship between variables using scatterplots.\n\nIn such Exploratory Data analyses, you are interested in what the data visualization tells you about the nature of the data-set you have accessed. The papers associated with the data-sets can sometimes offer only outline information: how the data were collected, coded, and processed. You may need to satisfy yourself that there is nothing odd or surprising about the distributions of scores. This stage can help you to identify problems like survey responses with implausible scores.\nThe work you do in exploring, and summarizing, the data variables that interest you will often constitute a substantial element of the work you can do and present for your report. You may discuss, for advice, what parts of this work will be interesting or useful to present.\nThen, our advice is simple.\n\n\n\n\n\n\nTip\n\n\n\n\nWhen working with open data-sets, consider keeping the analysis simple.\n\n\n\nNote that simple is relative. Do what interests you. Work with the methods you have learned or will learn (the linear model).\nIn practice, you will find that part of the challenge is located not in using the data or in running an analysis like a linear model, it is in (1.) justifying or motivating the analysis and (2.) explaining the implications of your findings.\nWorking on the thinking you must develop to motivate an analysis or to explain implications requires you to do some (limited) reading of relevant research. (Relevant sources will be cited in data papers, as part of their outline of the background for their data collection.) If you consider the advice we discussed in the graduate class on developing writing skills, you will see that there I talked about how you might extract data from a set of relevant sources (papers) to get an understanding of the questions people ask, the assumptions they make. That is the kind of process you can follow to develop your thinking around the analysis you will do. What you are looking for is information you can use so that you can say something brief about, for example, why it might be interesting to analyse, say, whether personality (measured using the Big-5) varies given differences in gender or differences between population cohorts. The reading and the conceptual development should be fairly limited, not extensive, but should be sufficient that you can write something sensible when you introduce and then when you discuss your analysis results.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>How you can do the analysis work</span>"
    ]
  },
  {
    "objectID": "how.html#sec-how-summary",
    "href": "how.html#sec-how-summary",
    "title": "8  How you can do the analysis work",
    "section": "8.4 Summary: how",
    "text": "8.4 Summary: how\nIn this chapter, I have outlined some advice on how you might approach the task of locating, accessing, and analyzing previously collected data.\n\n\n\n\n\n\nTip\n\n\n\nThe main advice is to think about your workflow in stages, then progress through the work one step at a time.\n\n\nYou will need to begin by assuring yourself that you can find a data-set that interests you, and that you can access and use the data. The usability of data will require clear, understandable, descriptions in the published article (if any) about the research question and hypothesis, the study design, the data collection methods, the data processing steps, and the data analysis (if any). Sometimes, useful information about data processing and data analysis can be found in detail in repository documentation (e.g., in guides to materials) but only referenced in the text of the article.\nIf you know you can locate, access and have checked data as usable, you will want to think about what analysis you want to do the data. The approach you take depending on what aims you would like to pursue.\nIf you are interested in attempting a methods reproducibility test (i.e. checking if you can repeat presented results, given shared data), then you will first need to identify a substantive and straightforward result to try to reproduce. If you identify a primary result to examine, you will want to check that you can work with the data that have been shared, and then that you can use the analysis methods you have learned to reproduce some or all of the result that interests you.\nIf you are interested in doing an alternate or a different analysis (from what may be presented), you may need to consider the information you can locate on data processing and on data analysis choices. Did the original study authors process the data before sharing it, how? are the raw data available? What analyses did the authors do and why? When you consider this information, you may critically evaluate the choices made. In the context of this critical evaluation, you may find good reasons to justify doing a different analysis, whether to examine the impact of making different data processing choices, or to examine the impact of using a different analysis method, or of applying the same method differently (e.g., by including different variables).\nIn considering an analysis of data shared without a published set of results, you may want to keep your approach simple. Focus on what analysis you can do using the methods you have learned. And think about the understanding you will need to develop, to justify the analysis you do, and to make sense, in the discussion of your report of the analysis results you will present.\nIt is always a good idea to explore your data using visualization techniques throughout your workflow.\n\n\n\n\n\n\nTip\n\n\n\n\nYou can always get advice, do not hesitate to ask.\nWe are happy to discuss your thinking, especially in class.\n\n\n\n\n\n\n\nBrosowsky, N., Parshina, O., Locicero, A., & Crump, M. (n.d.). Teaching undergraduate students to read empirical articles: An evaluation and revision of the QALMRI method. https://doi.org/10.31234/osf.io/p39sc\n\n\nDavies, R. A. I., Birchenough, J. M. H., Arnell, R., Grimmond, D., & Houlson, S. (2017). Reading through the life span: Individual differences in psycholinguistic effects. Journal of Experimental Psychology: Learning Memory and Cognition, 43(8). https://doi.org/10.1037/xlm0000366\n\n\nDavies, R., Barbón, A., & Cuetos, F. (2013). Lexical and semantic age-of-acquisition effects on word naming in spanish. Memory and Cognition, 41(2), 297–311. https://doi.org/10.3758/s13421-012-0263-8\n\n\nHardwicke, T. E., Bohn, M., MacDonald, K., Hembacher, E., Nuijten, M. B., Peloquin, B. N., deMayo, B. E., Long, B., Yoon, E. J., & Frank, M. C. (n.d.). Analytic reproducibility in articles receiving open data badges at the journal psychological science: An observational study. Royal Society Open Science, 8(1), 201494. https://doi.org/10.1098/rsos.201494\n\n\nHardwicke, T. E., Mathur, M. B., MacDonald, K., Nilsonne, G., Banks, G. C., Kidwell, M. C., Hofelich Mohr, A., Clayton, E., Yoon, E. J., Henry Tessler, M., Lenne, R. L., Altman, S., Long, B., & Frank, M. C. (2018). Data availability, reusability, and analytic reproducibility: evaluating the impact of a mandatory open data policy at the journal Cognition. Royal Society Open Science, 5(8), 180448. https://doi.org/10.1098/rsos.180448\n\n\nKosslyn, S. M., & Rosenberg, R. S. (2005). Fundamentals of psychology: The brain, the person, the world, 2nd ed. Pearson Education New Zealand.\n\n\nMeteyard, L., & Davies, R. A. I. (2020). Best practice guidance for linear mixed-effects models in psychological science. Journal of Memory and Language, 112. https://doi.org/10.1016/j.jml.2020.104092\n\n\nMinocher, R., Atmaca, S., Bavero, C., McElreath, R., & Beheim, B. (n.d.). Estimating the reproducibility of social learning research published between 1955 and 2018. Royal Society Open Science, 8(9), 210450. https://doi.org/10.1098/rsos.210450\n\n\nPatel, C. J., Burford, B., & Ioannidis, J. P. A. (2015). Assessment of vibration of effects due to model specification can demonstrate the instability of observational associations. Journal of Clinical Epidemiology, 68(9), 1046–1058. https://doi.org/10.1016/j.jclinepi.2015.05.029\n\n\nRicketts, J., Dawson, N., & Davies, R. (2021). The hidden depths of new word knowledge: Using graded measures of orthographic and semantic learning to measure vocabulary acquisition. Learning and Instruction, 74, 101468. https://doi.org/10.1016/j.learninstruc.2021.101468\n\n\nRodríguez-Ferreiro, J., Aguilera, M., & Davies, R. (2020). Semantic priming and schizotypal personality: reassessing the link between thought disorder and enhanced spreading of semantic activation. PeerJ, 8, e9511. https://doi.org/10.7717/peerj.9511",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>How you can do the analysis work</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "9  Why we are asking you to do this",
    "section": "",
    "text": "9.1 The key ideas\nThere are two ideas motivating our approach. It will be helpful to you if I sketch them out early, here. We can demonstrate the usefulness of these ideas as we progress through our work.\nThe first key idea is expressed clearly in sociological discussions of science. This is that there is a difference between science “…being done, science in the making, and science already done, a finished product …” [Bourdieu (2004); p.2]. The awareness we want to develop is that there are two things: there is the story that may be presented in a textbook or in a lecture about scientific work or scientific claims; and there is the work we do in practice, as we develop graduate skills, and as we exercise those skills professionally in the workplace.\nThe second key idea connects to the first. This idea is that reported analyses are not necessary or sufficient to the data or the question. What does this mean? It means that the same data can reasonably be analysed in different ways. There is no necessary way to analyse some data though there may be conventions or normal practices (Kuhn, 1970). It means that it is unlikely that any one analysis will do all the work that could be done (a sufficiency) to get you from your data to useful or reasonable answers to your questions.\nThese ideas may be unsettling but they are realistic. Stating them will better prepare you for professional work. In the workplace, the accuracy of these ideas will emerge when you see how a team in any sector (health, marketing …) gets from its data to its product. If we talk about the ideas now, we can get you ready for dealing with the practical and the ethical concerns you will confront when that happens.\nWe will begin by discussing psychological research, and research about psychological research, to answer the question: Why: what is the motivation for the assignment? We will then move to answering the What question Chapter 7 and the How question Chapter 8.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Why we are asking you to do this</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-motivation-for-assignment",
    "href": "intro.html#sec-motivation-for-assignment",
    "title": "9  Why we are asking you to do this",
    "section": "9.2 Why: what is the motivation for the assignment?",
    "text": "9.2 Why: what is the motivation for the assignment?\n\n9.2.1 The wider context: crisis and revolution\nWe are here because we are interested in humans and human behaviour, and because we are interested in scientific methods of making sense of these things. Some of us are aware that science (including psychological science) has undergone a rolling series of crises: the replicability or replication crisis (Pashler & Harris, 2012; Pashler & Wagenmakers, 2012); the statistical crisis (A. Gelman & Loken, 2014b); and the generalizability crisis (Yarkoni, 2022). And that science is undergoing a response to these crises, evidenced in the advocacy of pre-registration (Nosek et al., 2018, 2019), and of registered reports (Nosek & Lakens, 2014), the use of open science badges (e.g., for the journal Psychological Science), the completion of large-scale replication studies (Aarts et al., 2015), and the identification of open science principles (Munafò et al., 2017). We may usefully refer, collectively, to the crises and the responses, as the credibility revolution (Vazire, 2018)\nWe could teach a course on this (in Lancaster, we do) but, here, I invite you to follow the references if you are interested. Before going on, I want to call your attention to the fact that important elements of the hard work in trying to make science work better has been led by PhD students and by junior researchers (e.g., Herndon et al., 2014). Graduate students may, at first, assume that the fact that a research article has been published in a journal means the findings that are reported must be true. Most of the time, some educated skepticism is more appropriate. An important driver of the realization that there are problems evident in the literature, and that there are changes we can make to improve practice, comes from independent post-publication review work exposing the problems in published work (see, e.g., this account by Andrew Gelman)\n\n\n\n\n\n\nTip\n\n\n\n\nAllow yourself to feel skeptical about the reports you read then work with the motivation this feeling provides.\n\n\n\nIn brief, then, most practicing scientists now understand or should understand that many of the claims we encounter in the published scientific literature are unlikely to be supported by the evidence (Ioannidis, 2005), whether we are looking at the evidence of the results in the reports themselves, or evidence in later attempts to find the same results (e.g., Aarts et al., 2015). We suspect that this may result from a number of causes. We understand that researchers may engage in questionable research practices (John et al., 2012). We understand that researchers may exploit the potential for flexibility in doing and reporting analyses (Simmons et al., 2011a). We understand that there are problems in how psychologists use or talk about the measurement of psychological constructs (Flake & Fried, 2020). We understand that there are problems in how psychologists sample people for their studies, both in where we recruit (Bornstein et al., 2013; Henrich et al., 2010; Wild et al., 2022), and in how many we recruit (Button et al., 2013; Cohen, 1962; Sedlmeier & Gigerenzer, 1989; Vankov et al., 2014). We understand that there are problems in how psychologists specify or think about their hypotheses or predictions (Meehl, 1967; Scheel, 2022). And we understand that there are problems in how scientists do, or rather do not, comply with good practice recommendations designed to fix these problems (discussed further in the following).\nThis discussion could (again) be unsettling. This list of problems could make you angry or sad. I, like others, think it is exciting. It is exciting because these problems have probably existed for a long time (e.g., Cohen, 1962; Meehl, 1967) but now, having identified the problems, we can hope to do something about it. It is exciting because if you care about people, the study of people, or the applications in clinical, education and other domains of the results of the study of people, then you might hope to see better, more useful, science in the future (Vazire, 2018).\nAs someone who teaches graduate and undergraduate students, I want to help you to be the change you want to see in the world 1. We cannot solve every problem but we can try to do better those things that are within our reach. I am going to end this introduction with a brief discussion of some ideas we can use to guide our better practices.\n\n\n9.2.2 The specific context: what we need to look at, conceptually and practically\nIn this course, for this assignment, we are going to focus on:\n\nmultiverse analyses\nkinds of reproducibility\nthe current state of the match between open science ideas and practices\n\nIn the classes on the linear model, we will discuss:\n\nthe links between theory, prediction and analysis\npsychological measurement\nsamples\nvariation in results\n\n\n\n9.2.3 Multiverse analyses: multi- what?\n\n9.2.3.1 A first useful metaphor: the pipeline\nI am going to link this discussion to a metaphor (see Figure Figure 9.1) or a description you will find useful: the data analysis pipeline or workflow.\n\n\n\n\n\n\n\n\nQ\n\n\ncluster_R\n\n\n\n\nnd_1\n\nGet raw data\n\n\n\nnd_2\n\nTidy data\n\n\n\nnd_1-&gt;nd_2\n\n\n\n\n\nnd_3_l\n\nVisualize\n\n\n\nnd_2-&gt;nd_3_l\n\n\n\n\n\nnd_3\n\nanalyse\n\n\n\nnd_2-&gt;nd_3\n\n\n\n\n\nnd_3_r\n\nExplore\n\n\n\nnd_2-&gt;nd_3_r\n\n\n\n\n\nnd_3_a\n\nAssumptions\n\n\n\nnd_3_a-&gt;nd_3_l\n\n\n\n\n\nnd_3_a-&gt;nd_3\n\n\n\n\n\nnd_3_a-&gt;nd_3_r\n\n\n\n\n\nnd_3_l-&gt;nd_3\n\n\n\n\nnd_4\n\nPresent\n\n\n\nnd_3_l-&gt;nd_4\n\n\n\n\n\nnd_3-&gt;nd_4\n\n\n\n\n\n\n\n\nFigure 9.1: The data analysis pipeline or workflow\n\n\n\n\n\nThis metaphor or way of thinking is very common (take a look at the diagram in Wickham and Grolemund’s 2017 book “R for Data Science) and you may see the words “data pipeline” used in job descriptions, or you may benefit from saying, in a job application, something like: I am skilled in designing and implementing each stage of the quantitative data analysis pipeline, from data tidying to results presentation. I say this because scientists I have mentored got their jobs because they can do these things – and successfully explained that they can do these things – in sectors like educational testing, behavioural analysis, or public policy research.\nThe reason this metaphor is useful is that it helps us to organize our thinking, and to manage what we do when we do data analysis, we:\n\nget some data;\nprocess or tidy the data;\nexplore, visualize, and analyse the data;\npresent or report our findings.\n\nWe introduce the idea that your analysis work will flow through the stages of a pipeline from getting the data to presenting your findings because, next, we will examine how pipelines can multiply.\n\n\n\n\n\n\nTip\n\n\n\n\nAs you practice your data analysis work, try to identify the elements and the order of your work, as the parts of a workflow.\n\n\n\n\n\n9.2.3.2 A second useful metaphor: the garden of forking paths\nWhat researchers have come to realize: because we started looking … The open secret that has been well kept (Bourdieu, 2004): because everybody who does science knows about it, yet we may not teach it; and because we do not write textbooks revealing it … Is that at each stage in the analysis workflow, we can and do make choices where multiple alternative choices are possible. A. Gelman & Loken (2014a) capture this insight as the “garden of forking paths”2 (see Figure 9.2).\nThe general idea is that it is possible to have multiple potential different paths from the data to the results. The results will vary, depending on the path we take. In an analysis, we could take multiple different paths simply because at point A we decide to do B1, B2 or B3, maybe we choose B1, and then at point B1, we may decide to do C1, C2 or C3. Here, maybe we have our raw data at point A. Maybe we could do one of two different things when we tidy the data: action B1 or B2. Then, when we have our tidy data, maybe we can choose to do our analysis in one of six ways. Where we are at each step depends on the choices we made at the previous steps.\n\n\n\n\n\n\n\n\nD\n\n\n\nA\n\nA\n\n\n\nB1\n\nB1\n\n\n\nA-&gt;B1\n\n\n\n\n\nB2\n\nB2\n\n\n\nA-&gt;B2\n\n\n\n\n\nC1\n\nC1\n\n\n\nB1-&gt;C1\n\n\n\n\n\nC2\n\nC2\n\n\n\nB1-&gt;C2\n\n\n\n\n\nC3\n\nC3\n\n\n\nB1-&gt;C3\n\n\n\n\n\nC4\n\nC4\n\n\n\nB2-&gt;C4\n\n\n\n\n\nC5\n\nC5\n\n\n\nB2-&gt;C5\n\n\n\n\n\nC6\n\nC6\n\n\n\nB2-&gt;C6\n\n\n\n\n\n\n\n\nFigure 9.2: Forking paths in data analysis\n\n\n\n\n\nIn the end, it may appear to us that we took one path or that only one path was possible. When we report our analysis, in a dissertation or in a published journal article, we may report the analysis as if only one analysis path had been considered. But, critically, our findings may depend on the choices we made and this variation in results may be hidden from view.\nI am talking about forking paths because the multiplicity of paths has consequences, and we discuss these next.\n\n\n\n\n\n\nTip\n\n\n\n\nIt is about here, I hope, that you can start to see why it would makes sense to access data from a published study and to examine if you can get the same results as the study authors.\n\n\n\n\n\n\n9.2.4 Multiverse analyses\nI am going to discuss, now, what are commonly called multiverse analyses. Psychologists use this term, having been introduced to it in an influential paper by Steegen et al. (2016a), but it comes from theoretical physics (take a look at wikipedia).\nI explain this because I do not want you to worry. The ideas themselves are within your grasp whatever your background in psychology or elsewhere. It is the implications for our data analysis practices that are challenging. They are challenging because what we discuss should increase your skepticism about the results you encounter in published papers. And they are challenging because they reveal your freedom to question whether published authors could have done their analysis in a different way.\nWe are going to look at:\n\ndata-set construction\nanalysis choices\n\n\n9.2.4.1 The link between the credibility revolution and the multiverse\nIn first discussing the wider context (of crisis and revolution), then discussing the specific context (of multiverses and, in the following, of reproducibility), I should be clear about the link between the two things. The finding that some results may not be supported by the evidence is probably due to a mix of causes. But one of those causes will be the combination of uncertainty over data processing or the uncertainty over analysis methods revealed in multiverse analyses, as we see next, combined with the limitations of data and code sharing, and the incompleteness of results reporting (as we see later).\n\n\n9.2.4.2 The data multiverse\nWhen you collect or access data for a research study, the complete raw data-set you receive is almost never the complete data-set you analyse or whose analysis you report. This is not a story about deliberately cheating. It is a story about the normal practice of science (Kuhn, 1970).\nPicture some common scenarios. You did a survey, you got responses from a 100 participants on 10 questions, and you asked people to report their education, ethnicity and gender. You did an experimental study, you tested two groups of 50 people each in 100 trials (imagine a common task like the Stroop test), and you observed the accuracy and the timing of their responses. You tested 100 children, 20 children in each of five different schools, on a range of educational ability measures.\nIn these scenarios, the psychologist or the analyst of behavioural data must process their data. In doing so, you will ask yourself a series of questions like:\n\nhow do we code for gender, ethnicity, education?\nwhat do we about reaction times that are very short, e.g., \\(RT &lt; 200ms\\) or very long, e.g., \\(RT &gt; 1500ms\\))?\nif we present multiple questions measuring broadly the same thing (e.g. how confident are you that you understand what you have read? how easy did you find what you read?) how do we summarize the scores on those questions? do we combine scores?\nwhat do we do about people who may not appear to have understood the task instructions?\n\nTypically, the answers to these questions will be given to you by your supervisor, a colleague or a textbook example. For example, we might say:\n\n“We excluded all reaction times greater than 1500ms before analysis.”\n\nTypically, the explanation for these answers are rarely explained. We might say:\n\n“Consistent with common practice in this field, we excluded all reaction times greater than 1500ms before analysis.”\n\nBut the reader of a journal article typically will not see an explanation for why, as in the example, we exclude reaction times greater than 1500ms and not 2000ms or 3000ms, etc. We typically do not see an explanation for why we exclude all reaction times greater than 1500ms but other researchers exclude all reaction times greater than 2000ms. (I do not pick this example at random: there are serious concerns about the impact on analyses of exclusions like this (Ulrich & Miller, 1994).)\nWhat Steegen et al. (2016a) showed is that a data-set can be processed for analysis in multiple different ways, with a number of reasonable alternate choices that can be applied, for each choice point: construction choices about classifying people or about excluding participants given their responses. If a different data-set is constructed for each combination of alternatives then many different data-sets can be produced, all starting from the same raw data. (For their example study, Steegen et al. (2016a) found they could construct 120 or 210 different data-sets, based on the choice combinations.) Critically, for us, Steegen et al. (2016a) showed that if we apply the same analysis method to the different data-sets then our results will vary.\nLet me spell this out, bit by bit:\n\nwe approach our study with the same research question, and the same verbal prediction;\nwe begin with the exact same data;\nwe then construct different data-sets depending on different but equally reasonable processing choices;\nwe then apply the same analysis analysis, to test the same prediction, using each different data-set;\nwe will see different results for the analyses of the different data-sets.\n\nAlternate constructions of the same data may cause variation in the results of statistical tests. Some kinds of data processing choices may be more influential on results than others. It seems unlikely that we can identify, in advance, which choices matter more.\nSteegen et al. (2016a) suggest that we can deflate (shrink) the multiverse in different ways. I want to state their suggestions, here, because we will come back to these ideas in the classes on the linear model.\n\nDevelop better theories and improved measurement of the constructs of interest.\nDevelop more complete and precise theory for why some processing options are better than others.\n\nBut you will be asking yourself: What do I need to think about, for the research report assignment?\n\n\n\n\n\n\nTip\n\n\n\n\nWhen you read a psychological research report, identify where the researchers talk about how they process their data: classification, coding, exclusion, transformation, etc.\nIf you can access the raw data, ask yourself: could different choices change the results of the same analysis?\n\n\n\n\n\n9.2.4.3 Analysis multiverses\nEven if we begin with the same research question and, critically, the same data-set, the results of a series of studies show that different researchers will often (reasonably) make different choices about the analysis they do to answer the research question. We often call these studies (analysis or model) multiverse studies. In these studies, we see variation in analysis and this variation is also associated with variation in results.\nAn influential example, in psychology, is reported by Silberzahn and colleagues (Silberzahn et al., 2017; Silberzahn & Uhlmann, 2015) who asked 29 teams of researchers to answer the same question (“Are (soccer) referees more likely to give red cards to players with dark skin than to players with light skin?”) with the same data-set (data about referee decisions in football league games). The teams made their own decisions about how to answer the question in doing the analysis. The teams shared their plans, and commented on each others’ ideas. The discussion did not lead to a consensus about what analysis approach is best. In the end, the different teams did different analyses and, critically, the different analyses had different results. The results varied in whether the test of the effect of players skin colour (on whether red cards were given) was significant or not, and on the strength of the estimated association between the darkness of skin colour (lighter to darker) and the chances (low to high) of getting a red card.\nThere have now been a series of multiverse or multi-analyst studies which demonstrate that, under certain conditions, different researchers may adopt different analysis approaches – which will have different results – in answering the same research question with the same data. This demonstration has been repeated in studies in health, medicine, psychology, neuoscience, and sociology, among other research fields (e.g., Parsons (n.d.); Breznau et al. (2022); Klau et al. (n.d.); Klau et al. (2021); Wessel et al. (2020); Poline et al. (2006); Maier-Hein et al. (2017); Starns et al. (2019); Fillard et al. (2011); Dutilh et al. (2019); Salganik et al. (2020); Bastiaansen et al. (2020); Botvinik-Nezer et al. (2020); Schweinsberg et al. (2021); Patel et al. (2015); see, for reviews, and some helpful guidance, Aczel et al. (2021); Del Giudice & Gangestad (2021); Hoffmann et al. (n.d.); Wagenmakers et al. (2022)).\nIn these studies, we typically see variation in how psychological constructs are operationalized (e.g., how do we measure or code for social status?), how data are processed or data-sets constructed (as in Steegen et al. (2016b)), plus variation in what statistical techniques are used, and in how those techniques are used. This variation can be understood to reflect kinds of uncertainty (Klau et al., n.d.; Klau et al., 2021): uncertainty about how to process data, and uncertainty about the model or methods we should use to test or estimate effects. Further research makes it clear that we should be aware, if we are not already, of the variation in results that can be expected because different researchers may choose to design studies, and construct stimulus materials, in different ways given the same research hypothesis information (Landy et al., 2020).\nBut you will be asking yourself: What do I need to think about, for the research report assignment?\n\n\n\n\n\n\nTip\n\n\n\n\nWhen you read a psychological research report, identify where the researchers talk about how they analyse their data: the hypothesis or prediction they test; the method; their assumptions; the variables they include; the checks or the alternate analyses they did or did not do.\nIf you can access the data and analysis code, ask yourself: could different methods change the results of the same analysis?\n\n\n\n\n\n9.2.4.4 What can we conclude – the story so far?\nThis is a good place to look at what we have discussed, and present an evaluation of the story so far.\nThis is not a story where everybody or nobody is right or where everything or nothing is true 3. Instead, we can be guided by the advice (Meehl, 1967; Scheel, 2022; Steegen et al., 2016a) that we should:\n\nseek better and more complete theorizing about the constructs of interest and how we measure them, and\nseek more complete and more precise theory so that some options are theoretically superior than others, and should be preferred, when constructing data-sets or specifying analysis methods.\n\nNot all research questions and not all hypothesis information will allow an equally wide variety of potential reasonable approaches to the analysis. As Paul Meehl argued a long time ago (Meehl, 1967, 1978), and researchers like Anne Scheel (Scheel et al., 2021; Scheel, 2022) argued more recently, the complexity of the thing we study – people, and what they do – and the still early development of our understanding of this thing, mean that what we want but what we do not see, in psychology, are scientifically productive tests of falsifiable theories. (See, consistent with this perspective, discussions by Auspurg & Brüderl (2021) and by Del Giudice & Gangestad (2021) about the range of analysis possibilities that may or may not be allowed, in multiverse analyses, by more or less clear research questions or well-developed causal theories.) Our concern should not so much be with being able to do statistical analysis, or with finding significant or not significant results. It would be more useful to do analyses to test concrete, inflexible, precise predictions that can be wrong.\nNor is this a story, I think, about the potential for cheating. While we may refer to subjective choices or to researcher flexibility, the differences that we see do not resemble the researcher degrees of freedom (Simmons et al., 2011b) some may exploit, consciously or unconsciously, to change results to suit their aims. Instead, the multiverse results show us the impact of the reasonable differences in approach that different researchers may sensibly choose to take when they try to answer a research question with data.\nNot all alternates, at a given point of choosing, in the data analysis workflow, will have equal impact. Work by Young (Young, 2018; Young & Holsteen, 2017) indicates that if we deliberately examine the impact of method or model uncertainty, over different sets of possible choices — about what variables or what observations we include in an analysis, for example — we may find that some results are robust to an array of different options, while other results are highly susceptible to different choices. This work suggests another way in which uncertainty about methods or variation in results can be turned into progress in understanding the phenomena that interest us: through systematic, informed, interrogation of the ways that results can vary.\nIn general, in science, the acceptance of research findings must always be negotiated (Bourdieu, 2004). Here, we see that the grounds of negotiation should often include an analysis of the impact on the value of evidence of the different analysis approaches that researchers can or do apply to the data that underlie that evidence.\nBut you will be asking yourself: what do I need to think about, for the assignment?\n\n\n\n\n\n\nTip\n\n\n\n\nThe results of multiverse analyses show us that if we see one analysis reported in a paper, or one workflow, that does not mean that only one analysis can reasonably be applied.\nIf you read the methods or results section of a paper, you should reflect: what other analysis methods could be used here? How could variation in analysis method — in what or how you do the analysis — influence the results?\n\n\n\nMaking you aware of the potential for analysis choices is useful because developing researchers, including graduate students, are often not aware of the room for choice in the data analysis workflow. Developing researchers — you — may be instructed that “this is how we do things” or “you should follow what researchers did previously”. Following convention is not necessarily a bad thing: it is a feature of the normal practice of science (Kuhn, 1970). However, you can now see, perhaps, that there likely will be alternative ways to process or to analyse data than the approach a supervisor, lab or field normally adopts.\nThis understanding or awareness has three implications for practice, it means:\n\nWhen we talk about the analysis we do, we should explain our choices.\nWe should check, or enable others to check, what impact making different choices would have on our results.\nMost importantly: we can allow ourselves the freedom to critically evaluate the choices researchers make, even the choices researchers make in published articles.\n\n\n\n\n9.2.5 From the multiverse to kinds of reproducibility\nMultiverse analyses and post-publication analyses, in general, show that we can and should question or critically evaluate the analyses we encounter in the literature. This work can usefully detect problems in original published analyses (e.g., A. Gelman & Weakliem, 2009; Herndon et al., 2014; Wagenmakers et al., 2011). It can demonstrate where original published claims are or are not robust to variation of analysis method or approach.\nGiven these lessons, and the implications we have identified, we should expect or hope to see open science practices (Munafò et al., 2017; Nosek et al., 2022):\n\nshare data and code;\npublish research reports in ways that enable others to check or query analyses.\n\nAs we discuss, following, these practices are now common but the quality of practice can sometimes be questioned. This matters for you because it makes it more challenging – in specific identifiable locations – to locate, access, analyse and report previously collected data.\nThe discussion of current practices identifies where or how the assignment may be more challenging, but also identifies some of the exact places where the assignment provides a real opportunity to do original research work.\nFirst, I am going to introduce some ideas that will help you to think about what you are doing when you do this work. We focus on the concept of reproducibility.\nGilmore et al. (2017; following Goodman et al., 2016) present three kinds of reproducibility:\n\nmethods reproducibility\nresults reproducibility\ninferential reproducibility\n\nIn looking at reproducibility, here, we are considering how much, or in what ways, the results or the claims that are made in a published study can be found or repeated by someone else.\n\n9.2.5.1 Methods reproducibility\nAs Gilmore et al. (2017) discuss, methods reproducibility means that another researcher should be able to get the same results if they use the same tools and analysis methods to analyse the same data-set [some researchers also refer to analytic reproducibility or computational reproducibility; see e.g. Crüwell et al. (n.d.); Hardwicke et al. (2018); Hardwicke et al. (n.d.); Laurinavichyute et al. (2022); Minocher et al. (n.d.)].\nIn neuroimaging, the multiplicity of possible implementations of the data analysis pipeline (Carp, 2012a), and the fact that important elements or information about the pipeline deployed by researchers may be missing from published reports (Carp, 2012b), can make it challenging to identify how results can be reproduced.\nIn psychological science, in evaluating reports of results from analyses of behavioural data collected through survey or experimental work, in principle, we should expect to be able to access the data collected by the study authors, follow the description of their analysis method, and reproduce the results they report.\n\n\n\n\n\n\nTip\n\n\n\n\nFor an assignment in which we ask students to locate, access, analyse and report previously collected data, we are directly concerned with methods reproducibility.\n\n\n\n\n\n9.2.5.2 Results reproducibility\nResults reproducibility means that if another researcher completes a new study with new data they are able to get the same results as the results reported following an original study: this often referred to as replication. The replication studies that have been reported (e.g., Aarts et al., 2015), and continue to be reported (see, for example, the studies discussed by Nosek et al. (2022)), in the last several years, present attempts to examine the results reproducibility of published findings.\nIn the classes on the linear model, we will examine if similar or different results are observed in a series of studies using the same procedure and the same materials. We shall discuss, in those classes, in more depth, what results reproducibility (or study replication) can or cannot tell us about the behaviours that interest us.\n\n\n9.2.5.3 Inferential reproducibility\nInferential reproducibility means that if a researcher repeats a study (aiming for results reproducibility) or re-analyses an original data-set (aiming for methods reproducibility) then they can come to the same or similar conclusions as the authors of the report of an original study.\nHow is inferential reproducibility not methods or results reproducibility? Goodman et al. (2016) explain that researchers can make the same conclusions from different sets of results and can reach different conclusions from the same set of results.\nHow is it possible to reach different conclusions from the same results? We can imagine two scenarios.\nFirst, we have to think about the wider research field, the research context, within which we consider a set of results. It may be that two different researchers will come to look at the same results with different expectations about what the results could tell us (in Bayesian terms, with different prior expectations). Given different expectations, it is easy to imagine different researchers looking at the same results and, for example, one researcher being more skeptical than another about what conclusion can be taken from those results. (In the class on graduate writing skills, I discuss in some depth the importance of reviewing a research literature in order to get an understanding of the assumptions, conventions or expectations that may be shared by the researchers working in the field.)\nSecond, imagine two different researchers looking at the same results — picture the original authors of a published study, and someone doing a post-publication re-analysis of their data — you can expect that the re-analysis or the reproducibility analysis could identify reasons to value the evidence differently, or to reach more skeptical conclusions, through critical evaluation of:\n\ndata processing choices;\nthe choice of the method used to do analysis;\nchoices in how the analysis method is used.\n\n… where that critical evaluation involves an analysis of the choices the original researchers made, perhaps involving an analysis of other choices they could have made, perhaps reflecting on how effectively the analyses address a given research question or test a given prediction.\n\n\n\n\n\n\nTip\n\n\n\n\nWe can think about the work we do, when we analyse previously reported data, in terms of the need to identify the reproducibility of results, methods and inferences.\nIn psychological science, determining that someone can get the same results, by analysing the same data, or will reach the same conclusions from the same results, are important – potentially, original – research contributions.\n\n\n\n\n\n\n9.2.6 The current state of the match between open science ideas and practices\nI have said that we should expect or hope to see open science practices (Munafò et al., 2017; Nosek et al., 2022) where researchers:\n\nshare data and code;\npublish research reports in ways that enable others to check or query analyses.\n\nThis raises an important question: What exactly do we see, when we look at current practices? The question is important because answering it helps to identify where the challenges are located when you complete your work to locate, access, analyse and report previously collected data.\nI break the discussion of what we see into two parts. Firstly, I look at the results of audits of data and code sharing (see Section 9.2.6.2): are data shared and can we access the data? Secondly, I discuss analyses of methods reproducibility, and shared data and code usability (see Section 9.2.6.3): can others reproduce the results reported in published articles, given shared data? can others access and run shared analysis code? can others use the shared code to reproduce the reported results? Again, I need to be brief but reference sources that you can follow-up.\n\n9.2.6.1 The link between the credibility revolution and the reproducibility of results\nI should be clear, before we go on, about the link between the credibility revolution in science, and the effort to examine reproducibility of results. Many elements of the credibility revolution emerged out of the observation that it has often been difficult to repeat the results of published studies when we conduct new studies (replication studies or results reproducibility; e.g., Aarts et al. (2015)). However, it is clearly difficult to know what to replicate or reproduce if we cannot reproduce the results presented in a study report (methods reproducibility), given the study data (Artner et al., 2021; Laurinavichyute et al., 2022; Minocher et al., n.d.).\n\n\n9.2.6.2 Data and code sharing\nResearch on data and code sharing practices suggest that practices have improved, from earlier low levels.\nIn an important early report, Wicherts et al. (2006) observed that it was very difficult to obtain data reported in psychological research articles from the authors of the articles. They asked for data from the lead authors of 141 articles published in four leading psychology journals, for about 25% of the studies. This low response rate was found despite the fact that authors in these journals must agree to the principle that data can be shared with others wishing to verify claims.\nPractice has changed: how?\nOne change to practice has involved the use of open science badges. In journals like Psychological Science authors of articles may be awarded badges — Open Data, Open Materials, Preregistration badges — by the editorial team. Authors can apply for and earn the badges by providing information about open practices, and journal articles are published with the badges displayed near the front of the articles.\nIn theory, initiatives like encouraging authors to earn open science badges should mean that data sharing practices improve, enabling access to data and code for those, like you, who would like to re-analyse previously published data. In theory, all you should need to do — to locate and access data — is just search articles in the journal Psychological Science for studies with open data badges, and follow links from the published articles to then access study data at an open repository like the Open Science Framework (OSF) What do we see in practice?\nAnalyses reported by Kidwell et al. (2016) as well as analyses reviewed by Nosek et al. (2022) indicate that more articles have claimed to make data available in the time since badges were introduced. When they did their analysis, Kidwell et al. (2016) found that a substantial proportion, but not all, of the articles in Psychological Science can be found to actually provide access to shared data. However, critically, many but not all the articles with open data badges provide access to data available through an open repository, data that are correct, complete and usable (Kidwell et al., 2016). In their later report, the analyses reviewed by Nosek et al. (2022) suggest that the use of repositories like OSF for data sharing may be accelerating but that, over the last few years, the rate at which open science practices like sharing data, overall, appears to be substantial but not yet reported or observed in a majority of the work of researchers.\nMany journals now require the authors of articles to include a Data Availability Statement to locate their data. Analyses by Federer (2022) indicate that Data Availability Statements for articles published in the open access 4 journal PLOS ONE often, helpfully, include Digital Object Identifiers (DOIs) or Universal resource locators (URLs) enabling direct access to shared data (i.e., without having to contact authors). Of those DOIs or URLs, most appeared to be associated with resources that could successfully be retrieved. In contrast, analyses reported by Gabelica et al. (2022) indicate that where article authors state that “data sets are available on reasonable request” (the most common availability statement), most of the time, the authors did not respond or declined to share the data (see similar findings, across fields, by Tedersoo et al., 2021). Clearly, in the analyses of open science practices we have seen so far, data sharing is more effective where sharing does not have to work through authors.\n\n\n\n\n\n\nTip\n\n\n\n\nWhen you are looking for a study in order to get data that you can then re-analyse, it makes sense to look, first, for studies focusing on research questions that interest you.\nWhen you are looking for published reports where the authors share data, look for articles with open science badges or where you can see a Data Availability Statement.\nChoose articles where the authors provide a direct link to their data, where the data are located on an open repository like the Open Science Framework (there are other repositories).\n\n\n\n\n\n9.2.6.3 Enabling others to check or query analyses\nResearch on data and code sharing practices suggest that practices have improved but that there are concerns about the quality of the sharing. Here, the critical concern relates to the word enable in the objective: that we should publish research reports in ways that enable others to check or query analyses.\nJohn Towse and colleagues (Towse et al., 2021) at Lancaster University examined the quality of open data-sets to assess their quality in terms of their completeness and reusability (see also Roche et al., 2015).\n\ncompleteness: are all the data and the data descriptors supporting a study’s findings publicly available?\nreusability: how readily can the data be accessed and understood by others?\n\nFor a sample of data-sets, they found that about half were incomplete, and about two-thirds were shared in a way that made them difficult to use. Practices tended to be slightly better in more recent publications. (Broadly similar results are reported by (Hardwicke et al., 2018).)\nWhere data were found to be incomplete, this appeared to be, in part, because participants were excluded in the processing of the data for analysis but this information was not in the report, or because data were shared without a guide or “readme” file or data dictionary (or codebook) explaining the structure, coding or composition of the shared data.\nPotentially important for future open science practices, (Towse et al., 2021; also Roche et al., 2015) found that sharing data as Supplementary materials may appear to carry risks that, in the long term, mean that data may become inaccessible.\n\n\n\n\n\n\nTip\n\n\n\n\nWhen you locate open data you can access, look for a guide, “readme” file, codebook or data dictionary explaining the data: you need to be able to understand what the variables are, what the observations relate to (observations per person, per trial?) and how variables are coded.\nLocate and examine carefully the parts of the published report, or the data guide, where the authors explain how they processed their data.\n\n\n\nA number of studies have been conducted to examine whether shared data and analysis code can be reused by others to reproduce the results reported in papers (e.g., Artner et al., 2021; Crüwell et al., n.d.; Hardwicke et al., n.d.; Hardwicke et al., 2018; Laurinavichyute et al., 2022; Minocher et al., n.d.; Obels et al., 2020; see Artner et al., 2021 for a review of reproducibility studies). In critical respects, the researchers doing this work are doing work similar to the work we are helping students to do, locating, accessing, and analysing previously collected data. In these studies, typically, the researchers progressed through a series of steps.\n\nSearched the articles published in a journal (e.g., Cognition, the Journal of Memory and Language, Psychological Science), published in a topic area across multiple journals (e.g., social learning, psychological research), or associated with a specific practice (e.g., registered reports.\nSelected a subset of articles where it was identified that data could be accessed.\nIdentify a target result or outcome to reproduce, for each article. In their analyses, Hardwicke and colleagues (Hardwicke et al., n.d.; Hardwicke et al., 2018) focused on attempting to reproduce primary or straightforward and substantive outcomes: substantive – if emphasized in the abstract, or presented in a table or figure; straightforward – if the outcome could be calculated using the kind of test one would learn in an introductory psychology course (e.g., t-test, correlation).\nAttempted to reproduce the results reported in the article, using the description of the data analysis presented in the article, and the analysis code (if provided), in some cases asking for information from the original study authors, in other cases working independently of original authors.\n\nWhat the reproducibility studies appear to show is that, for many published reports, if data are shared and if the shared data are accessible and reusable then, most of the time, the researchers could reproduce the results presented by the original study authors (Hardwicke et al., n.d.; Hardwicke et al., 2018; Laurinavichyute et al., 2022; Minocher et al., n.d.; Obels et al., 2020; but see Crüwell et al., n.d.). This is great. But what is interesting, for us, is where the reproducibility researchers encountered challenges. You may encounter the same or similar challenges.\nI list some challenges that the researchers describe, following. Before you look at the list, I want to assure you: you will not find all these challenges present for any one article you look at. Most likely, you will find one or two challenges. Obviously, some challenges will be more difficult than others.\n\n\n\n\n\n\nTip\n\n\n\n\nWhen you find a study you are interested in, with open data and maybe open analysis code, your main challenge will often be to identify exactly what analysis the original study authors did to answer their research question.\nLocate and examine carefully the parts of the published report where the authors explain how they did the analysis that gave them their key result. Usually that key result should be identified in the abstract or in the conclusion.\n\n\n\n\n9.2.6.3.1 Data challenges\n\nData Availability Statements or open science badges indicate data are shared but data are not directly accessible through a link to an open repository.\nThe data are shared and accessible but there is missing or incorrect information about the data. The documentation, codebook or data dictionary is missing or incomplete. There is unclear or missing information about the variables or the observations, or about the coding of variable values, responses.\nOriginal study authors may share raw and processed data or just processed or just raw data. It may not be clear how raw data were processed to construct the data analysed for the report. It may not be clear how variables were transformed or calculated or processed.\nThere may be mismatches between the variables referred to in the report and the variables named in the data file. It may be unclear how a data file corresponds to a study described in a report, where there are multiple studies and multiple data files.\n\n\n\n9.2.6.3.2 Analysis challenges\n\nThe original report includes a description of the analysis but the description of the analysis procedure is incomplete or ambiguous.\nThere may be a mismatch, in the report, between a hypothesis, and the analysis specified to test the hypothesis (maybe in the Methods section), compared to a long sequence of results reported in the Results section. This makes it difficult to identify the key analysis.\nIt is easier to reproduce results if both data and code are shared because the presentation of the analysis code usually (not always) makes clear what analysis was done to get the results presented in the report.\nSometimes, analysis code is shared but it is difficult to use because it requires proprietary software (e.g., SPSS) or because it requires function libraries that are no longer publicly available.\nSometimes, there are errors in the analysis. Sometimes, there are errors in the presentation of the results, where results have been incorrectly copied into reports from analysis outputs.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Why we are asking you to do this</span>"
    ]
  },
  {
    "objectID": "intro.html#this-is-why",
    "href": "intro.html#this-is-why",
    "title": "9  Why we are asking you to do this",
    "section": "9.3 This is why",
    "text": "9.3 This is why\nThe research report assignment requires students to locate, access, analyse and report previously collected data. At the start of the introduction, I said I would explain the answer to the question:\n\nWhy: what is the motivation for the assignment?\n\nI summarize, following, the main points of the answer I have given. When you review these points, I want you to think about two things, returning to the ideas of Bourdieu (2004) and Kuhn (1970) I sketched at the start.\nOften what we do in science is guided by convention, the assumptions and habits of normal practice (Kuhn, 1970). These conventions can work in our minds so that if we encounter an anomaly or discrepancy between what we expect and what we find, in our work, we may usually blame ourselves: it was something wrong that we did or failed to do. It can cause us anxiety if we do not reproduce a result we think we should be able to reproduce (Lubega et al., n.d.). But I want you to understand, from the start, that sometimes, if you think you have found an error or a problem in a published analysis or a shared data-set, you may be right.\nIf there is anything we have learned, through the findings of replication studies, multiverse analyses, and reproducibility audits it is that people make mistakes, different choices are often reasonable, and we always need to check the evidence.\n\n9.3.1 Summary: this is why\n\nWe are in the middle of a credibility revolution. The lessons we have learned so far oblige us to think about and to teach good open science practices that safeguard the value of evidence in psychology.\nThis matters, even if we do not care about scientific methods, because if we care about the translation into policy or practice – in clinical psychology, in education, health, marketing and other fields – what we do will depend on the value of the research evidence that informs policy ideas or practice guides.\nFocusing on data analysis, it is useful to think about the whole data pipeline in analysis, the workflow that takes us from data collection to raw data to data processing to analysis to the presentation of results.\nAt every stage of the data pipeline, there are choices about what to do. There are not always reasons why we make one choice instead of another. Sometimes, we are guided by convention, example or instruction.\nThe existence of choices means the path we take, when we do data analysis, can be one path among multiple different forking paths.\nFor some parts of the pipeline – data-set construction, data analysis choices – reasonable people might make different decisions to sensibly answer the same research question, given the same data. This variation between pathways can be more or less important in influencing the results we see.\nIf results tend to stay similar across different ways of doing analysis, we might conclude that the results are reasonably robust across contexts, choices, or other variation in methods.\nTo enable others to see what we did (versus what we could have done), to see how we got to our results from our data, it is important to share our data and code.\nEveryone makes mistakes and we should make it easy for others, and ourselves, to find those mistakes by sharing our data and code in accessible, clear, usable ways.\nWe need to teach and learn how to share effectively the data and the code that we used to answer our research questions.\n\nIn constructing the assignment – in asking and supporting students to locate, access, analyse and report previously collected data – we are presenting an opportunity to really investigate and evaluate existing practices.\nYou may find that this work is challenging, in some of the places that reproducibility research has identified there can be challenges. Where the challenges cannot be fixed – if you have found an interesting study but the study data are inaccessible or unusable – we will advise you to move on to another study. Where the challenges can be fixed – if data require processing, or if analysis information requires clarification – we will provide you with help or enabling information so that you fix the problems yourself.\n\n\n\n\n\n\nTip\n\n\n\n\nMaybe the main lesson from this exercise is a reminder of the Golden rule: treat others as you would like to be treated.\nIf it is frustrating when it is difficult to understand information about an analysis or about data, or when it is difficult to access and reuse shared data and code.\nWhen it is your turn — do better — reflecting on what frustrated you.\n\n\n\nOne last question: why not just do less demanding or challenging tasks? Because this is part of what makes graduate degree valuable, what will make you more skilled in the workplace. Most of the time, we work in teams, we inherit problems or data analysis tasks, or are given results with partial information. The lessons you learn here will help you to effectively navigate those situations.\n\n\n\n\nAarts, E., Dolan, C. V., Verhage, M., & Van der Sluis, S. (2015). Multilevel analysis quantifies variation in the experimental effect while optimizing power and preventing false positives. BMC Neuroscience, 16(1), 1–15. https://doi.org/10.1186/s12868-015-0228-5\n\n\nAczel, B., Szaszi, B., Nilsonne, G., Akker, O. R. van den, Albers, C. J., Assen, M. A. van, Bastiaansen, J. A., Benjamin, D., Boehm, U., Botvinik-Nezer, R., Bringmann, L. F., Busch, N. A., Caruyer, E., Cataldo, A. M., Cowan, N., Delios, A., Dongen, N. N. van, Donkin, C., Doorn, J. B. van, … Wagenmakers, E.-J. (2021). Consensus-based guidance for conducting and reporting multi-analyst studies. eLife, 10, e72185. https://doi.org/10.7554/eLife.72185\n\n\nArtner, R., Verliefde, T., Steegen, S., Gomes, S., Traets, F., Tuerlinckx, F., & Vanpaemel, W. (2021). The reproducibility of statistical results in psychological research: An investigation using unpublished raw data. Psychological Methods, 26(5), 527–546. https://doi.org/10.1037/met0000365\n\n\nAuspurg, K., & Brüderl, J. (2021). Has the Credibility of the Social Sciences Been Credibly Destroyed? Reanalyzing the “Many Analysts, One Data Set” Project. Socius, 7, 23780231211024421. https://doi.org/10.1177/23780231211024421\n\n\nBastiaansen, J. A., Kunkels, Y. K., Blaauw, F. J., Boker, S. M., Ceulemans, E., Chen, M., Chow, S.-M., Jonge, P. de, Emerencia, A. C., Epskamp, S., Fisher, A. J., Hamaker, E. L., Kuppens, P., Lutz, W., Meyer, M. J., Moulder, R., Oravecz, Z., Riese, H., Rubel, J., … Bringmann, L. F. (2020). Time to get personal? The impact of researchers choices on the selection of treatment targets using the experience sampling methodology. Journal of Psychosomatic Research, 137, 110211. https://doi.org/10.1016/j.jpsychores.2020.110211\n\n\nBornstein, M. H., Jager, J., & Putnick, D. L. (2013). Sampling in developmental science: Situations, shortcomings, solutions, and standards. Developmental Review, 33(4), 357–370. https://doi.org/10.1016/j.dr.2013.08.003\n\n\nBotvinik-Nezer, R., Holzmeister, F., Camerer, C. F., Dreber, A., Huber, J., Johannesson, M., Kirchler, M., Iwanir, R., Mumford, J. A., Adcock, R. A., Avesani, P., Baczkowski, B. M., Bajracharya, A., Bakst, L., Ball, S., Barilari, M., Bault, N., Beaton, D., Beitner, J., … Schonberg, T. (2020). Variability in the analysis of a single neuroimaging dataset by many teams. Nature, 582(7810), 84–88. https://doi.org/10.1038/s41586-020-2314-9\n\n\nBourdieu, P. (2004). Science of Science and Reflexivity. Polity.\n\n\nBreznau, N., Rinke, E. M., Wuttke, A., Nguyen, H. H. V., Adem, M., Adriaans, J., Alvarez-Benjumea, A., Andersen, H. K., Auer, D., Azevedo, F., Bahnsen, O., Balzer, D., Bauer, G., Bauer, P. C., Baumann, M., Baute, S., Benoit, V., Bernauer, J., Berning, C., … Żółtak, T. (2022). Observing many researchers using the same data and hypothesis reveals a hidden universe of uncertainty. Proceedings of the National Academy of Sciences, 119(44), e2203150119. https://doi.org/10.1073/pnas.2203150119\n\n\nButton, K. S., Ioannidis, J. P. A., Mokrysz, C., Nosek, B. A., Flint, J., Robinson, E. S. J., & Munafò, M. R. (2013). Power failure: Why small sample size undermines the reliability of neuroscience. Nature Reviews Neuroscience, 14(5), 365–376.\n\n\nCarp, J. (2012a). On the plurality of (methodological) worlds: Estimating the analytic flexibility of FMRI experiments. Frontiers in Neuroscience, 6, 149.\n\n\nCarp, J. (2012b). The secret lives of experiments: Methods reporting in the fMRI literature. Neuroimage, 63(1), 289–300.\n\n\nCohen, J. (1962). The statistical power of abnormal-social psychological research: A review. Journal of Abnormal and Social Psychology, 65(3), 145–153. https://doi.org/10.1037/h0045186\n\n\nCrüwell, S., Apthorp, D., Baker, B. J., Colling, L., Elson, M., Geiger, S. J., Lobentanzer, S., Monéger, J., Patterson, A., Schwarzkopf, D. S., Zaneva, M., & Brown, N. J. L. (n.d.). What’s in a badge? A computational reproducibility investigation of the open data badge policy in one issue of psychological science. https://doi.org/10.31234/osf.io/729qt\n\n\nDel Giudice, M., & Gangestad, S. W. (2021). A Traveler’s Guide to the Multiverse: Promises, Pitfalls, and a Framework for the Evaluation of Analytic Decisions. Advances in Methods and Practices in Psychological Science, 4(1), 2515245920954925. https://doi.org/10.1177/2515245920954925\n\n\nDutilh, G., Annis, J., Brown, S. D., Cassey, P., Evans, N. J., Grasman, R. P. P. P., Hawkins, G. E., Heathcote, A., Holmes, W. R., Krypotos, A.-M., Kupitz, C. N., Leite, F. P., Lerche, V., Lin, Y.-S., Logan, G. D., Palmeri, T. J., Starns, J. J., Trueblood, J. S., Maanen, L. van, … Donkin, C. (2019). The Quality of Response Time Data Inference: A Blinded, Collaborative Assessment of the Validity of Cognitive Models. Psychonomic Bulletin & Review, 26(4), 1051–1069. https://doi.org/10.3758/s13423-017-1417-2\n\n\nFederer, L. M. (2022). Long-term availability of data associated with articles in PLOS ONE. PLOS ONE, 17(8), e0272845. https://doi.org/10.1371/journal.pone.0272845\n\n\nFillard, P., Descoteaux, M., Goh, A., Gouttard, S., Jeurissen, B., Malcolm, J., Ramirez-Manzanares, A., Reisert, M., Sakaie, K., Tensaouti, F., Yo, T., Mangin, J.-F., & Poupon, C. (2011). Quantitative evaluation of 10 tractography algorithms on a realistic diffusion MR phantom. NeuroImage, 56(1), 220–234. https://doi.org/10.1016/j.neuroimage.2011.01.032\n\n\nFlake, J. K., & Fried, E. I. (2020). Measurement Schmeasurement: Questionable Measurement Practices and How to Avoid Them. Advances in Methods and Practices in Psychological Science, 3(4), 456–465. https://doi.org/10.1177/2515245920952393\n\n\nGabelica, M., Bojčić, R., & Puljak, L. (2022). Many researchers were not compliant with their published data sharing statement: a mixed-methods study. Journal of Clinical Epidemiology, 150, 33–41. https://doi.org/10.1016/j.jclinepi.2022.05.019\n\n\nGelman, a. (2015). The connection between varying treatment effects and the crisis of unreplicable research: A bayesian perspective. Journal of Management, 41(2), 632–643. https://doi.org/10.1177/0149206314525208\n\n\nGelman, A., & Loken, E. (2014a). The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-hacking” and the research hypothesis was posited ahead of time. Psychological Bulletin, 140(5), 1272–1280.\n\n\nGelman, A., & Loken, E. (2014b). The statistical crisis in science. American Scientist, 102(6), 460–465. https://doi.org/10.1511/2014.111.460\n\n\nGelman, A., & Weakliem, D. (2009). Of beauty, sex and power. American Scientist, 97(4), 310–316. https://doi.org/10.1511/2009.79.310\n\n\nGilmore, R. O., Diaz, M. T., Wyble, B. A., & Yarkoni, T. (2017). Progress toward openness, transparency, and reproducibility in cognitive neuroscience. Annals of the New York Academy of Sciences, 1396, 5–18. https://doi.org/10.1111/nyas.13325\n\n\nGoodman, S. N., Fanelli, D., & Ioannidis, J. P. A. (2016). What does research reproducibility mean? Science Translational Medicine, 8(341).\n\n\nHardwicke, T. E., Bohn, M., MacDonald, K., Hembacher, E., Nuijten, M. B., Peloquin, B. N., deMayo, B. E., Long, B., Yoon, E. J., & Frank, M. C. (n.d.). Analytic reproducibility in articles receiving open data badges at the journal psychological science: An observational study. Royal Society Open Science, 8(1), 201494. https://doi.org/10.1098/rsos.201494\n\n\nHardwicke, T. E., Mathur, M. B., MacDonald, K., Nilsonne, G., Banks, G. C., Kidwell, M. C., Hofelich Mohr, A., Clayton, E., Yoon, E. J., Henry Tessler, M., Lenne, R. L., Altman, S., Long, B., & Frank, M. C. (2018). Data availability, reusability, and analytic reproducibility: evaluating the impact of a mandatory open data policy at the journal Cognition. Royal Society Open Science, 5(8), 180448. https://doi.org/10.1098/rsos.180448\n\n\nHenrich, J., Heine, S. J., & Norenzayan, A. (2010). The weirdest people in the world? The Behavioral and Brain Sciences, 33(2-3). https://doi.org/10.1017/S0140525X0999152X\n\n\nHerndon, T., Ash, M., & Pollin, R. (2014). Does high public debt consistently stifle economic growth? A critique of Reinhart and Rogoff. Cambridge Journal of Economics, 38(2), 257–279. https://doi.org/10.1093/cje/bet075\n\n\nHoffmann, S., Schönbrodt, F., Elsas, R., Wilson, R., Strasser, U., & Boulesteix, A.-L. (n.d.). The multiplicity of analysis strategies jeopardizes replicability: Lessons learned across disciplines. Royal Society Open Science, 8(4), 201925. https://doi.org/10.1098/rsos.201925\n\n\nIoannidis, J. P. a. (2005). Why most published research findings are false. PLoS Medicine, 2(8), 0696–0701. https://doi.org/10.1371/journal.pmed.0020124\n\n\nJohn, L. K., Loewenstein, G., & Prelec, D. (2012). Measuring the prevalence of questionable research practices with incentives for truth telling. Psychological Science, 23(5), 524–532. https://doi.org/10.1177/0956797611430953\n\n\nKidwell, M. C., Lazarević, L. B., Baranski, E., Hardwicke, T. E., Piechowski, S., Falkenberg, L. S., Kennett, C., Slowik, A., Sonnleitner, C., Hess-Holden, C., Errington, T. M., Fiedler, S., & Nosek, B. A. (2016). Badges to acknowledge open practices: A simple, low-cost, effective method for increasing transparency. PLoS Biology, 14(5), 1–15. https://doi.org/10.1371/journal.pbio.1002456\n\n\nKlau, S., Hoffmann, S., Patel, C. J., Ioannidis, J. P., & Boulesteix, A.-L. (2021). Examining the robustness of observational associations to model, measurement and sampling uncertainty with the vibration of effects framework. International Journal of Epidemiology, 50(1), 266–278. https://doi.org/10.1093/ije/dyaa164\n\n\nKlau, S., Schönbrodt, F., Patel, C. J., Ioannidis, J., Boulesteix, A.-L., & Hoffmann, S. (n.d.). Comparing the vibration of effects due to model, data pre-processing and sampling uncertainty on a large data set in personality psychology. https://doi.org/10.31234/osf.io/c7v8b\n\n\nKuhn, T. S. (1970). The structure of scientific revolutions ([2d ed., enl). University of Chicago Press.\n\n\nLandy, J. F., Jia, M. L., Ding, I. L., Viganola, D., Tierney, W., Dreber, A., Johannesson, M., Pfeiffer, T., Ebersole, C. R., Gronau, Q. F., Ly, A., Bergh, D. van den, Marsman, M., Derks, K., Wagenmakers, E.-J., Proctor, A., Bartels, D. M., Bauman, C. W., Brady, W. J., … Uhlmann, E. L. (2020). Crowdsourcing hypothesis tests: Making transparent how design choices shape research results. Psychological Bulletin, 146(5), 451–479. https://doi.org/10.1037/bul0000220\n\n\nLaurinavichyute, A., Yadav, H., & Vasishth, S. (2022). Share the code, not just the data: A case study of the reproducibility of articles published in the Journal of Memory and Language under the open data policy. Journal of Memory and Language, 125, 104332. https://doi.org/10.1016/j.jml.2022.104332\n\n\nLubega, N., Anderson, A., & Nelson, N. (n.d.). Experience of irreproducibility as a risk factor for poor mental health in biomedical science doctoral students: A survey and interview-based study. https://doi.org/10.31222/osf.io/h37kw\n\n\nMaier-Hein, K. H., Neher, P. F., Houde, J.-C., Côté, M.-A., Garyfallidis, E., Zhong, J., Chamberland, M., Yeh, F.-C., Lin, Y.-C., Ji, Q., Reddick, W. E., Glass, J. O., Chen, D. Q., Feng, Y., Gao, C., Wu, Y., Ma, J., He, R., Li, Q., … Descoteaux, M. (2017). The challenge of mapping the human connectome based on diffusion tractography. Nature Communications, 8(1), 1349. https://doi.org/10.1038/s41467-017-01285-x\n\n\nMeehl, P. E. (1967). Theory-testing in psychology and physics: A methodological paradox. Philosophy of Science, 34(2), 103–115.\n\n\nMeehl, P. E. (1978). Theoretical risks and tabular asterisks: Sir karl, sir ronald, and the slow progress of soft psychology. 46(September 1976), 806–834.\n\n\nMinocher, R., Atmaca, S., Bavero, C., McElreath, R., & Beheim, B. (n.d.). Estimating the reproducibility of social learning research published between 1955 and 2018. Royal Society Open Science, 8(9), 210450. https://doi.org/10.1098/rsos.210450\n\n\nMunafò, M. R., Nosek, B. A., Bishop, D. V. M., Button, K. S., Chambers, C. D., Percie Du Sert, N., Simonsohn, U., Wagenmakers, E. J., Ware, J. J., & Ioannidis, J. P. A. (2017). A manifesto for reproducible science. Nature Human Behaviour, 1(1), 1–9. https://doi.org/10.1038/s41562-016-0021\n\n\nNosek, B. A., Beck, E. D., Campbell, L., Flake, J. K., Hardwicke, T. E., Mellor, D. T., van?t Veer, A. E., & Vazire, S. (2019). Preregistration is hard, and worthwhile. Trends in Cognitive Sciences, 23(10), 815–818.\n\n\nNosek, B. A., Ebersole, C. R., DeHaven, A. C., & Mellor, D. T. (2018). The preregistration revolution. Proceedings of the National Academy of Sciences, 115(11), 2600–2606.\n\n\nNosek, B. A., Hardwicke, T. E., Moshontz, H., Allard, A., Corker, K. S., Dreber, A., Fidler, F., Hilgard, J., Kline Struhl, M., Nuijten, M. B., Rohrer, J. M., Romero, F., Scheel, A. M., Scherer, L. D., Schönbrodt, F. D., & Vazire, S. (2022). Replicability, Robustness, and Reproducibility in Psychological Science. Annual Review of Psychology, 73, 719–748. https://doi.org/10.1146/annurev-psych-020821-114157\n\n\nNosek, B. A., & Lakens, D. (2014). Registered reports: A method to increase the credibility of published results. Social Psychology, 45(3), 137–141. https://doi.org/10.1027/1864-9335/a000192\n\n\nObels, P., Lakens, D., Coles, N. A., Gottfried, J., & Green, S. A. (2020). Analysis of open data and computational reproducibility in registered reports in psychology. Advances in Methods and Practices in Psychological Science, 3(2), 229–237. https://doi.org/10.1177/2515245920918872\n\n\nParsons, S. (n.d.). Exploring reliability heterogeneity with multiverse analyses: Data processing decisions unpredictably influence measurement reliability. https://doi.org/10.31234/osf.io/y6tcz\n\n\nPashler, H., & Harris, C. (2012). Is the replicability crisis overblown? Three arguments examined. Perspectives on Psychological Science, 7(6), 531–536. https://doi.org/10.1177/1745691612463401\n\n\nPashler, H., & Wagenmakers, E. J. (2012). Editors’ introduction to the special section on replicability in psychological science: A crisis of confidence? Perspectives on Psychological Science, 7(6), 528–530. https://doi.org/10.1177/1745691612465253\n\n\nPatel, C. J., Burford, B., & Ioannidis, J. P. A. (2015). Assessment of vibration of effects due to model specification can demonstrate the instability of observational associations. Journal of Clinical Epidemiology, 68(9), 1046–1058. https://doi.org/10.1016/j.jclinepi.2015.05.029\n\n\nPoline, J.-B., Strother, S. C., Dehaene-Lambertz, G., Egan, G. F., & Lancaster, J. L. (2006). Motivation and synthesis of the FIAC experiment: Reproducibility of fMRI results across expert analyses. Human Brain Mapping, 27(5), 351–359. https://doi.org/10.1002/hbm.20268\n\n\nRoche, D. G., Kruuk, L. E. B., Lanfear, R., & Binning, S. A. (2015). Public data archiving in ecology and evolution: How well are we doing? PLoS Biology, 13(11), 1–12. https://doi.org/10.1371/journal.pbio.1002295\n\n\nSalganik, M. J., Lundberg, I., Kindel, A. T., Ahearn, C. E., Al-Ghoneim, K., Almaatouq, A., Altschul, D. M., Brand, J. E., Carnegie, N. B., Compton, R. J., Datta, D., Davidson, T., Filippova, A., Gilroy, C., Goode, B. J., Jahani, E., Kashyap, R., Kirchner, A., McKay, S., … McLanahan, S. (2020). Measuring the predictability of life outcomes with a scientific mass collaboration. Proceedings of the National Academy of Sciences, 117(15), 8398–8403. https://doi.org/10.1073/pnas.1915006117\n\n\nScheel, A. M. (2022). Why most psychological research findings are not even wrong. Infant and Child Development, 31(1), e2295. https://doi.org/10.1002/icd.2295\n\n\nScheel, A. M., Tiokhin, L., Isager, P. M., & Lakens, D. (2021). Why Hypothesis Testers Should Spend Less Time Testing Hypotheses. Perspectives on Psychological Science, 16(4), 744–755. https://doi.org/10.1177/1745691620966795\n\n\nSchweinsberg, M., Feldman, M., Staub, N., Akker, O. R. van den, Aert, R. C. M. van, Assen, M. A. L. M. van, Liu, Y., Althoff, T., Heer, J., Kale, A., Mohamed, Z., Amireh, H., Venkatesh Prasad, V., Bernstein, A., Robinson, E., Snellman, K., Amy Sommer, S., Otner, S. M. G., Robinson, D., … Luis Uhlmann, E. (2021). Same data, different conclusions: Radical dispersion in empirical results when independent analysts operationalize and test the same hypothesis. Organizational Behavior and Human Decision Processes, 165, 228–249. https://doi.org/10.1016/j.obhdp.2021.02.003\n\n\nSedlmeier, P., & Gigerenzer, G. (1989). Statistical power studies. Psychological Bulletin, 105(2), 309–316.\n\n\nSilberzahn, R., & Uhlmann, E. L. (2015). Crowdsourced research: Many hands make tight work. Nature, 526(7572), 189–191. https://doi.org/10.1038/526189a\n\n\nSilberzahn, R., Uhlmann, E. L., Martin, D. P., Anselmi, P., Aust, F., Awtrey, E., Bahník, Š., Bai, F., Bannard, C., Bonnier, E., Carlsson, R., Cheung, F., Christensen, G., Clay, R., Craig, M., Dalla Rosa, A., Dam, L., Evans, M., Flores Cervantes, I., … Nosek, B. (2017). Many analysts, one dataset: Making transparent how variations in analytical choices affect results. Advances in Methods and Practices in Psychological Science. https://doi.org/10.31234/osf.io/qkwst\n\n\nSimmons, J. P., Nelson, L. D., & Simonsohn, U. (2011b). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological Science, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632\n\n\nSimmons, J. P., Nelson, L. D., & Simonsohn, U. (2011a). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological Science, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632\n\n\nStarns, J. J., Cataldo, A. M., Rotello, C. M., Annis, J., Aschenbrenner, A., Bröder, A., Cox, G., Criss, A., Curl, R. A., Dobbins, I. G., Dunn, J., Enam, T., Evans, N. J., Farrell, S., Fraundorf, S. H., Gronlund, S. D., Heathcote, A., Heck, D. W., Hicks, J. L., … Wilson, J. (2019). Assessing Theoretical Conclusions With Blinded Inference to Investigate a Potential Inference Crisis. Advances in Methods and Practices in Psychological Science, 2(4), 335–349. https://doi.org/10.1177/2515245919869583\n\n\nSteegen, S., Tuerlinckx, F., Gelman, A., & Vanpaemel, W. (2016a). Increasing transparency through a multiverse analysis. Perspectives on Psychological Science, 11(5), 702–712.\n\n\nSteegen, S., Tuerlinckx, F., Gelman, A., & Vanpaemel, W. (2016b). Increasing transparency through a multiverse analysis. Perspectives on Psychological Science, 11(5), 702–712.\n\n\nTedersoo, L., Küngas, R., Oras, E., Köster, K., Eenmaa, H., Leijen, Ä., Pedaste, M., Raju, M., Astapova, A., Lukner, H., Kogermann, K., & Sepp, T. (2021). Data sharing practices and data availability upon request differ across scientific disciplines. Scientific Data, 8(1), 192. https://doi.org/10.1038/s41597-021-00981-0\n\n\nTowse, J. N., Ellis, D. A., & Towse, A. S. (2021). Opening Pandora’s Box: Peeking inside Psychology’s data sharing practices, and seven recommendations for change. Behavior Research Methods, 53(4), 1455–1468. https://doi.org/10.3758/s13428-020-01486-1\n\n\nUlrich, R., & Miller, J. (1994). Effects of truncation on reaction time analysis. Journal of Experimental Psychology: General, 123, 34–80.\n\n\nVankov, I., Bowers, J., & Munafò, M. R. (2014). On the persistence of low power in psychological science. Quarterly Journal of Experimental Psychology, 67(5), 1037–1040. https://doi.org/10.1080/17470218.2014.885986\n\n\nVasishth, S., & Gelman, A. (2021). How to embrace variation and accept uncertainty in linguistic and psycholinguistic data analysis. Linguistics, 59(5), 1311–1342. https://doi.org/10.1515/ling-2019-0051\n\n\nVazire, S. (2018). Implications of the Credibility Revolution for Productivity, Creativity, and Progress. Perspectives on Psychological Science, 13(4), 411–417. https://doi.org/10.1177/1745691617751884\n\n\nWagenmakers, E.-J., Sarafoglou, A., & Aczel, B. (2022). One statistical analysis must not rule them all. Nature, 605(7910), 423–425. https://doi.org/10.1038/d41586-022-01332-8\n\n\nWagenmakers, E.-J., Wetzels, R., Borsboom, D., & Maas, H. L. J. van der. (2011). Why psychologists must change the way they analyze their data: The case of psi: Comment on bem (2011). Journal of Personality and Social Psychology, 100(3), 426–432. https://doi.org/10.1037/a0022790\n\n\nWessel, I., Albers, C., Zandstra, A. R. E., & Heininga, V. E. (2020). A multiverse analysis of early attempts to replicate memory suppression with the think/no-think task.\n\n\nWicherts, J. M., Borsboom, D., Kats, J., & Molenaar, D. (2006). The poor availability of psychological research data for reanalysis. American Psychologist, 61(7), 726–728. https://doi.org/10.1037/0003-066X.61.7.726\n\n\nWild, H., Kyröläinen, A.-J., & Kuperman, V. (2022). How representative are student convenience samples? A study of literacy and numeracy skills in 32 countries. PLOS ONE, 17(7), e0271191. https://doi.org/10.1371/journal.pone.0271191\n\n\nYarkoni, T. (2022). The generalizability crisis. Behavioral and Brain Sciences, 45, e1. https://doi.org/10.1017/S0140525X20001685\n\n\nYoung, C. (2018). Model uncertainty and the crisis in science. Socius, 4, 2378023117737206.\n\n\nYoung, C., & Holsteen, K. (2017). Model Uncertainty and Robustness: A Computational Framework for Multimodel Analysis. Sociological Methods & Research, 46(1), 3–40. https://doi.org/10.1177/0049124115610347",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Why we are asking you to do this</span>"
    ]
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "9  Why we are asking you to do this",
    "section": "",
    "text": "This encouragement is often attributed to Gandhi but is attributed ((here)) to a Brooklyn school teacher, Ms Arleen Lorrance, who led a transformative school project in the 1970s.↩︎\nThe term is taken from the name of a short story by Jorge Luis Borges, “El jardin de senderos que se bifurcan” (translated as ‘The garden of forking paths’).↩︎\nThere could be a story where the hero (us) ultimately learns to reject binary (present, absent; significant, non-significant) choices, and embrace variation, or embrace uncertainty (a. Gelman, 2015; Vasishth & Gelman, 2021).↩︎\nOpen access journals publish articles that are free to read or download.↩︎",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Why we are asking you to do this</span>"
    ]
  },
  {
    "objectID": "knowledge-ecosystem.html",
    "href": "knowledge-ecosystem.html",
    "title": "10  The R knowledge ecosystem and how to help yourself",
    "section": "",
    "text": "Warning\n\n\n\nUnder construction",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>The R knowledge ecosystem and how to help yourself</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "11  Summary",
    "section": "",
    "text": "To complete when book is completed.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Aarts, E., Dolan, C. V., Verhage, M., & Van der Sluis, S. (2015).\nMultilevel analysis quantifies variation in the experimental effect\nwhile optimizing power and preventing false positives. BMC\nNeuroscience, 16(1), 1–15. https://doi.org/10.1186/s12868-015-0228-5\n\n\nAczel, B., Szaszi, B., Nilsonne, G., Akker, O. R. van den, Albers, C.\nJ., Assen, M. A. van, Bastiaansen, J. A., Benjamin, D., Boehm, U.,\nBotvinik-Nezer, R., Bringmann, L. F., Busch, N. A., Caruyer, E.,\nCataldo, A. M., Cowan, N., Delios, A., Dongen, N. N. van, Donkin, C.,\nDoorn, J. B. van, … Wagenmakers, E.-J. (2021). Consensus-based guidance\nfor conducting and reporting multi-analyst studies. eLife,\n10, e72185. https://doi.org/10.7554/eLife.72185\n\n\nAnscombe, F. J. (1973). Graphs in statistical analysis. The American\nStatistician, 27(1), 17–21. https://doi.org/10.2307/2682899\n\n\nArtner, R., Verliefde, T., Steegen, S., Gomes, S., Traets, F.,\nTuerlinckx, F., & Vanpaemel, W. (2021). The reproducibility of\nstatistical results in psychological research: An investigation using\nunpublished raw data. Psychological Methods, 26(5),\n527–546. https://doi.org/10.1037/met0000365\n\n\nAuspurg, K., & Brüderl, J. (2021). Has the Credibility of the Social\nSciences Been Credibly Destroyed? Reanalyzing the “Many\nAnalysts, One Data Set” Project. Socius,\n7, 23780231211024421. https://doi.org/10.1177/23780231211024421\n\n\nBastiaansen, J. A., Kunkels, Y. K., Blaauw, F. J., Boker, S. M.,\nCeulemans, E., Chen, M., Chow, S.-M., Jonge, P. de, Emerencia, A. C.,\nEpskamp, S., Fisher, A. J., Hamaker, E. L., Kuppens, P., Lutz, W.,\nMeyer, M. J., Moulder, R., Oravecz, Z., Riese, H., Rubel, J., …\nBringmann, L. F. (2020). Time to get personal? The impact of researchers\nchoices on the selection of treatment targets using the experience\nsampling methodology. Journal of Psychosomatic Research,\n137, 110211. https://doi.org/10.1016/j.jpsychores.2020.110211\n\n\nBates, D., Mächler, M., Bolker, B., & Walker, S. (2015). Fitting\nlinear mixed-effects models using lme4.\nJournal of Statistical Software, 67(1), 1–48. https://doi.org/10.18637/jss.v067.i01\n\n\nBelenky, G., Wesensten, N. J., Thorne, D. R., Thomas, M. L., Sing, H.\nC., Redmond, D. P., Russo, M. B., & Balkin, T. J. (2003). Patterns\nof performance degradation and restoration during sleep restriction and\nsubsequent recovery: a sleep dose-response study. Journal of Sleep\nResearch, 12(1), 1–12. https://doi.org/10.1046/j.1365-2869.2003.00337.x\n\n\nBornstein, M. H., Jager, J., & Putnick, D. L. (2013). Sampling in\ndevelopmental science: Situations, shortcomings, solutions, and\nstandards. Developmental Review, 33(4), 357–370. https://doi.org/10.1016/j.dr.2013.08.003\n\n\nBorsboom, D., Mellenbergh, G. J., & Heerden, J. van. (2004). The\nconcept of validity. Psychological Review, 111(4),\n1061–1071. https://doi.org/10.1037/0033-295X.111.4.1061\n\n\nBotvinik-Nezer, R., Holzmeister, F., Camerer, C. F., Dreber, A., Huber,\nJ., Johannesson, M., Kirchler, M., Iwanir, R., Mumford, J. A., Adcock,\nR. A., Avesani, P., Baczkowski, B. M., Bajracharya, A., Bakst, L., Ball,\nS., Barilari, M., Bault, N., Beaton, D., Beitner, J., … Schonberg, T.\n(2020). Variability in the analysis of a single neuroimaging dataset by\nmany teams. Nature, 582(7810), 84–88. https://doi.org/10.1038/s41586-020-2314-9\n\n\nBourdieu, P. (2004). Science of Science and Reflexivity.\nPolity.\n\n\nBreznau, N., Rinke, E. M., Wuttke, A., Nguyen, H. H. V., Adem, M.,\nAdriaans, J., Alvarez-Benjumea, A., Andersen, H. K., Auer, D., Azevedo,\nF., Bahnsen, O., Balzer, D., Bauer, G., Bauer, P. C., Baumann, M.,\nBaute, S., Benoit, V., Bernauer, J., Berning, C., … Żółtak, T. (2022).\nObserving many researchers using the same data and hypothesis reveals a\nhidden universe of uncertainty. Proceedings of the National Academy\nof Sciences, 119(44), e2203150119. https://doi.org/10.1073/pnas.2203150119\n\n\nBrosowsky, N., Parshina, O., Locicero, A., & Crump, M. (n.d.).\nTeaching undergraduate students to read empirical articles: An\nevaluation and revision of the QALMRI method. https://doi.org/10.31234/osf.io/p39sc\n\n\nButton, K. S., Ioannidis, J. P. A., Mokrysz, C., Nosek, B. A., Flint,\nJ., Robinson, E. S. J., & Munafò, M. R. (2013). Power failure: Why\nsmall sample size undermines the reliability of neuroscience. Nature\nReviews Neuroscience, 14(5), 365–376.\n\n\nCarp, J. (2012a). On the plurality of (methodological) worlds:\nEstimating the analytic flexibility of FMRI experiments. Frontiers\nin Neuroscience, 6, 149.\n\n\nCarp, J. (2012b). The secret lives of experiments: Methods reporting in\nthe fMRI literature. Neuroimage, 63(1), 289–300.\n\n\nChang, W. (2013). R graphics cookbook. o’Reilly Media.\n\n\nChristensen, R. H. B. (n.d.). Cumulative Link Models for Ordinal\nRegression with the R Package ordinal.\n\n\nCohen, J. (1962). The statistical power of abnormal-social psychological\nresearch: A review. Journal of Abnormal and Social Psychology,\n65(3), 145–153. https://doi.org/10.1037/h0045186\n\n\nCrüwell, S., Apthorp, D., Baker, B. J., Colling, L., Elson, M., Geiger,\nS. J., Lobentanzer, S., Monéger, J., Patterson, A., Schwarzkopf, D. S.,\nZaneva, M., & Brown, N. J. L. (n.d.). What’s in a\nbadge? A computational reproducibility investigation of the open data\nbadge policy in one issue of psychological science. https://doi.org/10.31234/osf.io/729qt\n\n\nDavies, R. A. I., Birchenough, J. M. H., Arnell, R., Grimmond, D., &\nHoulson, S. (2017). Reading through the life span: Individual\ndifferences in psycholinguistic effects. Journal of Experimental\nPsychology: Learning Memory and Cognition, 43(8). https://doi.org/10.1037/xlm0000366\n\n\nDavies, R., Barbón, A., & Cuetos, F. (2013). Lexical and semantic\nage-of-acquisition effects on word naming in spanish. Memory and\nCognition, 41(2), 297–311. https://doi.org/10.3758/s13421-012-0263-8\n\n\nDel Giudice, M., & Gangestad, S. W. (2021). A\nTraveler’s Guide to the Multiverse: Promises, Pitfalls, and\na Framework for the Evaluation of Analytic Decisions. Advances in\nMethods and Practices in Psychological Science, 4(1),\n2515245920954925. https://doi.org/10.1177/2515245920954925\n\n\nDutilh, G., Annis, J., Brown, S. D., Cassey, P., Evans, N. J., Grasman,\nR. P. P. P., Hawkins, G. E., Heathcote, A., Holmes, W. R., Krypotos,\nA.-M., Kupitz, C. N., Leite, F. P., Lerche, V., Lin, Y.-S., Logan, G.\nD., Palmeri, T. J., Starns, J. J., Trueblood, J. S., Maanen, L. van, …\nDonkin, C. (2019). The Quality of Response Time Data Inference: A\nBlinded, Collaborative Assessment of the Validity of Cognitive Models.\nPsychonomic Bulletin & Review, 26(4), 1051–1069.\nhttps://doi.org/10.3758/s13423-017-1417-2\n\n\nFederer, L. M. (2022). Long-term availability of data associated with\narticles in PLOS ONE. PLOS ONE, 17(8), e0272845. https://doi.org/10.1371/journal.pone.0272845\n\n\nFillard, P., Descoteaux, M., Goh, A., Gouttard, S., Jeurissen, B.,\nMalcolm, J., Ramirez-Manzanares, A., Reisert, M., Sakaie, K., Tensaouti,\nF., Yo, T., Mangin, J.-F., & Poupon, C. (2011). Quantitative\nevaluation of 10 tractography algorithms on a realistic diffusion MR\nphantom. NeuroImage, 56(1), 220–234. https://doi.org/10.1016/j.neuroimage.2011.01.032\n\n\nFlake, J. K., & Fried, E. I. (2020). Measurement Schmeasurement:\nQuestionable Measurement Practices and How to Avoid Them. Advances\nin Methods and Practices in Psychological Science, 3(4),\n456–465. https://doi.org/10.1177/2515245920952393\n\n\nFranconeri, S. L., Padilla, L. M., Shah, P., Zacks, J. M., &\nHullman, J. (2021). The Science of Visual Data Communication: What\nWorks. Psychological Science in the Public Interest,\n22(3), 110–161. https://doi.org/10.1177/15291006211051956\n\n\nFreed, E. M., Hamilton, S. T., & Long, D. L. (2017). Comprehension\nin proficient readers: The nature of individual variation. Journal\nof Memory and Language, 97, 135–153. https://doi.org/10.1016/j.jml.2017.07.008\n\n\nGabelica, M., Bojčić, R., & Puljak, L. (2022). Many researchers were\nnot compliant with their published data sharing statement: a\nmixed-methods study. Journal of Clinical Epidemiology,\n150, 33–41. https://doi.org/10.1016/j.jclinepi.2022.05.019\n\n\nGelman, a. (2015). The connection between varying treatment effects and\nthe crisis of unreplicable research: A bayesian perspective. Journal\nof Management, 41(2), 632–643. https://doi.org/10.1177/0149206314525208\n\n\nGelman, A., & Loken, E. (2014a). The garden of forking paths: Why\nmultiple comparisons can be a problem, even when there is no\n“fishing expedition” or\n“p-hacking” and the research hypothesis was\nposited ahead of time. Psychological Bulletin, 140(5),\n1272–1280.\n\n\nGelman, A., & Loken, E. (2014b). The statistical crisis in science.\nAmerican Scientist, 102(6), 460–465. https://doi.org/10.1511/2014.111.460\n\n\nGelman, A., Pasarica, C., & Dodhia, R. (2002). Let’s practice what\nwe preach. The American Statistician, 56(2), 121–130.\nhttps://doi.org/10.1198/000313002317572790\n\n\nGelman, A., & Unwin, A. (2013). Infovis and Statistical Graphics:\nDifferent Goals, Different Looks. Journal of Computational and\nGraphical Statistics, 22(1), 2–28. https://doi.org/10.1080/10618600.2012.761137\n\n\nGelman, A., & Weakliem, D. (2009). Of beauty, sex and power.\nAmerican Scientist, 97(4), 310–316. https://doi.org/10.1511/2009.79.310\n\n\nGigerenzer, G. (2004). Mindless statistics. The Journal of\nSocio-Economics, 33(5), 587–606. https://doi.org/10.1016/j.socec.2004.09.033\n\n\nGilmore, R. O., Diaz, M. T., Wyble, B. A., & Yarkoni, T. (2017).\nProgress toward openness, transparency, and reproducibility in cognitive\nneuroscience. Annals of the New York Academy of Sciences,\n1396, 5–18. https://doi.org/10.1111/nyas.13325\n\n\nGoodman, S. N., Fanelli, D., & Ioannidis, J. P. A. (2016). What does\nresearch reproducibility mean? Science Translational Medicine,\n8(341).\n\n\nHardwicke, T. E., Bohn, M., MacDonald, K., Hembacher, E., Nuijten, M.\nB., Peloquin, B. N., deMayo, B. E., Long, B., Yoon, E. J., & Frank,\nM. C. (n.d.). Analytic reproducibility in articles receiving open data\nbadges at the journal psychological science: An observational study.\nRoyal Society Open Science, 8(1), 201494. https://doi.org/10.1098/rsos.201494\n\n\nHardwicke, T. E., Mathur, M. B., MacDonald, K., Nilsonne, G., Banks, G.\nC., Kidwell, M. C., Hofelich Mohr, A., Clayton, E., Yoon, E. J., Henry\nTessler, M., Lenne, R. L., Altman, S., Long, B., & Frank, M. C.\n(2018). Data availability, reusability, and analytic reproducibility:\nevaluating the impact of a mandatory open data policy at the journal\nCognition. Royal Society Open Science, 5(8), 180448.\nhttps://doi.org/10.1098/rsos.180448\n\n\nHenrich, J., Heine, S. J., & Norenzayan, A. (2010). The weirdest\npeople in the world? The Behavioral and Brain Sciences,\n33(2-3). https://doi.org/10.1017/S0140525X0999152X\n\n\nHerndon, T., Ash, M., & Pollin, R. (2014). Does high public debt\nconsistently stifle economic growth? A critique of Reinhart and Rogoff.\nCambridge Journal of Economics, 38(2), 257–279. https://doi.org/10.1093/cje/bet075\n\n\nHoekstra, R., Morey, R. D., Rouder, J. N., & Wagenmakers, E.-J.\n(2014). Robust misinterpretation of confidence intervals.\nPsychonomic Bulletin & Review, 21(5), 1157–1164.\nhttps://doi.org/10.3758/s13423-013-0572-3\n\n\nHoffmann, S., Schönbrodt, F., Elsas, R., Wilson, R., Strasser, U., &\nBoulesteix, A.-L. (n.d.). The multiplicity of analysis strategies\njeopardizes replicability: Lessons learned across disciplines. Royal\nSociety Open Science, 8(4), 201925. https://doi.org/10.1098/rsos.201925\n\n\nHofman, J. M., Goldstein, D. G., & Hullman, J. (2020). How\nvisualizing inferential uncertainty can mislead readers about treatment\neffects in scientific results. 112. https://doi.org/10.1145/3313831.3376454\n\n\nIoannidis, J. P. a. (2005). Why most published research findings are\nfalse. PLoS Medicine, 2(8), 0696–0701. https://doi.org/10.1371/journal.pmed.0020124\n\n\nJohn, L. K., Loewenstein, G., & Prelec, D. (2012). Measuring the\nprevalence of questionable research practices with incentives for truth\ntelling. Psychological Science, 23(5), 524–532. https://doi.org/10.1177/0956797611430953\n\n\njumpingrivers. (n.d.). Datasets from the Datasaurus Dozen. https://jumpingrivers.github.io/datasauRus/\n\n\nKastellec, J. P., & Leoni, E. L. (2007). Using Graphs Instead of\nTables in Political Science. Perspectives on Politics,\n5(4), 755–771. https://doi.org/10.1017/S1537592707072209\n\n\nKidwell, M. C., Lazarević, L. B., Baranski, E., Hardwicke, T. E.,\nPiechowski, S., Falkenberg, L. S., Kennett, C., Slowik, A., Sonnleitner,\nC., Hess-Holden, C., Errington, T. M., Fiedler, S., & Nosek, B. A.\n(2016). Badges to acknowledge open practices: A simple, low-cost,\neffective method for increasing transparency. PLoS Biology,\n14(5), 1–15. https://doi.org/10.1371/journal.pbio.1002456\n\n\nKintsch, W. (1994). Text comprehension, memory, and learning.\nAmerican Psychologist, 49(4), 294–303. https://doi.org/10.1037/0003-066x.49.4.294\n\n\nKlau, S., Hoffmann, S., Patel, C. J., Ioannidis, J. P., &\nBoulesteix, A.-L. (2021). Examining the robustness of observational\nassociations to model, measurement and sampling uncertainty with the\nvibration of effects framework. International Journal of\nEpidemiology, 50(1), 266–278. https://doi.org/10.1093/ije/dyaa164\n\n\nKlau, S., Schönbrodt, F., Patel, C. J., Ioannidis, J., Boulesteix,\nA.-L., & Hoffmann, S. (n.d.). Comparing the vibration of effects\ndue to model, data pre-processing and sampling uncertainty on a large\ndata set in personality psychology. https://doi.org/10.31234/osf.io/c7v8b\n\n\nKosslyn, S. M., & Rosenberg, R. S. (2005). Fundamentals of\npsychology: The brain, the person, the world, 2nd ed. Pearson\nEducation New Zealand.\n\n\nKuhn, T. S. (1970). The structure of scientific revolutions\n([2d ed., enl). University of Chicago Press.\n\n\nLandy, J. F., Jia, M. L., Ding, I. L., Viganola, D., Tierney, W.,\nDreber, A., Johannesson, M., Pfeiffer, T., Ebersole, C. R., Gronau, Q.\nF., Ly, A., Bergh, D. van den, Marsman, M., Derks, K., Wagenmakers,\nE.-J., Proctor, A., Bartels, D. M., Bauman, C. W., Brady, W. J., …\nUhlmann, E. L. (2020). Crowdsourcing hypothesis tests: Making\ntransparent how design choices shape research results. Psychological\nBulletin, 146(5), 451–479. https://doi.org/10.1037/bul0000220\n\n\nLaurinavichyute, A., Yadav, H., & Vasishth, S. (2022). Share the\ncode, not just the data: A case study of the reproducibility of articles\npublished in the Journal of Memory and Language under the open data\npolicy. Journal of Memory and Language, 125, 104332.\nhttps://doi.org/10.1016/j.jml.2022.104332\n\n\nLiddell, T. M., & Kruschke, J. K. (2018). Analyzing ordinal data\nwith metric models: What could possibly go wrong? Journal of\nExperimental Social Psychology, 79(September), 328–348. https://doi.org/10.1016/j.jesp.2018.08.009\n\n\nLubega, N., Anderson, A., & Nelson, N. (n.d.). Experience of\nirreproducibility as a risk factor for poor mental health in biomedical\nscience doctoral students: A survey and interview-based study. https://doi.org/10.31222/osf.io/h37kw\n\n\nMaier-Hein, K. H., Neher, P. F., Houde, J.-C., Côté, M.-A.,\nGaryfallidis, E., Zhong, J., Chamberland, M., Yeh, F.-C., Lin, Y.-C.,\nJi, Q., Reddick, W. E., Glass, J. O., Chen, D. Q., Feng, Y., Gao, C.,\nWu, Y., Ma, J., He, R., Li, Q., … Descoteaux, M. (2017). The challenge\nof mapping the human connectome based on diffusion tractography.\nNature Communications, 8(1), 1349. https://doi.org/10.1038/s41467-017-01285-x\n\n\nMatejka, J., & Fitzmaurice, G. (2017). CHI ’17: CHI Conference\non Human Factors in Computing Systems. 1290–1294. https://doi.org/10.1145/3025453.3025912\n\n\nMeehl, P. E. (1967). Theory-testing in psychology and physics: A\nmethodological paradox. Philosophy of Science, 34(2),\n103–115.\n\n\nMeehl, P. E. (1978). Theoretical risks and tabular asterisks: Sir\nkarl, sir ronald, and the slow progress of soft psychology.\n46(September 1976), 806–834.\n\n\nMeehl, P. E. (1990). Why summaries of research on psychological theories\nare often uninterpretable. Psychological Reports,\n66(1), 195–244. https://doi.org/10.2466/pr0.1990.66.1.195\n\n\nMeteyard, L., & Davies, R. A. I. (2020). Best practice guidance for\nlinear mixed-effects models in psychological science. Journal of\nMemory and Language, 112. https://doi.org/10.1016/j.jml.2020.104092\n\n\nMinocher, R., Atmaca, S., Bavero, C., McElreath, R., & Beheim, B.\n(n.d.). Estimating the reproducibility of social learning research\npublished between 1955 and 2018. Royal Society Open Science,\n8(9), 210450. https://doi.org/10.1098/rsos.210450\n\n\nMunafò, M. R., Nosek, B. A., Bishop, D. V. M., Button, K. S., Chambers,\nC. D., Percie Du Sert, N., Simonsohn, U., Wagenmakers, E. J., Ware, J.\nJ., & Ioannidis, J. P. A. (2017). A manifesto for reproducible\nscience. Nature Human Behaviour, 1(1), 1–9. https://doi.org/10.1038/s41562-016-0021\n\n\nNosek, B. A., Beck, E. D., Campbell, L., Flake, J. K., Hardwicke, T. E.,\nMellor, D. T., van?t Veer, A. E., & Vazire, S. (2019).\nPreregistration is hard, and worthwhile. Trends in Cognitive\nSciences, 23(10), 815–818.\n\n\nNosek, B. A., Ebersole, C. R., DeHaven, A. C., & Mellor, D. T.\n(2018). The preregistration revolution. Proceedings of the National\nAcademy of Sciences, 115(11), 2600–2606.\n\n\nNosek, B. A., Hardwicke, T. E., Moshontz, H., Allard, A., Corker, K. S.,\nDreber, A., Fidler, F., Hilgard, J., Kline Struhl, M., Nuijten, M. B.,\nRohrer, J. M., Romero, F., Scheel, A. M., Scherer, L. D., Schönbrodt, F.\nD., & Vazire, S. (2022). Replicability, Robustness, and\nReproducibility in Psychological Science. Annual Review of\nPsychology, 73, 719–748. https://doi.org/10.1146/annurev-psych-020821-114157\n\n\nNosek, B. A., & Lakens, D. (2014). Registered reports: A method to\nincrease the credibility of published results. Social\nPsychology, 45(3), 137–141. https://doi.org/10.1027/1864-9335/a000192\n\n\nObels, P., Lakens, D., Coles, N. A., Gottfried, J., & Green, S. A.\n(2020). Analysis of open data and computational reproducibility in\nregistered reports in psychology. Advances in Methods and Practices\nin Psychological Science, 3(2), 229–237. https://doi.org/10.1177/2515245920918872\n\n\nParsons, S. (n.d.). Exploring reliability heterogeneity with\nmultiverse analyses: Data processing decisions unpredictably influence\nmeasurement reliability. https://doi.org/10.31234/osf.io/y6tcz\n\n\nPashler, H., & Harris, C. (2012). Is the replicability crisis\noverblown? Three arguments examined. Perspectives on Psychological\nScience, 7(6), 531–536. https://doi.org/10.1177/1745691612463401\n\n\nPashler, H., & Wagenmakers, E. J. (2012). Editors’ introduction to\nthe special section on replicability in psychological science: A crisis\nof confidence? Perspectives on Psychological Science,\n7(6), 528–530. https://doi.org/10.1177/1745691612465253\n\n\nPatel, C. J., Burford, B., & Ioannidis, J. P. A. (2015). Assessment\nof vibration of effects due to model specification can demonstrate the\ninstability of observational associations. Journal of Clinical\nEpidemiology, 68(9), 1046–1058. https://doi.org/10.1016/j.jclinepi.2015.05.029\n\n\nPinheiro, J. C., & Bates, D. M. (2000). Mixed-effects models in\ns and s-plus (statistics and computing). Springer.\n\n\nPoline, J.-B., Strother, S. C., Dehaene-Lambertz, G., Egan, G. F., &\nLancaster, J. L. (2006). Motivation and synthesis of the FIAC\nexperiment: Reproducibility of fMRI results across expert analyses.\nHuman Brain Mapping, 27(5), 351–359. https://doi.org/10.1002/hbm.20268\n\n\nRicketts, J., Dawson, N., & Davies, R. (2021). The hidden depths of\nnew word knowledge: Using graded measures of orthographic and semantic\nlearning to measure vocabulary acquisition. Learning and\nInstruction, 74, 101468. https://doi.org/10.1016/j.learninstruc.2021.101468\n\n\nRoche, D. G., Kruuk, L. E. B., Lanfear, R., & Binning, S. A. (2015).\nPublic data archiving in ecology and evolution: How well are we doing?\nPLoS Biology, 13(11), 1–12. https://doi.org/10.1371/journal.pbio.1002295\n\n\nRodríguez-Ferreiro, J., Aguilera, M., & Davies, R. (2020). Semantic\npriming and schizotypal personality: reassessing the link between\nthought disorder and enhanced spreading of semantic activation.\nPeerJ, 8, e9511. https://doi.org/10.7717/peerj.9511\n\n\nSalganik, M. J., Lundberg, I., Kindel, A. T., Ahearn, C. E., Al-Ghoneim,\nK., Almaatouq, A., Altschul, D. M., Brand, J. E., Carnegie, N. B.,\nCompton, R. J., Datta, D., Davidson, T., Filippova, A., Gilroy, C.,\nGoode, B. J., Jahani, E., Kashyap, R., Kirchner, A., McKay, S., …\nMcLanahan, S. (2020). Measuring the predictability of life outcomes with\na scientific mass collaboration. Proceedings of the National Academy\nof Sciences, 117(15), 8398–8403. https://doi.org/10.1073/pnas.1915006117\n\n\nScheel, A. M. (2022). Why most psychological research findings are not\neven wrong. Infant and Child Development, 31(1),\ne2295. https://doi.org/10.1002/icd.2295\n\n\nScheel, A. M., Tiokhin, L., Isager, P. M., & Lakens, D. (2021). Why\nHypothesis Testers Should Spend Less Time Testing Hypotheses.\nPerspectives on Psychological Science, 16(4), 744–755.\nhttps://doi.org/10.1177/1745691620966795\n\n\nSchweinsberg, M., Feldman, M., Staub, N., Akker, O. R. van den, Aert, R.\nC. M. van, Assen, M. A. L. M. van, Liu, Y., Althoff, T., Heer, J., Kale,\nA., Mohamed, Z., Amireh, H., Venkatesh Prasad, V., Bernstein, A.,\nRobinson, E., Snellman, K., Amy Sommer, S., Otner, S. M. G., Robinson,\nD., … Luis Uhlmann, E. (2021). Same data, different conclusions: Radical\ndispersion in empirical results when independent analysts operationalize\nand test the same hypothesis. Organizational Behavior and Human\nDecision Processes, 165, 228–249. https://doi.org/10.1016/j.obhdp.2021.02.003\n\n\nSedlmeier, P., & Gigerenzer, G. (1989). Statistical power studies.\nPsychological Bulletin, 105(2), 309–316.\n\n\nSilberzahn, R., & Uhlmann, E. L. (2015). Crowdsourced research: Many\nhands make tight work. Nature, 526(7572), 189–191. https://doi.org/10.1038/526189a\n\n\nSilberzahn, R., Uhlmann, E. L., Martin, D. P., Anselmi, P., Aust, F.,\nAwtrey, E., Bahník, Š., Bai, F., Bannard, C., Bonnier, E., Carlsson, R.,\nCheung, F., Christensen, G., Clay, R., Craig, M., Dalla Rosa, A., Dam,\nL., Evans, M., Flores Cervantes, I., … Nosek, B. (2017). Many analysts,\none dataset: Making transparent how variations in analytical choices\naffect results. Advances in Methods and Practices in Psychological\nScience. https://doi.org/10.31234/osf.io/qkwst\n\n\nSimmons, J. P., Nelson, L. D., & Simonsohn, U. (2011a).\nFalse-positive psychology: Undisclosed flexibility in data collection\nand analysis allows presenting anything as significant.\nPsychological Science, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632\n\n\nSimmons, J. P., Nelson, L. D., & Simonsohn, U. (2011b).\nFalse-positive psychology: Undisclosed flexibility in data collection\nand analysis allows presenting anything as significant.\nPsychological Science, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632\n\n\nSmaldino, P. (2019). Better methods can’t make up for mediocre theory.\nNature, 575(7781), 9.\n\n\nStarns, J. J., Cataldo, A. M., Rotello, C. M., Annis, J., Aschenbrenner,\nA., Bröder, A., Cox, G., Criss, A., Curl, R. A., Dobbins, I. G., Dunn,\nJ., Enam, T., Evans, N. J., Farrell, S., Fraundorf, S. H., Gronlund, S.\nD., Heathcote, A., Heck, D. W., Hicks, J. L., … Wilson, J. (2019).\nAssessing Theoretical Conclusions With Blinded Inference to Investigate\na Potential Inference Crisis. Advances in Methods and Practices in\nPsychological Science, 2(4), 335–349. https://doi.org/10.1177/2515245919869583\n\n\nSteegen, S., Tuerlinckx, F., Gelman, A., & Vanpaemel, W. (2016a).\nIncreasing transparency through a multiverse analysis. Perspectives\non Psychological Science, 11(5), 702–712.\n\n\nSteegen, S., Tuerlinckx, F., Gelman, A., & Vanpaemel, W. (2016b).\nIncreasing transparency through a multiverse analysis. Perspectives\non Psychological Science, 11(5), 702–712.\n\n\nTedersoo, L., Küngas, R., Oras, E., Köster, K., Eenmaa, H., Leijen, Ä.,\nPedaste, M., Raju, M., Astapova, A., Lukner, H., Kogermann, K., &\nSepp, T. (2021). Data sharing practices and data availability upon\nrequest differ across scientific disciplines. Scientific Data,\n8(1), 192. https://doi.org/10.1038/s41597-021-00981-0\n\n\nTowse, J. N., Ellis, D. A., & Towse, A. S. (2021). Opening Pandora’s\nBox: Peeking inside Psychology’s data sharing practices, and seven\nrecommendations for change. Behavior Research Methods,\n53(4), 1455–1468. https://doi.org/10.3758/s13428-020-01486-1\n\n\nUlrich, R., & Miller, J. (1994). Effects of truncation on reaction\ntime analysis. Journal of Experimental Psychology: General,\n123, 34–80.\n\n\nVan Der Bles, A. M., Van Der Linden, S., Freeman, A. L. J., Mitchell,\nJ., Galvao, A. B., Zaval, L., & Spiegelhalter, D. J. (2019).\nCommunicating uncertainty about facts, numbers and science\n(Vol. 6).\n\n\nVankov, I., Bowers, J., & Munafò, M. R. (2014). On the persistence\nof low power in psychological science. Quarterly Journal of\nExperimental Psychology, 67(5), 1037–1040. https://doi.org/10.1080/17470218.2014.885986\n\n\nVasishth, S., & Gelman, A. (2021). How to embrace variation and\naccept uncertainty in linguistic and psycholinguistic data analysis.\nLinguistics, 59(5), 1311–1342. https://doi.org/10.1515/ling-2019-0051\n\n\nVazire, S. (2018). Implications of the Credibility Revolution for\nProductivity, Creativity, and Progress. Perspectives on\nPsychological Science, 13(4), 411–417. https://doi.org/10.1177/1745691617751884\n\n\nWagenmakers, E.-J., Sarafoglou, A., & Aczel, B. (2022). One\nstatistical analysis must not rule them all. Nature,\n605(7910), 423–425. https://doi.org/10.1038/d41586-022-01332-8\n\n\nWagenmakers, E.-J., Wetzels, R., Borsboom, D., & Maas, H. L. J. van\nder. (2011). Why psychologists must change the way they analyze their\ndata: The case of psi: Comment on bem (2011). Journal of Personality\nand Social Psychology, 100(3), 426–432. https://doi.org/10.1037/a0022790\n\n\nWessel, I., Albers, C., Zandstra, A. R. E., & Heininga, V. E.\n(2020). A multiverse analysis of early attempts to replicate memory\nsuppression with the think/no-think task.\n\n\nWicherts, J. M., Borsboom, D., Kats, J., & Molenaar, D. (2006). The\npoor availability of psychological research data for reanalysis.\nAmerican Psychologist, 61(7), 726–728. https://doi.org/10.1037/0003-066X.61.7.726\n\n\nWickham, H. (2016). ggplot2: Elegant graphics for data\nanalysis. Springer-Verlag New York. https://ggplot2.tidyverse.org\n\n\nWickham, H. (2017). Tidyverse: Easily install and load the\n’tidyverse’. https://cran.r-project.org/package=tidyverse\n\n\nWickham, H., & Grolemund, G. (2016). R for data science: Import,\ntidy, transform, visualize, and model data. \" O’Reilly\nMedia, Inc.\".\n\n\nWild, H., Kyröläinen, A.-J., & Kuperman, V. (2022). How\nrepresentative are student convenience samples? A study of literacy and\nnumeracy skills in 32 countries. PLOS ONE, 17(7),\ne0271191. https://doi.org/10.1371/journal.pone.0271191\n\n\nWilke, C. O. (n.d.). Fundamentals of data visualization. https://clauswilke.com/dataviz/\n\n\nWilkinson, L. (2013). The Grammar of Graphics. Springer Science\n& Business Media.\n\n\nYarkoni, T. (2022). The generalizability crisis. Behavioral and\nBrain Sciences, 45, e1. https://doi.org/10.1017/S0140525X20001685\n\n\nYoung, C. (2018). Model uncertainty and the crisis in science.\nSocius, 4, 2378023117737206.\n\n\nYoung, C., & Holsteen, K. (2017). Model Uncertainty and Robustness:\nA Computational Framework for Multimodel Analysis. Sociological\nMethods & Research, 46(1), 3–40. https://doi.org/10.1177/0049124115610347\n\n\nZhang, S., Heck, P. R., Meyer, M. N., Chabris, C. F., Goldstein, D. G.,\n& Hofman, J. M. (2023). An illusion of predictability in scientific\nresults: Even experts confuse inferential uncertainty and outcome\nvariability. Proceedings of the National Academy of Sciences,\n120(33), e2302491120. https://doi.org/10.1073/pnas.2302491120",
    "crumbs": [
      "References"
    ]
  }
]