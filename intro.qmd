# Introduction

The research report assignment requires students to locate, access, analyse and report previously collected data. This introduction is intended to answer the first question anybody might ask.

-   Why: what is the motivation for the assignment?

In following materials, I will answer the questions.

-   How can the assignment be done?
-   What do we expect students to do?

It is going to appear, at first, that I am going a *long* way away from telling you what you need to do for the assignment. I hope you will agree that the discussion that follows is worth your time in reading it. It will help you to understand *why* we are asking you to do the assignment, and *why* we are looking for what we are looking for. It will help you to understand *how* this work will aid your development. And it will help to show *how* doing the assignment furnishes the opportunity for research experience that will help you later in your working life.

For those who are more eager to start the work, here are the links to the how \[\] and to the what \[\] information.

## The key ideas

There are two ideas motivating our approach. It will be helpful to you if I sketch them out early, here. We can demonstrate the usefulness of these ideas as we progress through our work.

The first key idea is expressed clearly in sociological discussions of science. This is that there is a difference between science "...being done, science in the making, and science already done, a finished product ..." [@bourdieu2004; p.2]. The awareness we want to develop is that there are two things: there is the story that may be presented in a textbook or in a lecture about scientific work or scientific claims; and there is the work we do in practice, as we develop graduate skills, and as we exercise those skills professionally in the workplace.

The second key idea connects to the first. This idea is that reported analyses are not *necessary* or *sufficient* to the data or the question. What does this mean? It means that the same data can reasonably be analysed in different ways. There is no *necessary* way to analyse some data though there may be conventions or normal practices [@kuhn1970]. It means that it is unlikely that any one analysis will do all the work that could be done (a sufficiency) to get you from your data to useful or reasonable answers to your questions.

These ideas may be unsettling but they are realistic. Stating them will better prepare you for professional work. In the workplace, the accuracy of these ideas will emerge when you see how a team in any sector (health, marketing ...) gets from its data to its product. If we talk about the ideas now, we can get you ready for dealing with the practical and the ethical concerns you will confront when that happens.

We will begin by discussing psychological research, and research *about* psychological research, to answer the question: **Why: what is the motivation for the assignment?** We will then move to answering the **what** and the **how** questions.

## Why: what is the motivation for the assignment?

### The wider context: crisis and revolution

We are all here because we are interested in humans and human behaviour, and because we are interested in scientific methods of making sense of these things. Some of us are aware that science (including psychological science) has undergone a rolling series of crises: the replicability or replication crisis [@Pashler2012a; @Pashler2012b]; the statistical crisis [@gelman2014a]; and the generalizability crisis [@yarkoni2022]. And that science is undergoing a response to these crises, evidenced in the advocacy of pre-registration [@nosek2018; @nosek2019prereg], and of registered reports [@nosek2014], the use of open science badges (e.g., [for the journal *Psychological Science*](https://www.psychologicalscience.org/publications/badges){.external target="_blank"}), the completion of large-scale replication studies [@aarts2015], and the identification of open science principles [@munafÃ²2017].

We could teach a course on this (in Lancaster, we do) but I must be brief, here, and invite you to follow the references, if you are interested. Before going on, I want to call your attention to the fact that important elements of the hard work in trying to make science work better has been led by PhD students and by junior researchers [e.g., @herndon2014]. Graduate students may, at first, assume that the fact that a research article has been published in a journal means the findings that are reported must be *true*. Most of the time, some educated skepticism is more appropriate. An important driver of the realization that there are problems evident in the literature, and that there are changes we can make to improve practice, comes from independent **post-publication review work** exposing the problems in published work (see, e.g., [this account by Andrew Gelman](https://statmodeling.stat.columbia.edu/2016/12/16/an-efficiency-argument-for-post-publication-review/){.external target="_blank"})

::: callout-tip
-   Allow yourself to feel skeptical about the reports you read *then* work with the motivation this feeling provides.
:::

In brief, then, most practicing scientists now understand *or should* understand that many of the claims we encounter in the published scientific literature are unlikely to be supported by the evidence [@Ioannidis2005], whether we are looking at the evidence of the results in the reports themselves, or evidence in later attempts to find the same results [e.g., @aarts2015]. We suspect that this may result from a number of causes. We understand that researchers may engage in questionable research practices [@john2012]. We understand that researchers may exploit the potential for flexibility in doing and reporting analyses [@Simmons2011a]. We understand that there are problems in how psychologists use or talk about the measurement of psychological constructs [@flake2020]. We understand that there are problems in how psychologists sample people for their studies, both in where we recruit [@bornstein2013; @wild2022; @Henrich2010], and in how many we recruit [@button2013; @cohen1962; @sedlmeier1989; @vankov2014]. We understand that there are problems in how psychologists specify or think about their hypotheses or predictions [@meehl1967; @scheel2022]. And we understand that there are problems in how scientists do, or rather do not, comply with good practice recommendations designed to fix these problems (discussed further in the following).

This discussion could (again) be unsettling. This list of problems could make you angry or sad. I, like others, think it is exciting. It is exciting because these problems have probably existed for a long time [e.g., @cohen1962; @meehl1967] and now, having identified the problems, we can hope to do something about it. It is exciting because if you care about people, the study of people, or the applications in clinical, education and other domains of the results of the study of people, then you might hope to see better, more useful, science in the future [@vazire2018].

As someone who teaches graduate and undergraduate students, I want to help you to be the change you want to see in the world. We cannot solve every problem but we can try to do better those things that are within our reach. I am going to end this introduction with a brief discussion of some ideas we can use to guide our better practices.

### The specific context: what we need to look at, conceptually and practically

In this course, for this assignment, we are going to focus on:

1.  multiverse analyses
2.  kinds of reproducibility
3.  the current mis-match between open science ideas and practices

In the classes on the linear model, we will discuss:

4.  the links between theory, prediction and analysis
5.  psychological measurement
6.  samples
7.  variation in results

### Multiverse analyses

#### A first useful metaphor: the pipeline

I am going to link this discussion to a metaphor (see Figure @fig-pipeline) or a description you will find useful: **the data analysis pipeline** or **workflow**.

```{dot}
//| fig-width: 5
//| label: fig-pipeline
//| fig-cap: The data analysis pipeline or workflow
//| fig-alt: The diagram shows a flowchart. The flowchart starts at the top with "get raw data" and finishes at the bottom with "present". "get raw data" has an arrow to "tidy data". "tidy data" has arrows to "analyze", "explore" and "visualize". On the same level as "tidy "data", "assumptions" also has arrows to "analyze", "explore" and "visualize". Then "analyze", "explore" and "visualize" have arrows to "present. 
digraph Q {

  node [shape=record];

  nd_1   [label = "Get raw data"];
  nd_2   [label = "Tidy data"];
  nd_3_a [label = "Assumptions"];
  nd_3_l [label = "Visualize"];
  nd_3   [label = "Analyze"];
  nd_3_r [label = "Explore"];
  nd_4   [label = "Present"];

  nd_3_a -> nd_3 [color= grey];
  nd_3_a -> nd_3_r [color= grey];
  nd_3_a -> nd_3_l [color= grey];
  
  nd_2 -> nd_3_r;
  nd_2 -> nd_3_l;
  
  nd_1 -> nd_2 -> nd_3 -> nd_4;
  
  nd_3_l -> nd_4;

  subgraph cluster_R {

    {rank=same nd_3_l nd_3 nd_3_r}

    nd_3_l -> nd_3 -> nd_3_r [color= none arrowhead=none];

  }

}
```

This metaphor or way of thinking is very common ([take a look at the diagram in Wickham and Grolemund's 2017 book "R for Data Science](https://r4ds.had.co.nz/introduction.html){.external target="_blank"}) and you may see the words "data pipeline" used in job descriptions, or you may benefit from saying, in a job application, something like: *I am skilled in designing and implementing each stage of the quantitative data analysis pipeline, from data tidying to results presentation*. I say this because scientists I have mentored got their jobs because they can do these things -- and successfully explained that they can do these things -- in sectors like educational testing, behavioural analysis, or public policy research.

The reason this metaphor is useful is that it helps us to organize our thinking, and to manage what we do when we do data analysis, we:

-   get some data;
-   process or tidy the data;
-   explore, visualize, and analyze the data;
-   present or report our findings.

We introduce the idea that your analysis work will flow through the stages of a *pipeline* from getting the data to presenting your findings because, next, we will examine how pipelines can *multiply*.

::: callout-tip
-   As you practice your data analysis work, try to identify the elements and the order of your work, as the parts of a *workflow*.
:::

#### A second useful metaphor: the garden of forking paths

What researchers have come to realize: because we started looking ... The open secret that has been well kept [@bourdieu2004]: because everybody who does science knows about it, yet we may not teach it; and because we do not write textbooks revealing it ... Is that at each stage in the analysis workflow, we can and do make choices where multiple alternative choices are possible. @gelman2014 capture this insight as the "garden of forking paths"[^intro-1] (see @fig-forking).

[^intro-1]: The term is taken from the name of a short story by Jorge Luis Borges, "El jardin de senderos que se bifurcan".

The general idea is that it is possible to have **multiple potential different paths** from the data to the results. The results will vary, depending on the path we take. In an analysis, we could take multiple different paths simply because at point A we decide to do B1, B2 or B3, maybe we choose B1, and then at point B1, we may decide to do C1, C2 or C3. Here, maybe we have our raw data at point A. Maybe we could do one of two different things when we tidy the data: action B1 or B2. Then, when we have our tidy data, maybe we can choose to do our analysis in one of six ways. Where we are at each step *depends* on the choices we made at the previous steps.

```{dot}
//| fig-width: 5
//| label: fig-forking
//| fig-cap: Forking paths in data analysis
//| fig-alt: The diagram shows a hierarchy of nodes. At the top, node A has arrows down to nodes B1 and B2. Nodes B1 and B2 are on the same level. B1 has arrows to nodes C1, C2 and C3. B2 has arrows to nodes C4, C5 and C6. 

digraph D {

  A -> {B1, B2}
  
  B1 -> {C1, C2, C3}

  B2 -> {C4, C5, C6}

}
```

In the end, it may appear to us that we took one path or that only one path was possible. When we report our analysis, in a dissertation or in a published journal article, we may report the analysis **as if only one analysis path had been considered**. But, critically, our findings may depend on the choices we made and this variation in results may be hidden from view.

I am talking about forking paths because the *multiplicity* of paths has consequences, and we discuss these next.

::: callout-tip
-   It is about here, I hope, that you can start to see why it would makes sense to access data from a published study and to examine if you can get the same results as the study authors.
:::

### Multiverse analyses

I am going to discuss, now, what are commonly called *multiverse analyses*. Psychologists use this term, having been introduced to it in an influential paper by @steegen2016, but it comes from theoretical physics ([take a look at wikipedia](https://en.wikipedia.org/wiki/Multiverse){.external target="_blank"}).

I explain this because I do not want you to worry. The ideas themselves are within your grasp whatever your background in psychology or elsewhere. It is the implications for our data analysis practices that are *challenging*. They are challenging because what we discuss should increase your skepticism about the results you encounter in published papers. And they are challenging because they *reveal your freedom* to question whether published authors could have done their analysis in a different way.

We are going to look at:

1.  dataset construction
2.  analysis choices

#### The link between crisis and the multiverse

In first discussing the wider context (of crisis and revolution), then discussing the specific context (of multiverses and, in the following, of reproducibility), I should be clear about **the link between the two things**.
The finding that some results may not be supported by the evidence is probably due to a mix of causes.
But one of those causes will be the uncertainty over data processing or the uncertainty over analysis methods revealed in multiverse analyses, as we see next.

#### The data multiverse

When you collect or access data for a research study, the complete raw dataset you receive is almost never the complete dataset you analyze or whose analysis you report. This is not a story about deliberately cheating. It is a story about the normal practice of science [@kuhn1970].

Picture some common scenarios. You did a survey, you got responses from a 100 participants on 10 questions, and you asked people to report their education, ethnicity and gender. You did an experimental study, you tested two groups of 50 people each in 100 trials (imagine a common task like the Stroop test), and you observed the accuracy and the timing of their responses. You tested 100 children, 20 children in each of five different schools, on a range of educational ability measures.

In these scenarios, the psychologist or the analyst of behavioural data *must* process their data. In doing so, you will ask yourself a series of questions like:

-   how do we code for `gender, ethnicity, education`?
-   what do we about reaction times that are very short, e.g., $RT < 200ms$ or very long, e.g., $RT > 1500ms$)?
-   if we present multiple questions measuring broadly the same thing (e.g. how confident are you that you understand what you have read? how easy did you find what you read?) how do we summarize the scores on those questions? do we combine scores?
-   what do we do about people who may not appear to have understood the task instructions?

Typically, the answers to these questions will be given to you by your supervisor, a colleague or a textbook example. For example, we might say:

-   "We excluded all reaction times greater than 1500ms before analysis."

Typically, the explanation for these answers are rarely explained. We might say:

-   "*Consistent with common practice in this field*, we excluded all reaction times greater than 1500ms before analysis."

But the reader of a journal article typically **will not see** an explanation for why, as in the example, we exclude reaction times greater than 1500ms and not 2000ms or 3000ms, etc. We typically do not see an explanation for why *we* exclude all reaction times greater than 1500ms but *other* researchers exclude all reaction times greater than 2000ms. (I do not pick this example at random: there are serious concerns about the impact on analyses of exclusions like this [@Ulrich1994a].)

What @steegen2016 showed is that a dataset can be processed for analysis in multiple different ways, with a number of reasonable alternate choices that can be applied, for each choice point: construction choices about classifying people or about excluding participants given their responses. If a different dataset is constructed for each combination of alternatives then many different datasets can be produced, all starting from the same raw data. (For their example study, @steegen2016 found they could construct 120 or 210 different datasets, based on the choice combinations.) Critically, for us, @steegen2016 showed that if we apply the same analysis method to the different datasets then our results will vary.

Let me spell this out, bit by bit:

-   we approach our study with the same research question, and the same verbal prediction;
-   we begin with the exact same data;
-   we then construct different datasets depending on different *but equally reasonable* processing choices;
-   we then apply the same analysis analysis, to test the same prediction, using each different dataset;
-   we will see different results for the analyses of the different datasets.

Alternate constructions of the same data may cause variation in the results of statistical tests. Some kinds of data processing choices may be more influential on results than others. It seems unlikely that we can identify, in advance, which choices matter more.

@steegen2016 suggest that we can *deflate* (shrink) the multiverse in different ways. I want to state their suggestions, here, because we will come back to these ideas in the classes on the linear model.

1.  Develop better theories and improved measurement of the constructs of interest.
2.  Develop more complete and precise theory for why some processing options are better than others.

But you will be asking yourself: **what do I need to think about, for the assignment?**

::: callout-tip
-   When you read a psychological research report, identify where the researchers talk about how they process their data: classification, coding, exclusion, transformation, etc.
-   If you can access the raw data, ask yourself: could different choices change the results of the same analysis?
:::

#### Analysis multiverses

Even if we begin with the same research question and, critically, the *same dataset*, the results of a series of studies show that different researchers will often (reasonably) make *different choices about the analysis* they do to answer the research question. We often call these studies (analysis or model) **multiverse** studies. In these studies, we see variation in analysis and this variation is also associated with variation in results.

An influential example, in psychology, is reported by Silberzahn and colleagues [@silberzahn2015; @silberzahn2017] who asked 29 teams of researchers to answer the same question ("Are (soccer) referees more likely to give red cards to players with dark skin than to players with light skin?") with the same dataset (data about referee decisions in football league games). The teams made their own decisions about how to answer the question in doing the analysis. The teams shared their plans, and commented on each others' ideas. The discussion did not lead to a consensus about what analysis approach is best. In the end, the different teams did different analyses and, critically, the **different analyses had different results**. The results varied in whether the test of the effect of players skin colour (on whether red cards were given) was significant or not, and on the strength of the estimated association between the darkness of skin colour (lighter to darker) and the chances (low to high) of getting a red card.

There have now been a series of multiverse or multi-analyst studies which demonstrate that, under certain conditions, different researchers may adopt different analysis approaches -- which will have *different results* -- in answering the same research question with the same data. This demonstration has been repeated in studies in health, medicine, psychology, neuoscience, and sociology, among other research fields (e.g., @parsons; @breznau2022; @klau; @klau2021; @wessel2020multiverse; @poline2006; @maier-hein2017; @starns2019; @fillard2011; @dutilh2019; @salganik2020; @bastiaansen2020; @botvinik-nezer2020; @schweinsberg2021; @patel2015; see, for reviews, and some helpful guidance, @aczel2021; @delgiudice2021; @hoffmann; @wagenmakers2022).

In these studies, we typically see variation in how psychological constructs are operationalized (e.g., how do we measure or code for social status?), how data are processed or datasets constructed (as in @steegen2016a), plus variation in *what* statistical techniques are used, and in *how* those techniques are used. This variation can be understood to reflect kinds of **uncertainty** [@klau; @klau2021]: uncertainty about how to process data, and uncertainty about the model or methods we should use to test or estimate effects. Further research makes it clear that we should be aware, if we are not already, of the variation in results that can be expected because different researchers may choose to design studies, and construct stimulus materials, in different ways given the same research hypothesis information [@landy2020a].

But you will be asking yourself: **what do I need to think about, for the assignment?**

::: callout-tip
-   When you read a psychological research report, identify where the researchers talk about how they analyse their data: the hypothesis or prediction they test; the method; their assumptions; the variables they include; the checks or the alternate analyses they did or did not do.
-   If you can access the data and analysis code, ask yourself: could different methods change the results of the same analysis?
:::

#### What can we conclude?

This is not a story where everybody or nobody is right or where everything or nothing is true [^intro-variation].
Instead, we can be guided by the advice [@meehl1967; @scheel2022; @steegen2016] that we should (1.) seek better and more complete theorizing about the constructs of interest and how we measure them, and (2.) seek more complete and more precise theory so that some options are theoretically superior than others, and should be preferred, when constructing datasets or specifying analysis methods.

[^intro-variation]: There could be a story where the hero (us) ultimately learns to reject binary (right, wrong) choices, and embrace variation, or embrace uncertainty [@Gelman2015; @vasishth2021]. 

Not all research questions and not all hypothesis information will allow an equally wide variety of potential reasonable approaches to the analysis. As Paul Meehl argued a long time ago [@meehl1967; @meehl1978], and researchers like Anne Scheel [@scheel2021; @scheel2022] argued more recently, the complexity of the thing we study -- people, and what they do -- and the still early development of our understanding of this thing, mean that what we *want* but what we do not see, in psychology, are scientifically productive tests of *falsifiable theories*. (See, in line with this perspective, discussions by @auspurg2021 and by @delgiudice2021 about the range of analysis possibilities that may or may not be allowed, in multiverse analyses, by more or less clear research questions or well-developed causal theories.) Our concern should not so much be with being able to do statistical analysis, or with finding significant or not significant results. It would be more useful to do analyses to test concrete, inflexible, precise predictions that *can be wrong*.

Nor is this a story, I think, about the potential for *cheating*. While we may refer to subjective choices or to researcher flexibility, the differences that we see do not resemble the *researcher degrees of freedom* [@Simmons2011] some may exploit, consciously or unconsciously, to change results to suit their aims. Instead, the multiverse results show us the impact of the reasonable differences in approach that different researchers may sensibly choose to take when they try to answer a research question with data.

Not all alternates, at a given point of choosing, in the data analysis workflow, will have equal impact. Work by Young [@young2017; @young2018] indicates that if we deliberately examine the impact of method or model uncertainty, over different sets of possible choices --- about what variables or what observations we include in an analysis, for example --- we may find that some results are robust to an array of different options, while other results are highly susceptible to different choices. This work suggests another way in which uncertainty about methods or variation in results can be turned into progress in understanding the phenomena that interest us: through systematic, informed, interrogation of the ways that results can vary.

In general, in science, the acceptance of research findings must always be negotiated [@bourdieu2004]. Here, we see that the grounds of negotiation should often include an analysis of the impact on the value of evidence of the different analysis approaches that researchers can or do apply to the data that underly that evidence.

But you will be asking yourself: **what do I need to think about, for the assignment?**

::: callout-tip
-   The results of multiverse analyses show us that if we see one analysis reported in a paper, or one workflow, that does not mean that only one analysis can reasonably be applied.
-   If you read the methods or results section of a paper, you should reflect: what other analysis methods could be used here? How could variation in analysis method --- in what or how you do the analysis --- influence the results?
:::

Making you aware of the potential for analysis choices is useful because developing researchers, including graduate students, are often not aware of the room for choice in the data analysis workflow. Developing researchers --- you --- may be instructed that "this is how we do things" or "you should follow what researchers did previously". Following convention is not necessarily a bad thing: it is a feature of the normal practice of science [@kuhn1970a]. However, you can now see, perhaps, that there likely will be alternative ways to process or to analyse data than the approach a supervisor, lab or field normally adopts.

This understanding or awareness has three implications for practice, it means:

1.  When we talk about the analysis we do, we should explain our choices.
2.  We should check, or enable others to check, what impact making different choices would have on our results.
3.  Most importantly: **we can allow ourselves the freedom to critically evaluate** the choices researchers make, even the choices researchers make in published articles.

### Kinds of reproducibility

-- results, methods and inferential reproducibility -- results will vary

-- are results stable? -- specific to choices or to samples? -- scientific virtues require: -- examination of stability over contexts, choices

### The mis-match between open science ideas and practices

-- reproducibility varies -- now normative to expect data and code -- but declaration that data or code available not always -- rarely -- fulfilled -- data and code if shared may not be usable, poorly documented or incomplete -- even where there are badges -- data and code if shared and usable may not result in reproduction of results reported -- sensitivity or robustness checks: results can depend on specification choices

### Scientific virtues

In some fields, there have long been calls to conduct *robustness* or *sensitivity* analyses to examine the extent to which analysis results may depend on analyst choices [@leamer1983let; @leamer1985sensitivity]. In practice,

-   stability over contexts

## This is why

-- this is why: -- want students to graduate aware of issues so can make deliberate choices -- want to be ready to share good data with high quality documentation -- want to be aware of utility of tests for robustness or sensitivity

-- a good way to learn this is to see what there is in the wild -- look for data and try to use it: if hard, learn the lesson and do better in own data practices -- know that there is almost never a necessary and sufficient analysis of data -- though seeing possible choices beyond those that published may require a bit of skills and experience -- that is where our teaching, support and learning opportunities comes in

-- why not just do less demanding or challenging tasks? -- because this is part of what makes a Lancaster MSc degree valuable -- what makes you more available in the job market, in employment

## References

::: {#refs}
:::
